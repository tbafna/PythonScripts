{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saccade detection using velocity threshold method by Edward Ryklin, as taught in his course at ETRA2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import copy\n",
    "import itertools\n",
    "import distance\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import snowball\n",
    "\n",
    "from itertools import *\n",
    "from operator import *\n",
    "\n",
    "# import other jupyter notebooks\n",
    "import import_ipynb\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptional removal of particular extra sentences not typed by the user \n",
    "dict_phraseStim = {\n",
    "    #'2019-02-05-14-10-39_2ndPart_2' : [1, 2, 3, 4, 5, 6, 9, 10],\n",
    "    #'2019-01-14-14-58-30' : [0], # ys, session_trial ()\n",
    "    '2019-01-16-16-36-17_1stPart_2' : [-1], # af_session1\n",
    "    '2019-01-16-17-00-12_2ndPart_2': [1], # af_session1\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-17-16-03-27_2ndPart_2' : [0, 1, 2], # Af session2\n",
    "    '2019-02-06-11-25-41_1' : [7],               # aq_session1    \n",
    "    '2019-02-08-11-33-53_1stPart_1' : [1],  # aq session3_1_part1\n",
    "    '2019-02-08-12-11-34_2ndPart_1' : [0, 1, 2, 3],  # aq session3_1_part2\n",
    "    '2019-01-31-09-37-5_2ndPart_2' : range(1,5), # bh1, session 4 , all sentences except the first one deleted\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4],  # bh1_session4_2_part1\n",
    "    '2019-02-21-16-09-44_1stPart_1' : [1], # bh2_session1\n",
    "    '2019-02-21-16-22-22_2ndPart_1' : [2, 3, 4],# bh2_session1\n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],       # bh2_session3\n",
    "    '2019-02-28-17-24-2_2ndPart_2' : [0, 2],     # bh2_session3\n",
    "    '2019-02-14-13-28-20_1stPart_2' : [2], # cw_session3_2_part1\n",
    "    '2019-02-14-13-57-41_2ndPart_2' : [0, 2, 3], # cw_session3_2_part2\n",
    "    '2019-02-21-15-01-4_1stPart_1' : [0],        # le_session3\n",
    "    '2019-02-21-15-25-56_2ndPart_1' : [1],        # le_session3\n",
    "    '2019-02-18-10-28-35_2' : [0],               # ls2_session4 # picture not described\n",
    "    '2019-02-05-14-00-27_1stPart_2' : [3],        # mh_session1\n",
    "    '2019-02-05-14-10-39_2ndPart_2' : [0, 1, 3],   # mh_session1\n",
    "    '2019-02-08-10-51-3_1stPart_1' : [4],        # mn_session1\n",
    "    '2019-02-08-11-05-7_2ndPart_1' : [0, 2, 3, 4], # mn_session1\n",
    "    '2019-02-19-10-34-7_1stPart_1' : [3],          # mn_session3\n",
    "    '2019-02-19-10-56-43_2ndPart_1' : [1, 2, 3, 4], # mn_session3\n",
    "    '2019-01-16-15-18-0_1' : [4],            # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-01-29-13-25-4_1' : [3],        # ph_session2\n",
    "    '2019-03-07-16-44-5_2' : [1],                   # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],                  # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4], # ys_session1\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [3, 4], # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [2], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [3, 5, 7],          # ys_session4\n",
    "    '2019-01-30-11-22-25_1' : [4, 6, 7] # ys, session 4\n",
    "}\n",
    "\n",
    "# exceptional removal of sentences/words typed by the user, but then deleted everything to have a blank scratchpad\n",
    "\n",
    "dict_phraseUser = {\n",
    "    \"2019-02-06-15-44-15_1\" : [2, 3, 6], \n",
    "    \"2019-02-06-16-19-9_2\" : [1, 3, 6, 7],\n",
    "    \"2019-02-12-11-21-21_2\" : [0],\n",
    "    \"2019-02-14-14-28-49_1\" : [0, 2, 3], # ac_session3_1\n",
    "    \"2019-02-14-14-45-49_2\" : [0, 5, 6], # ac_session3_2\n",
    "    '2019-01-29-14-19-26_1' : [0, 3, 4], # bh1_session2_1\n",
    "    '2019-01-29-14-40-36_2' : [0, 1, 2], # bh1_session2_2\n",
    "    '2019-01-30-14-29-29_2' : [4],       # bh1_session3_2\n",
    "    '2019-01-31-09-12-2_1' : [3],         # bh1_session4_1\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4], # bh1_session4_2_part1\n",
    "    '2019-03-05-09-15-11_1' : [1],         # bh2_session5_1\n",
    "    '2019-03-05-09-15-11_2' : [1],        # bh2_session5_2\n",
    "    '2019-02-21-15-55-56_2' : [2],       # ch_session5_2\n",
    "    '2019-01-30-15-19-36_2' : [1],       # jm_session2_1\n",
    "    '2019-01-30-15-04-30_1' : [0],         # jm_session2_2\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [1],  # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [0], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [2, 4],       # ys_session4\n",
    "    '2019-01-30-11-57-3_2' : [0] ,          # ys_session4\n",
    "    '2019-01-31-13-13-2_1' : [4],           # ys_session5\n",
    "    '2019-01-30-10-20-32_1' : [0, 1, 2, 3, 4, 5], # no_session4\n",
    "    '2019-01-30-10-46-38_2' : [0],          # \n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],   # bh2_session3\n",
    "    '2019-03-12-09-30-5_1' : [0],            # kj_session3\n",
    "    '2019-02-13-15-20-38_1' : [0, 1, 2, 3, 6], # ls1_session3\n",
    "    '2019-02-18-10-25-52_1' : [1],              # ls2_session4\n",
    "    '2019-02-18-10-46-26_2' : [0],            # ls2_session4\n",
    "    '2019-01-29-13-25-4_1' : [0, 1, 7],        # ph_session2\n",
    "    '2019-01-29-13-43-50_2' : [0],              # ph_session2\n",
    "    '2019-03-07-16-17-30_1' : [0],              # rh_session1\n",
    "    '2019-03-07-16-44-5_2' : [0, 1],         # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [0, 1, 3]         # rh_session3\n",
    "}\n",
    "\n",
    "# key selection can have extra selections of NextPhrase at the end\n",
    "dict_keySelectionOfNextPhrase = {\n",
    "    \"2019-02-11-11-18-30_1\" : [12, 13], # ac_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [12], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [12], # af_session2\n",
    "    \"2019-02-06-16-19-9_2\" : [12], # af_session3\n",
    "    \"2019-02-12-11-07-43_1\" : [12], # af_session4\n",
    "    \"2019-02-27-15-08-32_1\" : [12], # af_session5\n",
    "    \"2019-01-28-14-30-44_1\" : [12], # bh1_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [12], # bh2_session1\n",
    "    \"2019-02-18-14-02-56_2\" : [12], # le_session1\n",
    "    \"2019-02-19-10-03-14_1\" : [12], # le_session2\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [12], # mn_session1\n",
    "    \"2019-02-08-11-12-51_2\" : [12, 13], # mn_session1\n",
    "    \"2019-02-15-11-38-22_1\" : [12, 13], # mn_session2\n",
    "    \"2019-02-15-11-54-25_2\" : [12], # mn_session2\n",
    "    \"2019-01-16-15-18-0_1\" : [12], # no_session1\n",
    "    \"2019-01-28-13-31-51_1\" : [12], # ph_session1\n",
    "    \"2019-01-28-13-49-14_2\" : [12], # ph_session1\n",
    "    \"2019-01-14-15-07-21_1\" : [12], # ys_session1\n",
    "    \"2019-01-17-15-05-1_1\" : [12], # ys_session3\n",
    "    \"2019-01-30-11-22-25_1\" : [12], # ys_session4\n",
    "    \"2019-01-31-13-32-2_2\" : [12], # ys_session5\n",
    "}\n",
    "\n",
    "\n",
    "# key selection when participants skips some sentences\n",
    "dict_keySelectionNotCompleted = {\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : [0, 1, 3, 5, 7], # af_session1 ---- last sentence is not finished\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [0, 1, 3, 4, 5, 7, 9, 11], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [0, 1, 3, 5, 7, 9, 11], # af_session2 \n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 6, 7, 9, 11], # af_session2\n",
    "    \"2019-02-08-11-33-53_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # aq_session3\n",
    "    \"2019-02-08-12-11-34_2ndPart_1\" : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11], # aq_session3\n",
    "    \"2019-01-28-14-30-44_1\" : [0, 1, 3, 5], # bh1_session1\n",
    "    \"2019-01-31-09-22-49_1stPart_2\": [0, 1, 3, 5, 7, 9, 10, 11], # bh1_session4\n",
    "    \"2019-01-31-09-37-5_2ndPart_2\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # bh1_session4\n",
    "    \"2019-02-21-16-09-44_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # bh2_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [0, 1, 3, 5, 6, 7, 8, 9, 10, 11], # bh2_session1\n",
    "    \"2019-02-28-17-03-53_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # bh2_session3\n",
    "    \"2019-02-28-17-24-2_2ndPart_2\" : [0, 1, 2, 3, 5], # bh2_session3     ----\n",
    "    \"2019-02-14-13-28-20_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # cw_session3\n",
    "    \"2019-02-14-13-57-41_2ndPart_2\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 11], # cw_session3\n",
    "    \"2019-02-21-15-01-4_1stPart_1\" : [0, 1, 2, 3, 5, 7, 9, 11], # le_session3\n",
    "    \"2019-02-21-15-25-56_2ndPart_1\" : [0, 1, 3], # le_session3       ----\n",
    "    \"2019-02-05-14-00-27_1stPart_2\" : [0, 1, 3, 5, 7, 8], # mh_session1\n",
    "    \"2019-02-05-14-10-39_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 7, 8, 9, 11], # mh_session1\n",
    "    \"2019-02-08-10-51-3_1stPart_1\" : [0, 1, 3, 5, 7, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-19-10-34-7_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 11], # mn_session3\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # mn_session3\n",
    "    \"2019-01-29-13-25-4_1\" : [0, 1, 3, 5, 7], # ph_session2  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-18-50_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 10], # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : [0, 1, 3, 5],  # ys_session3  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-02-06-11-25-41_1\" : [0, 1, 3, 5, 11], # aq_session1 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2 -- different for reading and writing, this one is for\n",
    "    # writing\n",
    "    '2019-01-30-11-22-25_1' : [0, 1, 3, 5, 7, 9, 11]   # ys_session4 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "   \n",
    "}\n",
    "\n",
    "# dictionary for phrase removal just like in the dict_phraseStim, but since not all participants require that, some that \n",
    "# do, are added to this new dictionary here\n",
    "dict_keySelection_phraseStim = {\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-16-15-18-0_1' : [4],        # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-03-07-16-44-5_2' : [1],        # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],              # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4]         # ys_session1\n",
    "}\n",
    "\n",
    "\n",
    "# in the beginning experiments, not everyone started with 800 initial dwell time\n",
    "\n",
    "dict_dwellTimeOrig_not800 = {\n",
    "    \"2019-01-16-15-51-13_2\" : 600, # no_session1\n",
    "    \"2019-01-16-15-18-0_1\" : 600, # no_session1\n",
    "    \"2019-01-16-15-43-8_1\" : 100, # af_session1\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : 100, # af_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : 100, # af_session1\n",
    "    \"2019-01-17-15-03-40_1\" : 100, # af_session2\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : 0, # af_session2\n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : 100, # af_session2\n",
    "    \"2019-01-14-15-07-21_1\" : 500, # ys_session1\n",
    "    \"2019-01-14-15-25-55_2\" : 300, # ys_session1\n",
    "    \"2019-01-16-15-18-50_1stpart_1\" : 200, # ys_session2\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 100, # ys_session2\n",
    "    \"2019-01-16-15-59-55_2\" : 100, # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : 100, # ys_session3\n",
    "    \"2019-01-17-15-31-12_2\" : 100 # ys_session3\n",
    "}\n",
    "\n",
    "\n",
    "# list of all things that should be present when computing effective time\n",
    "list_keysToBeCounted = ['Comma', 'BackOne', 'BackMany', 'SpaceBar']\n",
    "\n",
    "# some sessions do not have gaze data\n",
    "dict_noGazeData = {\n",
    "    '2019-01-16-17-00-12_2ndPart_2' : 'no gaze data', # af_session2\n",
    "    '2019-01-17-15-31-12_2' : 'no gaze data', #ys_session2\n",
    "    '2019-01-30-11-57-3_2' : 'no gaze data' # ys_session4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keySelection_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 3, 5], # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_WritingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2   \n",
    "}\n",
    "\n",
    "# normally, reading part of trial ends when people look at the keyboardwithphrases. For some trials, this is not done,\n",
    "# as the reading is done, and the trial is accidentally skipped, and written in the next trial. Here, the trial number \n",
    "# given will have the reading time ending as sleep, and not keyboard with phrases. \n",
    "dict_keyboardNotChange_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 0, # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_firstSleepNotCounted = {\n",
    "    \"2019-01-28-14-50-41_2\" : (0, 2), # bh1_session1 -- 3rd sleep activation to be counted\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : 2  # mn_session3 -- 3rd sleep activation is to be counted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeDwellOrig = 800\n",
    "TimeFixation = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixUserKeys(UserKeys_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    Column_beforeDecimal = [item[2] for item in UserKeys_Old]\n",
    "    Column_afterDecimal = [item[3] if len(item)>3 else '00' for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_ProgressPercent = [float(Column_beforeDecimal[i]+'.'+ Column_afterDecimal[i]) for i in \n",
    "                                range(0, len(Column_beforeDecimal))]\n",
    "    UserKeys_Times = [item[0] for item in UserKeys_Old]\n",
    "    UserKeys_Keys = [item[1] for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_New = [[UserKeys_Times[ind], UserKeys_Keys[ind], UserKeys_ProgressPercent[ind]] for ind in \n",
    "                    range(0, len(UserKeys_ProgressPercent))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    \n",
    "    return UserKeys_New\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixScratchPad(ScratchPad_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    ScratchPad_Times = [item[0] for item in ScratchPad_Old]\n",
    "    \n",
    "    ScratchPad_Phrases = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    ScratchPadInd = -1 \n",
    "    while ScratchPadInd < len(ScratchPad_Old)-1:\n",
    "        ScratchPadInd = ScratchPadInd + 1\n",
    "        commasInPhrase = len(ScratchPad_Old[ScratchPadInd])-2\n",
    "        if commasInPhrase < 1:\n",
    "            #print(ScratchPad_Old[ScratchPadInd][1])\n",
    "            ScratchPad_Phrases.append(ScratchPad_Old[ScratchPadInd][1])\n",
    "            continue\n",
    "        scratchPadPhrase = ScratchPad_Old[ScratchPadInd][1]\n",
    "        for phraseJoinNr in range(1, commasInPhrase+1):\n",
    "            scratchPadPhrase = scratchPadPhrase + ', ' + ScratchPad_Old[ScratchPadInd][1+phraseJoinNr]\n",
    "        \n",
    "        ScratchPad_Phrases.append(scratchPadPhrase)\n",
    "            \n",
    "        \n",
    "    ScratchPad_New = [[ScratchPad_Times[ind], ScratchPad_Phrases[ind]] for ind in \n",
    "                    range(0, len(ScratchPad_Times))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    #print(ScratchPad_New)\n",
    "    return ScratchPad_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixKeysSelected(KeysSelected_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    KeysSelected_New = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    KeysSelectedInd = -1 \n",
    "    while KeysSelectedInd < len(KeysSelected_Old)-1:\n",
    "        KeysSelectedInd = KeysSelectedInd + 1\n",
    "        \n",
    "        if KeysSelected_Old[KeysSelectedInd][1].count(',') > 0:\n",
    "            \n",
    "            keys_split = KeysSelected_Old[KeysSelectedInd][1].split(\"\\r\\n\")\n",
    "            del keys_split[0]\n",
    "            del keys_split[-1]\n",
    "            \n",
    "            keys_split = [key.split(',') for key in keys_split]\n",
    "            \n",
    "            KeysSelected_New.extend(keys_split)\n",
    "        else:\n",
    "            KeysSelected_New.append(KeysSelected_Old[KeysSelectedInd])\n",
    "        \n",
    "    \n",
    "    return KeysSelected_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDwellTime(userKeys, full_path):\n",
    "    # modify userKeys to include a column of time instead of progress pct, which is dependent on the then dwell time\n",
    "    \n",
    "    TimeDwellOrig = 800\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    if session_folder_name in dic_dwellTimeOrig_not800:\n",
    "        TimeDwellOrig = dic_dwellTimeOrig_not800[session_folder_name]\n",
    "    \n",
    "    #print(TimeDwellOrig)\n",
    "    \n",
    "    timeDwell = TimeDwellOrig\n",
    "    nKey = -1\n",
    "    for key in userKeys:\n",
    "        nKey = nKey + 1\n",
    "        #print(key[1])\n",
    "        if key[1] == 'IncreaseDwellTime':\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell + 100\n",
    "        elif key[1] == 'DecreaseDwellTime':\n",
    "            #print(key[2])\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell - 100\n",
    "        else:\n",
    "            userKeys[nKey].append(str(float(key[2])*timeDwell))\n",
    "    \n",
    "    return userKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stimPhrasesEdit(PhraseLog, full_path):\n",
    "   \n",
    "    # Now extract phrases from the phrase file\n",
    "    phraseStim_Phrases = [item[1] for item in PhraseLog]\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "        \n",
    "    phraseStim_PhrasesReduced, phraseStim_timeReduced = zip(*[(x[0], PhraseLog[phraseStim_Phrases.index(x[0])][0]) for x in groupby(phraseStim_Phrases)])\n",
    "    \n",
    "    PhraseLogReduced = [[phraseStim_timeReduced[i], phraseStim_PhrasesReduced[i]] for i in range(0, len(phraseStim_PhrasesReduced))]\n",
    "    \n",
    "    if PhraseLogReduced[-1][1] == 'THE EXPERIMENT IS NOW DONE':\n",
    "        del PhraseLogReduced[-1]\n",
    "        \n",
    "    if PhraseLogReduced[0][1] == 'phraseText':\n",
    "        del PhraseLogReduced[0]\n",
    "\n",
    "    # Here, we want only the sentences typed\n",
    "    notSentencesToType = list()\n",
    "    for index in range(0,len(PhraseLogReduced)):\n",
    "        sentence = PhraseLogReduced[index][1]\n",
    "        if 'Svar på følgende spørgsmål' in sentence or 'Answer the question:' in sentence or 'What is the complete name of your university?' in sentence or '(give a score between 1 and 7)' in sentence or sentence == '':\n",
    "            notSentencesToType.append(index)    \n",
    "    \n",
    "    for index in sorted(notSentencesToType, reverse=True):\n",
    "        del PhraseLogReduced[index]\n",
    "    \n",
    "    replacingList = []\n",
    "    PhraseLogReduced = findAndRemoveTrials(session_name, dict_phraseStim, PhraseLogReduced, replacingList)\n",
    "    \n",
    "    \n",
    "    return PhraseLogReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will return the datetime in items which is the closest to the date pivot\n",
    "def nearestTimePoint(dates, date):\n",
    "    \n",
    "    for d in dates:\n",
    "        if d < date:\n",
    "            nearestTP = d\n",
    "        else:\n",
    "            continue\n",
    "    try: \n",
    "        nearestTP\n",
    "        nearestTPind = dates.index(nearestTP)\n",
    "    except:\n",
    "        nearestTP = 0\n",
    "        nearestTPind = -1\n",
    "        \n",
    "    return nearestTP, nearestTPind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert list of date and time into datetime format list\n",
    "def timeConversion(timeStrList):\n",
    "    timeList = list()\n",
    "    for time in timeStrList:\n",
    "        time1, t1, t2 = time.partition('+')\n",
    "        timeList.append(datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\"))\n",
    "    return timeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptiKeyTypingTime(UserKeys):\n",
    "    \n",
    "    TimeTyping = dict()\n",
    "    \n",
    "    time1, t1, t2 = UserKeys[0][0].partition('+')\n",
    "    startTime = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "    \n",
    "    time2, t1, t2 = UserKeys[-1][0].partition('+')\n",
    "    endTime = datetime.datetime.strptime(re.sub('[:.T]','-',time2[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "    \n",
    "    TimeTyping['startTime'] = startTime\n",
    "    TimeTyping['endTime'] = endTime\n",
    "    \n",
    "    return TimeTyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeTypingStart(userKeys):\n",
    "    # From the user keys, find when the user actually starts typing, after having looked at the phrase and all the other \n",
    "    # function keys\n",
    "    \n",
    "    timeTypingStartInd = 0\n",
    "    \n",
    "    timeTypingStartIndList = list()\n",
    "            \n",
    "    timeUserTimeInd = 0\n",
    "    \n",
    "    ind = 0\n",
    "    # Get start time of first trial\n",
    "    \n",
    "    while ind < len(userKeys):\n",
    "        #print(len(userKeys[ind][1]))\n",
    "        if len(userKeys[ind][1]) > 1:\n",
    "            ind = ind + 1\n",
    "        else:\n",
    "            timeTypingStartInd = ind\n",
    "            timeTypingStartIndList.append(ind)\n",
    "            break\n",
    "    \n",
    "    #print(timeTypingStartInd)\n",
    "    # Get every next phrase start timings\n",
    "    while ind < len(userKeys):\n",
    "        \n",
    "        if userKeys[ind][1] == 'NextPhrase' and float(userKeys[ind][2]) == 1:\n",
    "            \n",
    "            #timeTypingStartIndList.append(ind+1)\n",
    "            for ind2 in range(ind+1, len(userKeys)):\n",
    "                if len(userKeys[ind2][1]) > 1:\n",
    "                    ind = ind + 1\n",
    "                    continue\n",
    "                elif userKeys[ind2][1] == 'NextPhrase' and float(userKeys[ind][2]) == 1:\n",
    "                    ind = ind + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    ind = ind2\n",
    "                    timeTypingStartIndList.append(ind)\n",
    "                    break\n",
    "                    \n",
    "        else:\n",
    "            ind = ind + 1\n",
    "            \n",
    "    #print(timeTypingStartIndList)\n",
    "    \n",
    "    return timeTypingStartIndList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAndRemoveTrials(session_name, dictionary_saved, trials, replacingList):\n",
    "    # function to check the session_name in the dictionary_saved and remove those trials from the dictionary_trial\n",
    "    \n",
    "    if session_name in dictionary_saved:\n",
    "        index_list = dictionary_saved[session_name]\n",
    "    else:\n",
    "        index_list = replacingList\n",
    "    \n",
    "    \n",
    "    if index_list:\n",
    "        if type(trials) == list:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials[index]\n",
    "                \n",
    "        else:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials['start'][index]\n",
    "                del trials['end'][index]\n",
    "        \n",
    "    return trials    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBlinks(pupilDataL, pupilDataR, timeInDatetime):\n",
    "    # filter any blinks and nan values lasting around 250ms (on average)\n",
    "    # first the single nan occurances are replaced with mean of the values on either sides, \n",
    "    # as they are assumed to be from hardware problems\n",
    "    # for the rest of the blinks, 250ms before and after the nan values are interpolated with a linear function\n",
    "    # returns a dataframe with pupil size, and timestamp\n",
    "    # http://faculty.washington.edu/chudler/facts.html\n",
    "   \n",
    "    \n",
    "    # create a dataframe from the pupilsize and time\n",
    "    pupilData_df = pd.DataFrame(list(zip(timeInDatetime, pupilDataL, pupilDataR)), columns=['timeStamp', 'pupilLeft', 'pupilRight'])\n",
    "    \n",
    "    # blink is every nan value in the range of 100-400ms \n",
    "    # 250 ms (22 samples) before and after the blink will also be removed\n",
    "    extraBlinkSamples = 22   \n",
    "    \n",
    "    \n",
    "    #pupilData_woSingleMissingData = pupilData.copy()\n",
    "    #timeList_woSingleMissingData = timeInDatetime.copy()\n",
    "    #timeInS_woSingleMissingData = timeInS_Trial[-1]\n",
    "    \n",
    "    # in case of single missing data, that are due to hardware error, replace with the mean of the pupil size before and\n",
    "    # after the nan value\n",
    "    # missing values will be the same for left and right pupil\n",
    "    missingVal_Single = np.argwhere(np.isnan(pupilDataL))\n",
    "    missingVal_Single = list(itertools.chain.from_iterable(missingVal_Single)) # flatten the list \n",
    "    \n",
    "    # if no blinks present, return the data\n",
    "    if len(missingVal_Single) == 0:\n",
    "        interpolatedNan = np.array([False]*len(pupilData_df))\n",
    "        return pupilData_df, interpolatedNan\n",
    "    \n",
    "    # find the index and values to replace for single nan values\n",
    "    pupilData_tuples_replaceSingleNan_left = [(val, np.mean([pupilDataL[val-1], pupilDataL[val+1]])) for i, val in enumerate(missingVal_Single) if (val != 0 and val != (len(pupilDataL)-1)) if not np.isnan(pupilDataL[val-1]) and not np.isnan(pupilDataL[val+1])]\n",
    "    pupilData_tuples_replaceSingleNan_right = [(val, np.mean([pupilDataR[val-1], pupilDataR[val+1]])) for i, val in enumerate(missingVal_Single) if (val != 0 and val != (len(pupilDataR)-1)) if not np.isnan(pupilDataR[val-1]) and not np.isnan(pupilDataR[val+1])]\n",
    "    \n",
    "    \n",
    "    interpolatedNan = np.array([True if ind in dict(pupilData_tuples_replaceSingleNan_left) else False for ind, val in enumerate(pupilDataL)])\n",
    "    \n",
    "    \n",
    "    # replace the single nan values with the mean of the pupil size on either sides\n",
    "    indList = -1\n",
    "    for ind, val in pupilData_tuples_replaceSingleNan_left:\n",
    "        indList = indList + 1\n",
    "        pupilData_df.iloc[ind, pupilData_df.columns.get_loc('pupilLeft')] = val\n",
    "        pupilData_df.iloc[ind, pupilData_df.columns.get_loc('pupilRight')] = pupilData_tuples_replaceSingleNan_right[indList][1]\n",
    "        \n",
    "    \n",
    "    # again, find the nan values in the pupil size\n",
    "    # the list missingVal_SingleDifference contains the index of the first blink, followed by the difference in the index\n",
    "    # to the next nan value\n",
    "    \n",
    "    \n",
    "    \n",
    "    # find the nan values again from pupilData['pupilLeft']\n",
    "    missingVal_Rest_trueFalse = pupilData_df['pupilLeft'].isnull()\n",
    "    missingVal_Rest = [i for i, x in enumerate(missingVal_Rest_trueFalse) if x]\n",
    "    \n",
    "    # if no blinks left, return the current pupilData\n",
    "    if len(missingVal_Rest) == 0:\n",
    "        return pupilData_df, interpolatedNan\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # in the blinks left, find when the blinks start by finding a difference in the consecutive values of the indices\n",
    "    missingVal_RestDifference = [t - s for s, t in zip(missingVal_Rest, missingVal_Rest[1:])]\n",
    "    missingVal_RestDifference.insert(0, missingVal_Rest[0])\n",
    "    \n",
    "    blinkStart_tupleList = [(ind, sum(missingVal_RestDifference[0:ind+1])) for ind, val in enumerate(missingVal_RestDifference) if val != 1]\n",
    "    \n",
    "    blinkStart_tupleList_wLength = list()\n",
    "    \n",
    "    # create a list of tuples of blink start index and the length of the blink\n",
    "    ind = -1\n",
    "    blinkLengthSum = 0\n",
    "    for blink_ind, blinkStartInd in blinkStart_tupleList:\n",
    "        ind = ind + 1\n",
    "        if ind != len(blinkStart_tupleList) - 1:\n",
    "            \n",
    "            blinkLength = blinkStart_tupleList[ind+1][0]-blink_ind\n",
    "            blinkLengthSum = blinkLengthSum + blinkLength\n",
    "            \n",
    "            blinkStart_tupleList_wLength.append(tuple((blinkStartInd, blinkLength)))\n",
    "        else:\n",
    "            # for the last blink -- all blink lengths summed and subtracted from the length of the list\n",
    "            # missingVal_RestDifference \n",
    "            blinkLength = len(missingVal_RestDifference)-blinkLengthSum\n",
    "            blinkStart_tupleList_wLength.append(tuple((blinkStartInd, blinkLength)))\n",
    "     \n",
    "    # add to vector with start and end of tuple\n",
    "    #beforeAfterNan = [False]*len(pupilData_df['pupilLeft'])\n",
    "    #for blinkStart, blinkLength in blinkStart_tupleList_wLength:\n",
    "    #    beforeAfterNan[blinkStart] = True\n",
    "    #    beforeAfterNan[blinkStart+blinkLength] = True\n",
    "    #    #print('start and end points: ', pupilData_df['timeStamp'][blinkStart], pupilData_df['timeStamp'][blinkStart + blinkLength])\n",
    "    \n",
    "    \n",
    "    # create lists with start and end values for the blinks, based on blinkStart_tupleList_wLength, regardless of the blink length\n",
    "    blink_missingData_startList = [blinkStartInd - extraBlinkSamples if (blinkStartInd - extraBlinkSamples) > 0 else 0 for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    blink_missingData_endList = [blinkStartInd + blinkLength + extraBlinkSamples if (blinkStartInd + blinkLength + extraBlinkSamples) < (len(pupilData_df['pupilLeft'])-1) else (len(pupilData_df['pupilLeft'])-1) for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    # create a list of tuples from the start and end points of the blink\n",
    "    blink_missingData_startEndTuple = [(blinkStart, blink_missingData_endList[ind]) for ind, blinkStart in enumerate(blink_missingData_startList)] \n",
    "    \n",
    "    \n",
    "    # check if blinks need to be combined - blinksCombine is a list of list of 2 elements, the index of the blinks that should be combined\n",
    "    blinksCombine = [[ind, ind+1] for ind, blink in enumerate(blink_missingData_startEndTuple[0:-1]) if blink[1] > blink_missingData_startEndTuple[ind+1][0]]\n",
    "        \n",
    "    if blinksCombine:\n",
    "        # combine blinks that need to be combined - if multiple consecutive blinks need to be removed: eg - [1, 2], [2, 3] \n",
    "        # are included in the blinksCombine, the combined version should be [1, 3] \n",
    "        blinksCombineFinal = list()\n",
    "        ind = -1\n",
    "        while ind < len(blinksCombine)-2:\n",
    "            \n",
    "            ind = ind + 1\n",
    "            blinkCombining = blinksCombine[ind]\n",
    "            blinksCombineFinal.append(blinkCombining)\n",
    "            while ind < len(blinksCombine)-2 and blinkCombining[1] == blinksCombine[ind+1][0]:\n",
    "                # change the ending of the last added blink of blinksCombineFinal\n",
    "                blinksCombineFinal[-1][1] = blinksCombine[ind+1][1]\n",
    "                ind = ind + 1\n",
    "            \n",
    "            \n",
    "        if len(blinksCombine) == 1:\n",
    "            blinksCombineFinal = blinksCombine.copy()\n",
    "            \n",
    "        \n",
    "        if blinksCombine[-1][1] != blinksCombineFinal[-1][1]:\n",
    "            if blinksCombine[-1][0] == blinksCombineFinal[-1][1]:\n",
    "                blinksCombineFinal[-1][1] = blinksCombine[-1][1]\n",
    "            else:\n",
    "                blinksCombineFinal.append(blinksCombine[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #    for w, z in groupby(sorted(list(blinksCombine)), lambda x, y=itertools.count(): next(y)-x):\n",
    "    #        group = list(z)\n",
    "    #        blinksCombineFinal.append(tuple((group[0], group[-1])))\n",
    "        \n",
    "        for x in sorted(blinksCombineFinal, reverse=True):\n",
    "            new_start = blink_missingData_startEndTuple[x[0]][0] \n",
    "            new_end = blink_missingData_startEndTuple[x[1]][1] \n",
    "            \n",
    "            x_start = x[0]\n",
    "            x_end = x[1]\n",
    "            \n",
    "            # delete also the blinkStart_tupleList_wLength, since it is going to be used to compute other metrics\n",
    "            for blinkRemove in range(x[1], x[0]-1, -1):\n",
    "                del blink_missingData_startEndTuple[blinkRemove]\n",
    "            \n",
    "            blink_missingData_startEndTuple.insert(x[0], tuple((new_start, new_end)))\n",
    "    \n",
    "    \n",
    "    #blinkAndNonBlinkDurationList = [length/90 for start, length in blinkStart_tupleList_wLength]\n",
    "    #timeInS_Trial_filter = timeInS_Trial[-1] - sum(blinkAndNonBlinkDurationList) \n",
    "    \n",
    "    \n",
    "    # remove blinks from data\n",
    "    for blinkStart, blinkEnd in blink_missingData_startEndTuple:\n",
    "        pupilData_df.loc[blinkStart:blinkEnd,'pupilLeft'] = np.nan\n",
    "        pupilData_df.loc[blinkStart:blinkEnd,'pupilRight'] = np.nan\n",
    "        replaceTrueList = range(blinkStart, blinkEnd+1, 1)\n",
    "        interpolatedNan[replaceTrueList] = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    pupilData_df['pupilLeft'] = pupilData_df['pupilLeft'].astype(float).interpolate('linear', limit_direction = 'both')\n",
    "    pupilData_df['pupilRight'] = pupilData_df['pupilRight'].astype(float).interpolate('linear', limit_direction = 'both')\n",
    "    \n",
    "    if pupilData_df.isnull().any().any():\n",
    "        print('nan values in filtered data')\n",
    "        #for i,val in enumerate(pupilData_filter[0:5000]):\n",
    "        #    print(i, val, pupilData_woSingleMissingData[i])\n",
    "        \n",
    "    \n",
    "    return pupilData_df, interpolatedNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel(dvec, radius=5, nsigma=3, rem_nomed=False):\n",
    "\n",
    "    # replace outliers with median values (hampel filter)\n",
    "    \n",
    "    mvec = pd.Series(dvec).rolling(radius*2+1, center=True, min_periods=radius).median()\n",
    "    svec = 1.4862 * np.abs(dvec-mvec).rolling(radius*2+1, center=True, min_periods=radius).median()\n",
    "    plonk = np.abs(dvec-mvec) > nsigma*svec\n",
    "    dvec = np.array(dvec)\n",
    "    dvec[plonk.tolist()] = mvec[plonk.tolist()]\n",
    "\n",
    "    # remove \"bad data\" where we cannot calculate a median value due to already missing values\n",
    "    if (rem_nomed):\n",
    "        dvec[np.isnan(mvec)] = np.nan\n",
    "    return dvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterPupilSize(GazeLog, TimeTyping, subjectAndSessionName):\n",
    "    # function that uses the list of start and end trial times to find the pupil sizes for those trials and plots them\n",
    "    \n",
    "    # first create a list of times in gaze log\n",
    "    timeStrGazeLog = [item3[0] for item3 in GazeLog]\n",
    "    # convert the list of strings to datetime formats\n",
    "    timeGazeLog = timeConversion(timeStrGazeLog)\n",
    "    \n",
    "    # internal time, to depict seconds\n",
    "    timeInternalGazeLog = [float(item3[1]) for item3 in GazeLog]\n",
    "    \n",
    "    # extract pupil sizes in decimals from the strange 2 columns for every pupil\n",
    "    pupilLogL_raw, pupilLogR_raw = Convert2ColumnSizesTo1(GazeLog, -5, -4, -2, -1)\n",
    "    \n",
    "    \n",
    "    # reduce the data to start and end of typing time\n",
    "    timeTyping_start, timeTyping_startInd = nearestTimePoint(timeGazeLog, TimeTyping['startTime'])\n",
    "    timeTyping_end, timeTyping_endInd = nearestTimePoint(timeGazeLog, TimeTyping['endTime'])\n",
    "    \n",
    "    pupilLogL_wDefinedTime = pupilLogL_raw[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    pupilLogR_wDefinedTime = pupilLogR_raw[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    \n",
    "    timeGazeLog_wDefinedTime = timeGazeLog[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    \n",
    "    timeInS_GazeLog_wDefinedTime = timeInternalGazeLog[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    timeInS_Difference = [(t - s)/1000000 for s, t in zip(timeInS_GazeLog_wDefinedTime, timeInS_GazeLog_wDefinedTime[1:])]\n",
    "    timeInS_Difference.insert(0, 0)\n",
    "    \n",
    "    \n",
    "    timeInS = [sum(timeInS_Difference[:i]) for i, v in enumerate(timeInS_Difference)]\n",
    "    \n",
    "    pupilData_df, interpolated_items = filterBlinks(pupilLogL_wDefinedTime, pupilLogR_wDefinedTime, timeGazeLog_wDefinedTime)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #timeGazeLog_plot = np.arange(0, timeInS[-1], 1/90)\n",
    "    \n",
    "    #plotPupilSize_checkFilter(pupilData_df, pupilLogL_wDefinedTime, blinkStartAndEnd, 'blink removal', subjectAndSessionName)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pupilData_df_hampel = dict()\n",
    "    \n",
    "    pupilData_df_hampel = pupilData_df.copy()\n",
    "    pupilData_df_hampel['pupilLeft'] = hampel(pupilData_df['pupilLeft'], 25, 3, False)\n",
    "    pupilData_df_hampel['pupilRight'] = hampel(pupilData_df['pupilRight'], 25, 3, False)\n",
    "        \n",
    "    \n",
    "        \n",
    "    return pupilData_df_hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindTrialTimes(KeysSelected, timeTyping, full_path):\n",
    "    # function to find start and end of tasks in experiments\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    timeTrialDict = dict()\n",
    "    timeTrialDict = {'start': [],\n",
    "                    'end':[]}\n",
    "    \n",
    "    nTrial = -1\n",
    "    \n",
    "    for keys in KeysSelected:\n",
    "        \n",
    "            \n",
    "        \n",
    "        if keys[1] == 'NextPhrase':\n",
    "            nTrial = nTrial + 1\n",
    "            time1, t1, t2 = keys[0].partition('+')\n",
    "            endTimeTrial = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "            \n",
    "            if nTrial != 0:\n",
    "                # print('end: ', endTimeTrial)\n",
    "                #print('')\n",
    "                timeTrialDict['end'].append(endTimeTrial)\n",
    "            \n",
    "            \n",
    "            # add 5s for the start time of the next phrase\n",
    "            seconds_start = keys[0][17:19]\n",
    "            \n",
    "            if int(seconds_start) > 54:\n",
    "                minute_start = keys[0][14:16]\n",
    "                seconds_start_new = str(int(seconds_start) - 55)\n",
    "            \n",
    "                if int(minute_start) > 58:\n",
    "                    minute_start_new = str(int(minute_start) - 59)\n",
    "                    hour_start_new = str(int(keys[0][11:13]) + 1)\n",
    "                        \n",
    "                else:\n",
    "                    minute_start_new = str(int(minute_start) + 1)\n",
    "                    hour_start_new = str(int(keys[0][11:13]))\n",
    "                        \n",
    "            else:\n",
    "                seconds_start_new = str(int(seconds_start) + 5)\n",
    "                minute_start_new = str(keys[0][14:16])\n",
    "                hour_start_new = str(int(keys[0][11:13]))\n",
    "                    \n",
    "            endTimew5s = keys[0][0:11] + hour_start_new + ':' + minute_start_new + ':' + seconds_start_new + keys[0][19:]\n",
    "            \n",
    "            time1, t1, t2 = endTimew5s.partition('+')\n",
    "            startTimeTrial = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "            \n",
    "            #print('start: ', startTimeTrial)\n",
    "            timeTrialDict['start'].append(startTimeTrial)\n",
    "        \n",
    "    del timeTrialDict['start'][-1]\n",
    "    \n",
    "    \n",
    "    # remove the extra selections of NewPhrase at the end of some sessions\n",
    "    replacingList = []\n",
    "    timeTrialDict = findAndRemoveTrials(session_folder_name, dict_keySelectionOfNextPhrase, timeTrialDict, replacingList)\n",
    "    \n",
    "    timeTrialDict_copy = copy.deepcopy(timeTrialDict)\n",
    "    \n",
    "    # separate the reading and writing trials for some participants who read in the actual trial, but write in the next\n",
    "    # trial\n",
    "    if session_folder_name in dict_keySelection_ReadingTrials:\n",
    "        # check the reading and writing separate dictionaries\n",
    "        print('reading and writing sessions are separate')\n",
    "        \n",
    "        #print(len(timeTrialDict['start']))\n",
    "        # writing trials - \n",
    "        timeTrialDict_writing = findAndRemoveTrials(session_folder_name, dict_keySelection_WritingTrials, timeTrialDict, replacingList)\n",
    "        #print(len(timeTrialDict_copy['start']))\n",
    "        \n",
    "        # reading trials\n",
    "        timeTrialDict_reading = findAndRemoveTrials(session_folder_name, dict_keySelection_ReadingTrials, timeTrialDict_copy, replacingList)\n",
    "    else:\n",
    "        # some participants skip some sentences, and then it affects the scoreQuestions too. Remove the skipped sentences or \n",
    "        # remove the score questions \n",
    "        # for these participants, reading and writing trials are the same\n",
    "        \n",
    "        print('same reading and writing trials')\n",
    "        scoreQuestions = [0, 1, 3, 5, 7, 9, 11]\n",
    "        timeTrialDict = findAndRemoveTrials(session_folder_name, dict_keySelectionNotCompleted, timeTrialDict, scoreQuestions)\n",
    "        \n",
    "        # most of the skipped sentences are removed, but for those that are not removed\n",
    "        timeTrialDict_writing = findAndRemoveTrials(session_folder_name, dict_keySelection_phraseStim, timeTrialDict, replacingList)\n",
    "        \n",
    "        timeTrialDict_reading = timeTrialDict_writing \n",
    "        \n",
    "            \n",
    "    return timeTrialDict_reading, timeTrialDict_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivideTimeIntoTrials(PupilData, TimeEpochTrial):\n",
    "    # use the dictionary of TimeEpochTrial to find the divide between trials\n",
    "    \n",
    "    EventTrials = dict()\n",
    "    EventTrials['start'] =  [False]*len(PupilData['pupilLeft'])\n",
    "    EventTrials['end'] = [False]*len(PupilData['pupilLeft'])\n",
    "    \n",
    "    \n",
    "    for trialNr in range(0, len(TimeEpochTrial['start'])):\n",
    "        \n",
    "        # find start and end time in gazeLog\n",
    "        timeStart_trial, timeStartInd_trial = nearestTimePoint(PupilData['timeStamp'].tolist(), TimeEpochTrial['start'][trialNr])\n",
    "        timeEnd_trial, timeEndInd_trial = nearestTimePoint(PupilData['timeStamp'].tolist(), TimeEpochTrial['end'][trialNr])\n",
    "        \n",
    "        EventTrials['start'][timeStartInd_trial] = True\n",
    "        EventTrials['end'][timeEndInd_trial] = True\n",
    "        \n",
    "    EventTrials_index = dict()\n",
    "    EventTrials_index['start'] = [i for i, x in enumerate(EventTrials['start']) if x] \n",
    "    EventTrials_index['end'] = [i for i, x in enumerate(EventTrials['end']) if x] \n",
    "            \n",
    "    return EventTrials, EventTrials_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindReadingPartsOfTrial(EventTrials_reading, KeysSelected_new, PupilSize_df):\n",
    "    \n",
    "    KeysSelected_timeStr = [key[0] for key in KeysSelected_new]\n",
    "    KeysSelected_time = timeConversion(KeysSelected_timeStr)\n",
    "    \n",
    "    KeysSelected_keys = [key[1] for key in KeysSelected_new]\n",
    "    \n",
    "    EventReading = dict()    \n",
    "    EventReading['start'] = list()\n",
    "    EventReading['end'] = list()\n",
    "    \n",
    "    EventReading_index = dict()    \n",
    "    EventReading_index['start'] = list()\n",
    "    EventReading_index['end'] = list()\n",
    "    \n",
    "    for ind, startTimeInd in enumerate(EventTrials_reading['start']):\n",
    "        startTimeTrial = PupilSize_df['timeStamp'][startTimeInd]\n",
    "        endTimeTrial = PupilSize_df['timeStamp'][EventTrials_reading['end'][ind]]\n",
    "        \n",
    "        # reading start is the same as trial start\n",
    "        EventReading['start'].append(startTimeTrial)\n",
    "        EventReading_index['start'].append(startTimeInd)\n",
    "        \n",
    "        # start and end time of trial, with respect to the keysSelected\n",
    "        startTrial_keysTime, startTrial_keysInd = nearestTimePoint(KeysSelected_time, startTimeTrial)\n",
    "        endTrial_keysTime, endTrial_keysInd = nearestTimePoint(KeysSelected_time, endTimeTrial)\n",
    "        \n",
    "        # create keysSelected_trial - keys selected in the trial, to make it easier to find the Sleep button selection \n",
    "        # after the trial start\n",
    "        keysSelected_trial = KeysSelected_keys[startTrial_keysInd:endTrial_keysInd]\n",
    "        \n",
    "        # compute the end of reading time -- which is when the sleep button is selected\n",
    "        endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('Sleep')])\n",
    "        \n",
    "        #print(KeysSelected_keys[startTrial_keysInd + keysSelected_trial.index('Sleep')], KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('Sleep')], endReading_gazeTime, PupilSize_df['timeStamp'][endReading_gazeInd+1])\n",
    "        \n",
    "        # add the end time of reading, but the next element after the sleep selection matches gaze, to allow complete \n",
    "        # sleep selection\n",
    "        \n",
    "        EventReading['end'].append(PupilSize_df['timeStamp'][endReading_gazeInd+1])\n",
    "        EventReading_index['end'].append(endReading_gazeInd+1)\n",
    "        \n",
    "        print(ind)\n",
    "        print('reading: ', EventReading['start'][ind], EventReading['end'][ind])\n",
    "        \n",
    "    return EventReading, EventReading_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindWritingPartsOfTrial(EventTrials_writing, KeysSelected_new, PupilSize_df, EventReading_index):\n",
    "    \n",
    "    KeysSelected_timeStr = [key[0] for key in KeysSelected_new]\n",
    "    KeysSelected_time = timeConversion(KeysSelected_timeStr)\n",
    "    \n",
    "    KeysSelected_keys = [key[1] for key in KeysSelected_new]\n",
    "    \n",
    "    EventWriting = dict()    \n",
    "    EventWriting['start'] = list()\n",
    "    EventWriting['end'] = list()\n",
    "    \n",
    "    EventWriting_index = dict()    \n",
    "    EventWriting_index['start'] = list()\n",
    "    EventWriting_index['end'] = list()\n",
    "    \n",
    "    for ind, startTimeInd in enumerate(EventTrials_writing['start']):\n",
    "        startTimeTrial = PupilSize_df['timeStamp'][startTimeInd]\n",
    "        endTimeTrial = PupilSize_df['timeStamp'][EventTrials_writing['end'][ind]]\n",
    "        \n",
    "        endTimeReading = PupilSize_df['timeStamp'][EventReading_index['end'][ind]]\n",
    "        \n",
    "        #print(endTimeReading)\n",
    "        \n",
    "        # for some participants, reading and writing trials are different. So their writing times will not be the end of \n",
    "        # the reading time.\n",
    "        # Regardless, the writing time should start later than when the reading time ends.\n",
    "        # We choose the starting time for writing as the one that is later than the start time from writing trials\n",
    "        # and end time from reading trials\n",
    "        \n",
    "        if startTimeTrial > endTimeReading:\n",
    "            EventWriting['start'].append(startTimeTrial)\n",
    "            EventWriting_index['start'].append(startTimeInd)\n",
    "        else:\n",
    "            EventWriting['start'].append(endTimeReading)\n",
    "            EventWriting_index['start'].append(EventReading_index['end'][ind])\n",
    "        \n",
    "        EventWriting['end'].append(endTimeTrial)\n",
    "        EventWriting_index['end'].append(EventTrials_writing['end'][ind])\n",
    "        \n",
    "        print(ind)\n",
    "        print('writing: ', EventWriting['start'][ind], EventWriting['end'][ind])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return EventWriting, EventWriting_index     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotPupilSizeWithEvents(PupilData_df, Event1_index, Event2_index, PlotTitle, SubjectAndSessionName, eventNumbers):\n",
    "    \n",
    "    subjectID = SubjectAndSessionName.split('__')[0]\n",
    "    \n",
    "    print(subjectID)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    leftPlot = ax.plot(PupilData_df['timeStamp'], PupilData_df['pupilLeft'], 'b', label = 'Left')\n",
    "    rightPlot = ax.plot(PupilData_df['timeStamp'], PupilData_df['pupilRight'], 'r', label = 'Right')\n",
    "    \n",
    "    for start in Event1_index['start']:\n",
    "        event1_start = ax.plot([PupilData_df['timeStamp'][start], PupilData_df['timeStamp'][start]], [np.min(PupilData_df['pupilLeft']), np.max(PupilData_df['pupilLeft'])], color = 'orange')\n",
    "    \n",
    "    for end in Event1_index['end']:\n",
    "        event1_end = ax.plot([PupilData_df['timeStamp'][end], PupilData_df['timeStamp'][end]], [np.min(PupilData_df['pupilLeft']), np.max(PupilData_df['pupilLeft'])], color = 'pink')\n",
    "    \n",
    "    ax.set_ylabel('Absolute pupil size [in mm]')\n",
    "    ax.set_xlabel('Time [in s]')\n",
    "    \n",
    "    plt.title('plot {}  for subject  {}'.format(PlotTitle, SubjectAndSessionName))\n",
    "    \n",
    "    if eventNumbers > 1:\n",
    "        for start in Event2_index['start']:\n",
    "            event2_start = ax.plot([PupilData_df['timeStamp'][start], PupilData_df['timeStamp'][start]], [np.min(PupilData_df['pupilLeft']), np.max(PupilData_df['pupilLeft'])], color = 'green')\n",
    "    \n",
    "        for end in Event2_index['end']:\n",
    "            event2_end = ax.plot([PupilData_df['timeStamp'][end], PupilData_df['timeStamp'][end]], [np.min(PupilData_df['pupilLeft']), np.max(PupilData_df['pupilLeft'])], color = 'gray')\n",
    "    \n",
    "    #plt.legend((leftPlot, rightPlot, event1_start, event1_end, event2_start, event2_end), ('Left pupil', 'Right pupil', 'Reading start', 'Reading end', 'Writing start', 'Writing End'))\n",
    "    plt.legend([leftPlot[0], rightPlot[0], event1_start[0], event1_end[0], event2_start[0], event2_end[0]], ('Left pupil', 'Right pupil', 'Reading start', 'Reading end', 'Writing start', 'Writing End'))\n",
    "    \n",
    "    \n",
    "    pickle.dump(fig, open(r'C:\\DTU\\Results\\201901_Expt' + '\\\\Expt_w' + plotTitle + '\\\\' + subjectID + '\\\\' + 'pupilSize_CompleteExperiment_w' + PlotTitle + '_' + SubjectAndSessionName, 'wb'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gazeConvert2ColumnsTo1(GazeLog, columnIndwValidity_list, indValidity):\n",
    "    # function to convert pupilsizes from 2 columns for every pupil due to comma use instead of decimal, \n",
    "    # to proper pupil sizes\n",
    "    \n",
    "    #columnInd_list = [joinColumn1_1, joinColumn1_2, joinColumn2_1, joinColumn2_2]\n",
    "    \n",
    "    # number of columns in the final dictionary\n",
    "    nColumns = int(len(columnIndwValidity_list)/2)\n",
    "    \n",
    "    # dictionary of columns that are to be joined later\n",
    "    columns_beforeDecimal = dict()\n",
    "    columns_afterDecimal = dict()\n",
    "    \n",
    "    # dictionary of joined columns\n",
    "    columnsFinal = dict()\n",
    "    \n",
    "    # dictionary to find and equalize missing values in every column\n",
    "    missingVal_column = dict()\n",
    "    missingVal = list()\n",
    "    \n",
    "    # find correct index of validity column to be used, to find the actual columns relative to that\n",
    "    columnsValidity_inUse = list()\n",
    "    \n",
    "    for ind, row in enumerate(GazeLog):\n",
    "        #print(ind)\n",
    "        #print(sorted(list(np.where(np.array(row) == 'Valid')[0])+list(np.where(np.array(row)=='Invalid')[0]))[indValidity])\n",
    "\n",
    "        columnsValidity = (sorted(list(np.where(np.array(row) == 'Valid')[0])+list(np.where(np.array(row)=='Invalid')[0]))[indValidity])\n",
    "        columnsValidity_inUse.append(int(columnsValidity))\n",
    "    \n",
    "    columnsValidity_inUse = np.array(columnsValidity_inUse)\n",
    "    \n",
    "    columnInd_list = [[columnsValidity_inUse+i] for i in columnIndwValidity_list]\n",
    "    \n",
    "    for ind in range(0, nColumns):\n",
    "        \n",
    "        dict_name = 'column' + str(ind+1)\n",
    "        columnsFinal[dict_name] = list()\n",
    "        columns_afterDecimal[dict_name] = list()\n",
    "                \n",
    "        #for indItem, item4 in enumerate(GazeLog):\n",
    "        #    if 'Invalid' not in item4:\n",
    "        #        if columnInd_list[2*ind+1][0][indItem] < len(item4):\n",
    "        #            columns_afterDecimal[dict_name].append(item4[columnInd_list[2*ind+1][0][indItem]])\n",
    "        #        else:\n",
    "        #            columns_afterDecimal[dict_name].append('0')\n",
    "        #    else:\n",
    "        #        columns_afterDecimal[dict_name].append('nan')\n",
    "        \n",
    "                \n",
    "        columns_beforeDecimal[dict_name] = [item4[columnInd_list[2*ind][0][indItem]] if 'Invalid' not in item4 else 'nan' for indItem, item4 in enumerate(GazeLog)]\n",
    "        columns_afterDecimal[dict_name] = [item4[columnInd_list[2*ind+1][0][indItem]] if 'Invalid' not in item4 and columnInd_list[2*ind+1][0][indItem] < len(item4) else 'nan' for indItem, item4 in enumerate(GazeLog)]\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(columns_beforeDecimal[dict_name])):\n",
    "            if 'Valid' not in columns_beforeDecimal[dict_name][i] and 'Valid' not in columns_afterDecimal[dict_name][i]:\n",
    "                if 'nan' not in columns_beforeDecimal[dict_name][i] and 'nan' not in columns_afterDecimal[dict_name][i]:\n",
    "                    if float(columns_afterDecimal[dict_name][i]) > 0: \n",
    "                        columnsFinal[dict_name].append(float(columns_beforeDecimal[dict_name][i]+'.'+columns_afterDecimal[dict_name][i]))\n",
    "                    else:\n",
    "                        columnsFinal[dict_name].append(np.nan)\n",
    "                else:\n",
    "                    columnsFinal[dict_name].append(np.nan)\n",
    "            else:\n",
    "                # Rarely, the pupil size is a whole number\n",
    "                columnsFinal[dict_name].append(np.nan) # we will ignore the row, since there is no way of automatically knowing which - \n",
    "                # right or left eye has whole number pupil size\n",
    "    \n",
    "        missingVal_column[dict_name] = np.argwhere(np.isnan(columnsFinal[dict_name]))\n",
    "        missingVal_column[dict_name] = list(itertools.chain.from_iterable(missingVal_column[dict_name])) # flatten the list\n",
    "        \n",
    "        missingVal.extend(missingVal_column[dict_name])\n",
    "        \n",
    "        \n",
    "    \n",
    "    missingVal = sorted(set(missingVal))\n",
    "    \n",
    "    \n",
    "    # if one of the columns are nan, the other one is converted too\n",
    "    for column in range(0, nColumns):\n",
    "        dict_name = 'column' + str(column+1)\n",
    "        for ind in missingVal:\n",
    "            if ind < len(columnsFinal[dict_name]):\n",
    "                columnsFinal[dict_name][ind] = np.nan\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(len(columnsFinal['column1']), len(columnsFinal['column2']))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return columnsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_velocity(x, y, t):\n",
    "    dx, dy, dt_timeDelta = np.diff(x), np.diff(y), np.diff(t)\n",
    "    dt = np.array([time.total_seconds() if time.total_seconds() != 0 else 0.01 for time in dt_timeDelta])\n",
    "    vx = dx/dt;\n",
    "    vy = dy/dt;\n",
    "\n",
    "    dr = np.sqrt(vx*vx+vy*vy);\n",
    "    return dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saccadeDetectionVelocity(GazeLog, subjAndSessionName):\n",
    "    # detect saccades \n",
    "    \n",
    "    # Position of eyes in the UCS by Tobii (in mm)\n",
    "    print('gazeOrigin')\n",
    "    gazeOriginUCS_validityL = 0\n",
    "    gazeOriginUCS_indWrtValidityL = [1, 2, 3, 4, 5, 6]\n",
    "    gazeOriginUCS_Left = gazeConvert2ColumnsTo1(GazeLog, gazeOriginUCS_indWrtValidityL, gazeOriginUCS_validityL)\n",
    "    \n",
    "    gazeOriginUCS_validityR = 1\n",
    "    gazeOriginUCS_indWrtValidityR = [1, 2, 3, 4, 5, 6]\n",
    "    gazeOriginUCS_Right = gazeConvert2ColumnsTo1(GazeLog, gazeOriginUCS_indWrtValidityR, gazeOriginUCS_validityR)\n",
    "    \n",
    "    # find the average of each to compute the gaze origin\n",
    "    gazeOriginUCS = dict()\n",
    "    gazeOriginUCS['x'] = np.array([(v+gazeOriginUCS_Right['column1'][i])/2 for i, v in enumerate(gazeOriginUCS_Left['column1'])])\n",
    "    gazeOriginUCS['y'] = np.array([(v+gazeOriginUCS_Right['column2'][i])/2 for i, v in enumerate(gazeOriginUCS_Left['column2'])])\n",
    "    gazeOriginUCS['z'] = np.array([(v+gazeOriginUCS_Right['column3'][i])/2 for i, v in enumerate(gazeOriginUCS_Left['column3'])])\n",
    "    \n",
    "    \n",
    "    # Location of eye gaze on screen in the UCS by Tobii (in mm)\n",
    "    print('gazePoint')\n",
    "    gazePointUCS_validityL = 2\n",
    "    gazePointUCS_indWrtValidityL = [1, 2, 3, 4, 5, 6]\n",
    "    gazePointUCS_Left = gazeConvert2ColumnsTo1(GazeLog, gazePointUCS_indWrtValidityL, gazePointUCS_validityL)\n",
    "    \n",
    "    gazePointUCS_validityR = 3\n",
    "    gazePointUCS_indWrtValidityR = [1, 2, 3, 4, 5, 6]\n",
    "    gazePointUCS_Right = gazeConvert2ColumnsTo1(GazeLog, gazePointUCS_indWrtValidityR, gazePointUCS_validityR)\n",
    "    \n",
    "    # find the average of gaze point in UCS\n",
    "    gazePointUCS = dict()\n",
    "    gazePointUCS['x'] = np.array([(v+gazePointUCS_Right['column1'][i])/2 for i, v in enumerate(gazePointUCS_Left['column1'])])\n",
    "    gazePointUCS['y'] = np.array([(v+gazePointUCS_Right['column2'][i])/2 for i, v in enumerate(gazePointUCS_Left['column2'])])\n",
    "    gazePointUCS['z'] = np.array([(v+gazePointUCS_Right['column3'][i])/2 for i, v in enumerate(gazePointUCS_Left['column3'])])\n",
    "    \n",
    "    # find the distance between the screen and eyes, dividing by 10 to get cm from mm \n",
    "    distanceEyeGaze = [(np.sqrt((gazePointUCS['x'][i]-gazeOriginUCS['x'][i])**2 + (gazePointUCS['y'][i]-gazeOriginUCS['y'][i])**2 + (gazePointUCS['z'][i]-gazeOriginUCS['z'][i])**2))/10 for i, v in enumerate(gazePointUCS['x'])]\n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.plot(distanceEyeGaze)\n",
    "    #plt.title('distance between eye and point of gaze')\n",
    "    \n",
    "    # Location of eye gaze on screen in the ADCS by Tobii (in arbitrary units)\n",
    "    print('gaze point on screen')\n",
    "    gazePointADCS_indWrtValidityL = [-4, -3, -2, -1]\n",
    "    gazePointADCS_validityL = 3\n",
    "    gazePointADCS_Left_au = gazeConvert2ColumnsTo1(GazeLog, gazePointADCS_indWrtValidityL, gazePointADCS_validityL)\n",
    "    \n",
    "    \n",
    "    gazePointADCS_indWrtValidityR = [-4, -3, -2, -1]\n",
    "    gazePointADCS_validityR = 4\n",
    "    gazePointADCS_Right_au = gazeConvert2ColumnsTo1(GazeLog, gazePointADCS_indWrtValidityR, gazePointADCS_validityR)\n",
    "    \n",
    "    \n",
    "    # gazePointADCS is in arbitrary units and needs to be converted to cm  \n",
    "    screenLength = 59\n",
    "    screenWidth = 34.5\n",
    "    \n",
    "    gazePointADCS_Left_au['column1'] = [i*59 for i in gazePointADCS_Left_au['column1']]\n",
    "    gazePointADCS_Left_au['column2'] = [i*34.5 for i in gazePointADCS_Left_au['column2']]\n",
    "    \n",
    "    gazePointADCS_Right_au['column1'] = [i*59 for i in gazePointADCS_Right_au['column1']]\n",
    "    gazePointADCS_Right_au['column2'] = [i*34.5 for i in gazePointADCS_Right_au['column2']]\n",
    "    \n",
    "    gazePointADCS = dict()\n",
    "    gazePointADCS['x'] = [(v+gazePointADCS_Right_au['column1'][i])/2 for i, v in enumerate(gazePointADCS_Left_au['column1'])]\n",
    "    gazePointADCS['y'] = [(v+gazePointADCS_Right_au['column2'][i])/2 for i, v in enumerate(gazePointADCS_Left_au['column2'])]\n",
    "    \n",
    "    \n",
    "    gazePoint_degrees = dict()\n",
    "    gazePoint_degrees['x'] = np.array([v*screenLength/distanceEyeGaze[i] for i, v in enumerate(gazePointADCS['x'])])\n",
    "    gazePoint_degrees['y'] = np.array([v*screenWidth/distanceEyeGaze[i] for i, v in enumerate(gazePointADCS['y'])])\n",
    "    \n",
    "    \n",
    "    #plt.figure()\n",
    "    #plt.plot(gazePoint_degrees['x'], gazePoint_degrees['y'])\n",
    "    \n",
    "    \n",
    "    # first create a list of times in gaze log\n",
    "    timeStrGazeLog = [item3[0] for item3 in GazeLog]\n",
    "    # convert the list of strings to datetime formats\n",
    "    timeGazeLog = np.array(timeConversion(timeStrGazeLog))\n",
    "    \n",
    "    velocity = cartesian_velocity(gazePoint_degrees['x'], gazePoint_degrees['y'], timeGazeLog)\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(range(0,len(velocity)), velocity)\n",
    "    plt.title('velocity: {}' .format(subjAndSessionName))\n",
    "    #pickle.dump(fig, open(r'C:\\DTU\\Results\\201901_Expt\\saccade\\velocity'+'\\\\'+subjAndSessionName, 'wb'))\n",
    "    \n",
    "    \n",
    "    vel_threshold = 70\n",
    "    \n",
    "    # will be later used to verify if the saccade index has already been detected or not\n",
    "    indicesSaccades = np.array([ind+1 for ind, vel in enumerate(velocity[1:-1]) if vel > vel_threshold\\\n",
    "                                if velocity[ind]>vel_threshold and velocity[ind+2]>vel_threshold])\n",
    "    if velocity[0] > vel_threshold and velocity[1] > vel_threshold:\n",
    "        indicesSaccades.insert(0,0)\n",
    "    if velocity[-1] > vel_threshold and velocity[-2] > vel_threshold:\n",
    "        indicesSaccades.insert(-1,len(velocity)-1)\n",
    "        \n",
    "    indicesSaccades_Difference = [t - s for s, t in zip(indicesSaccades, indicesSaccades[1:])]\n",
    "    indicesSaccades_Difference.insert(0, indicesSaccades[0])\n",
    "    \n",
    "    saccadeVelocity_StartEnd_TupleList = list()\n",
    "    for ind, val in enumerate(indicesSaccades_Difference):\n",
    "        # for the first saccade\n",
    "        if ind == 0:\n",
    "            saccadeVelocityStart = val\n",
    "            saccadeVelocityStartInd = ind\n",
    "            continue\n",
    "        if val > 1:\n",
    "            # adding the end to the previous saccade\n",
    "            consecutiveSaccadeVelocity_fromLastSaccadeToCurrent = ind - saccadeVelocityStartInd - 1\n",
    "            saccadeVelocityEnd = saccadeVelocityStart + consecutiveSaccadeVelocity_fromLastSaccadeToCurrent\n",
    "            saccadeVelocity_StartEnd_TupleList.append((saccadeVelocityStart,saccadeVelocityEnd))\n",
    "            \n",
    "            # the current saccade\n",
    "            saccadeVelocityStart = val+saccadeVelocityEnd\n",
    "            saccadeVelocityStartInd = ind\n",
    "            \n",
    "        if ind == len(indicesSaccades_Difference)-1:\n",
    "            consecutiveSaccadeVelocity_fromLastSaccadeToCurrent = ind - saccadeVelocityStartInd\n",
    "            saccadeVelocityEnd = saccadeVelocityStart + consecutiveSaccadeVelocity_fromLastSaccadeToCurrent\n",
    "            saccadeVelocity_StartEnd_TupleList.append((saccadeVelocityStart,saccadeVelocityEnd))\n",
    "         \n",
    "    print(indicesSaccades_Difference)\n",
    "    \n",
    "    print('')\n",
    "        \n",
    "    print(saccadeVelocity_StartEnd_TupleList)\n",
    "    \n",
    "    \n",
    "    saccadeIndices_startEnd_tupleList = list()\n",
    "    for saccadeVelocityStart, saccadeVelocityEnd in saccadeVelocity_StartEnd_TupleList:\n",
    "        saccadeStartFound = False\n",
    "        indStart = saccadeVelocityStart\n",
    "        while not saccadeStartFound:\n",
    "            indStart = indStart - 1\n",
    "            if velocity[indStart] < 5 :\n",
    "                saccadeStartFound = True                    \n",
    "                print('start found')\n",
    "                \n",
    "        saccadeEndFound = False\n",
    "        indEnd = saccadeVelocityEnd\n",
    "        while not saccadeEndFound:\n",
    "            indEnd = indEnd + 1\n",
    "            if velocity[indEnd] < 5 :\n",
    "                saccadeEndFound = True                    \n",
    "                print('end found')\n",
    "                \n",
    "        saccadeIndices_startEnd_tupleList.append((indStart, indEnd+1)) # if velocity end index is indEnd, gazeDegrees\n",
    "        # is indEnd+1\n",
    "    \n",
    "    print(saccadeIndices_startEnd_tupleList)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    saccade_IndStart_tupleList = [(ind, sum(indicesSaccades_RestDifference[0:ind+1])) for ind, val in enumerate(indicesSaccades_RestDifference) if val != 1]\n",
    "    \n",
    "    saccade_StartLength_tupleList = list()\n",
    "    \n",
    "    # create a list of tuples of saccade start index and the length of the saccade -- directly the end can be found from the \n",
    "    # length\n",
    "    ind = -1\n",
    "    saccadeLengthSum = 0\n",
    "    for saccade_ind, saccadeStartInd in saccade_IndStart_tupleList:\n",
    "        ind = ind + 1\n",
    "        if ind != len(saccade_IndStart_tupleList) - 1:\n",
    "            \n",
    "            saccadeLength = saccade_IndStart_tupleList[ind+1][0]-saccade_ind\n",
    "            saccadeLengthSum = saccadeLengthSum + saccadeLength\n",
    "            \n",
    "            saccade_StartLength_tupleList.append(tuple((saccadeStartInd, saccadeLength)))\n",
    "        else:\n",
    "            # for the last blink -- all blink lengths summed and subtracted from the length of the list\n",
    "            # missingVal_RestDifference \n",
    "            saccadeLength = len(indicesSaccades_RestDifference)-saccadeLengthSum\n",
    "            saccade_StartLength_tupleList.append(tuple((saccadeStartInd, saccadeLength)))\n",
    "     \n",
    "    \n",
    "    # create lists with start and end values for the blinks, based on blinkStart_tupleList_wLength, regardless of the blink length\n",
    "    blink_missingData_startList = [blinkStartInd - extraBlinkSamples if (blinkStartInd - extraBlinkSamples) > 0 else 0 for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    blink_missingData_endList = [blinkStartInd + blinkLength + extraBlinkSamples if (blinkStartInd + blinkLength + extraBlinkSamples) < (len(pupilData_df['pupilLeft'])-1) else (len(pupilData_df['pupilLeft'])-1) for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    # create a list of tuples from the start and end points of the blink\n",
    "    blink_missingData_startEndTuple = [(blinkStart, blink_missingData_endList[ind]) for ind, blinkStart in enumerate(blink_missingData_startList)] \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(saccade_IndStart_tupleList)\n",
    "    \n",
    "    indicesSaccade_startEnd_tuple = list()\n",
    "    for ind, vel in enumerate(velocity[1:-1]):\n",
    "        \n",
    "        if vel > vel_threshold:\n",
    "            print(ind,vel)\n",
    "            if velocity[ind-1] > vel_threshold and velocity[ind+1] > vel_threshold:\n",
    "                # artefacts with only one index with velocity greater than threshold not considered for index\n",
    "                saccadeStartFound = False\n",
    "                indStart = ind-1\n",
    "                while not saccadeStartFound:\n",
    "                    indStart = indStart - 1\n",
    "                    if velocity[indStart] < 5 :\n",
    "                        saccadeStartFound = True                    \n",
    "                        print('start found')\n",
    "                \n",
    "                saccadeEndFound = False\n",
    "                indEnd = ind+1\n",
    "                while not saccadeEndFound:\n",
    "                    indEnd = indEnd + 1\n",
    "                    if velocity[indEnd] < 5 :\n",
    "                        saccadeEndFound = True                    \n",
    "                        print('end found')\n",
    "                \n",
    "                indicesSaccade_startEnd_tuple.append(indStart, indEnd+1) # if velocity end index is indEnd, gazeDegrees\n",
    "                # is indEnd+1\n",
    "                \n",
    "    \n",
    "    \n",
    "    saccadeTime = timeGazeLog[indicesSaccades]\n",
    "    saccadeX = gazePoint_degrees['x'][indicesSaccades]\n",
    "    saccadeY = gazePoint_degrees['y'][indicesSaccades]\n",
    "    \n",
    "    saccadeDuration = [(indEnd-indStart+1)/90 for indStart, indEnd in indicesSaccade_startEnd_tuple]\n",
    "    saccadeAmplitude_xy = list()\n",
    "    saccadeAmplitude_x = list()\n",
    "    saccadeAmplitude_y = list()\n",
    "    \n",
    "    for indStart, indEnd in indicesSaccade_startEnd_tuple:\n",
    "        distance_xy, distance_x, distance_y = 0\n",
    "        for ind in range(indStart, indEnd-1):\n",
    "            distance_xy = distance + np.sqrt((gazePoint_degrees['x'][ind+1]-gazePoint_degrees['x'][ind])**2+(gazePoint_degrees['y'][ind+1]-gazePoint_degrees['y'][ind])**2)\n",
    "            distance_x = distance_x + np.abs(gazePoint_degrees['x'][ind+1]-gazePoint_degrees['x'][ind])\n",
    "            distance_y = distance_y + np.abs(gazePoint_degrees['y'][ind+1]-gazePoint_degrees['y'][ind])\n",
    "            \n",
    "        saccadeAmplitude_xy.append(distance_xy)\n",
    "        saccadeAmplitude_x.append(distance_x)\n",
    "        saccadeAmplitude_y.append(distance_y)\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject path E:\\Data\\Data\\ac\\1\\2019-02-11-11-18-30_1\n",
      "subject and session name:  ac__1__2019-02-11-11-18-30_1\n",
      "gazeOrigin\n",
      "gazePoint\n",
      "gaze point on screen\n",
      "[250, 1, 20, 1, 18, 1, 117, 1, 272, 1, 1, 141, 1, 1, 9, 1, 1, 455, 40, 52, 1, 1, 1, 1, 1, 74, 1, 170, 1, 1, 1, 42, 2064, 1, 1, 109, 118, 1, 25, 187, 1, 1, 315, 1, 1, 1, 274, 376, 1, 268, 1, 1, 4, 123, 69, 1, 1, 51, 115, 175, 1, 382, 110, 1, 1, 27, 236, 186, 41, 1, 1, 230, 1, 98, 126, 28, 1, 6, 1, 70, 215, 70, 1, 1, 16, 1, 1, 1, 1, 132, 1, 28, 106, 1, 103, 1, 626, 1, 1, 1, 24, 306, 1, 1, 176, 1, 14, 1, 439, 1, 1, 1, 1, 11, 27, 90, 1, 1, 155, 1, 103, 1, 1, 13, 13, 125, 1, 138, 1, 106, 1, 1, 199, 1, 1, 314, 1, 1, 86, 1, 193, 1, 108, 234, 48, 1, 23, 1, 18, 1, 1, 215, 1, 1, 1, 75, 183, 1, 1, 1, 1, 1, 251, 1, 1, 1, 1, 105, 1, 1, 55, 1, 34, 44, 1, 183, 521, 35, 1, 128, 1, 96, 100, 1, 228, 1, 74, 1, 48, 51, 1, 1, 98, 1, 1027, 130, 1, 1, 106, 1, 168, 1, 1, 1, 304, 1, 1, 126, 1, 1, 19, 1, 9, 284, 1, 1, 1, 1, 1, 173, 1, 1, 1, 98, 103, 1, 1, 289, 1, 129, 1, 24, 305, 151, 1, 94, 132, 93, 1, 1, 1, 1, 124, 119, 1, 104, 1, 1, 108, 105, 1, 101, 1, 1, 100, 1, 1, 1, 269, 1, 1, 1, 113, 116, 1, 1, 89, 1, 127, 104, 1, 1, 1, 523, 1, 1, 92, 95, 1, 1, 1, 116, 1, 1, 190, 1, 1, 1, 94, 1, 139, 1, 7, 1, 1, 214, 1, 1, 267, 105, 108, 24, 102, 1, 1, 1, 178, 1, 138, 1, 1, 1, 24, 92, 52, 58, 185, 109, 100, 228, 1, 1, 297, 1, 18, 49, 1, 1, 342, 1, 1, 332, 1, 1, 50, 1, 127, 1, 68, 85, 1, 1, 94, 1, 12, 246, 600, 1, 1, 43, 15, 1, 259, 1, 1, 301, 241, 1, 1, 1, 1, 86, 7, 1, 76, 30, 1, 230, 1, 478, 103, 40, 1, 291, 1, 1, 1, 1, 63, 1, 1, 25, 13, 1, 1, 37, 1, 94, 12, 1, 1, 1, 15, 1, 1, 1, 162, 1, 1, 183, 1, 209, 1, 93, 1, 230, 27, 1, 11, 1, 64, 73, 1, 1, 126, 1, 130, 72, 75, 1, 1, 1, 25, 100, 1, 1, 99, 1, 103, 93, 1, 1, 1, 136, 36, 97, 1, 1, 105, 1, 591, 1, 1, 1, 1, 1, 224, 32, 1, 1, 112, 101, 1, 28, 93, 1, 1, 210, 94, 1, 1, 1, 89, 1, 1, 1, 106, 113, 1, 1, 1, 102, 1, 1, 1, 136, 139, 1, 107, 1, 146, 142, 1, 1, 1, 1, 1, 229, 1, 34, 25, 93, 1, 103, 1, 1, 136, 1, 178, 150, 30, 1, 1, 1, 1, 46, 1, 22, 1, 1, 20, 1, 73, 38, 54, 1, 1, 1, 99, 33, 1, 1, 46, 17, 1, 154, 18, 1, 9, 12, 1, 1, 282, 1, 1, 1, 18, 176, 300, 1, 52, 25, 224, 1, 71, 1, 198, 30, 1, 173, 1, 278, 216, 1, 135, 1, 87, 1, 1, 1, 202, 1, 1, 183, 1, 1, 1, 180, 1, 77, 1, 5, 72, 1, 64, 1, 174, 56, 1, 1, 36, 1, 1, 43, 1, 1, 1, 285, 121, 1, 1, 156, 1, 1, 127, 1, 22, 1, 1, 1, 1, 129, 1, 257, 1, 1, 1, 204, 214, 167, 14, 1, 1, 1, 46, 1, 22, 1, 143, 1, 1, 54, 91, 1, 1, 240, 1, 1, 1, 1, 1, 150, 101, 1, 1, 1, 1, 23, 43, 1, 153, 1, 1, 19, 1, 1, 1, 258, 146, 1, 1, 1, 43, 1, 1, 1, 15, 1, 554, 187, 85, 69, 255, 251, 27, 108, 528, 216, 1, 42, 60, 1, 110, 1, 1, 1, 1, 1, 105, 1, 1, 96, 1, 213, 1, 1, 1, 1, 133, 1, 1, 1, 1, 89, 1, 1, 1, 1, 38, 1, 1, 1, 1, 26, 1, 237, 1, 1, 1, 112, 1, 1, 1, 1, 177, 1, 1, 195, 1, 32, 65, 1, 1, 1, 236, 1, 30, 1, 1, 1, 79, 101, 102, 1, 198, 1, 1, 108, 90, 1, 1, 202, 95, 68, 25, 50, 455, 87, 1, 1, 19, 1, 91, 1, 214, 204, 1, 98, 1, 1, 95, 1, 107, 1, 104, 105, 1, 171, 101, 1, 208, 1, 1, 358, 1, 1, 1, 1, 194, 105, 113, 1, 94, 1, 1, 20, 89, 1, 99, 1, 1, 1, 94, 1, 1, 154, 1, 1, 1, 26, 205, 1, 1, 101, 1, 38, 1, 1, 32, 1, 67, 1, 80, 87, 1, 1, 1, 1, 27, 195, 1, 1, 1, 95, 1, 1, 191, 211, 1, 1, 249, 1, 1, 1, 306, 1, 17, 44, 1, 22, 1, 1, 93, 1, 20, 21, 18, 1, 1, 19, 35, 1, 1, 1, 295, 38, 1, 1, 1, 1, 1, 218, 1, 1, 1, 341, 1, 43, 1, 1, 1, 152, 109, 116, 84, 29, 1, 1, 1, 203, 65, 119, 1, 385, 1, 1, 1, 1, 358, 178, 1, 20, 215, 1, 33, 8, 264, 1, 1, 1, 225, 1, 1, 1, 26, 93, 1, 1, 175, 1, 87, 1, 1, 1, 87, 35, 1, 80, 1, 28, 51, 1, 1, 1, 1, 1, 89, 1, 155, 181, 86, 1, 96, 1, 80, 1, 1, 1, 309, 1, 1, 1, 119, 1, 259, 104, 1, 22, 51, 1, 33, 1, 1, 1, 1, 80, 1, 18, 1, 1, 31, 41, 48, 24, 104, 1, 193, 1, 1, 1, 49, 1, 25, 86, 196, 1, 1, 1, 1, 92, 1, 1, 38, 588, 1, 223, 21, 1, 1, 1, 388, 1, 138, 1, 1, 1, 1, 704, 1, 1, 1, 1, 324, 1, 1, 1, 459, 1, 1, 383, 1, 1, 1, 109, 1, 479, 1, 1, 1, 1, 1, 77, 22, 51, 16, 160, 1, 1, 1, 181, 1, 20, 1, 17, 1, 19, 1, 1]\n",
      "\n",
      "[(250, 251), (271, 272), (290, 291), (408, 409), (681, 683), (824, 826), (835, 837), (1292, 1292), (1332, 1332), (1384, 1389), (1463, 1464), (1634, 1637), (1679, 1679), (3743, 3745), (3854, 3854), (3972, 3973), (3998, 3998), (4185, 4187), (4502, 4505), (4779, 4779), (5155, 5156), (5424, 5426), (5430, 5430), (5553, 5553), (5622, 5624), (5675, 5675), (5790, 5790), (5965, 5966), (6348, 6348), (6458, 6460), (6487, 6487), (6723, 6723), (6909, 6909), (6950, 6952), (7182, 7183), (7281, 7281), (7407, 7407), (7435, 7436), (7442, 7443), (7513, 7513), (7728, 7728), (7798, 7800), (7816, 7820), (7952, 7953), (7981, 7981), (8087, 8088), (8191, 8192), (8818, 8821), (8845, 8845), (9151, 9153), (9329, 9330), (9344, 9345), (9784, 9788), (9799, 9799), (9826, 9826), (9916, 9918), (10073, 10074), (10177, 10179), (10192, 10192), (10205, 10205), (10330, 10331), (10469, 10470), (10576, 10578), (10777, 10779), (11093, 11095), (11181, 11182), (11375, 11376), (11484, 11484), (11718, 11718), (11766, 11767), (11790, 11791), (11809, 11811), (12026, 12029), (12104, 12104), (12287, 12292), (12543, 12547), (12652, 12654), (12709, 12710), (12744, 12744), (12788, 12789), (12972, 12972), (13493, 13493), (13528, 13529), (13657, 13658), (13754, 13754), (13854, 13855), (14083, 14084), (14158, 14159), (14207, 14207), (14258, 14260), (14358, 14359), (15386, 15386), (15516, 15518), (15624, 15625), (15793, 15796), (16100, 16102), (16228, 16230), (16249, 16250), (16259, 16259), (16543, 16548), (16721, 16724), (16822, 16822), (16925, 16927), (17216, 17217), (17346, 17347), (17371, 17371), (17676, 17676), (17827, 17828), (17922, 17922), (18054, 18054), (18147, 18151), (18275, 18275), (18394, 18395), (18499, 18501), (18609, 18609), (18714, 18715), (18816, 18818), (18918, 18921), (19190, 19193), (19306, 19306), (19422, 19424), (19513, 19514), (19641, 19641), (19745, 19748), (20271, 20273), (20365, 20365), (20460, 20463), (20579, 20581), (20771, 20774), (20868, 20869), (21008, 21009), (21016, 21018), (21232, 21234), (21501, 21501), (21606, 21606), (21714, 21714), (21738, 21738), (21840, 21843), (22021, 22022), (22160, 22163), (22187, 22187), (22279, 22279), (22331, 22331), (22389, 22389), (22574, 22574), (22683, 22683), (22783, 22783), (23011, 23013), (23310, 23311), (23329, 23329), (23378, 23380), (23722, 23724), (24056, 24058), (24108, 24109), (24236, 24237), (24305, 24305), (24390, 24392), (24486, 24487), (24499, 24499), (24745, 24745), (25345, 25347), (25390, 25390), (25405, 25406), (25665, 25667), (25968, 25968), (26209, 26213), (26299, 26299), (26306, 26307), (26383, 26383), (26413, 26414), (26644, 26645), (27123, 27123), (27226, 27226), (27266, 27267), (27558, 27562), (27625, 27627), (27652, 27652), (27665, 27667), (27704, 27705), (27799, 27799), (27811, 27814), (27829, 27832), (27994, 27996), (28179, 28180), (28389, 28390), (28483, 28484), (28714, 28714), (28741, 28742), (28753, 28754), (28818, 28818), (28891, 28893), (29019, 29020), (29150, 29150), (29222, 29222), (29297, 29300), (29325, 29325), (29425, 29427), (29526, 29527), (29630, 29630), (29723, 29726), (29862, 29862), (29898, 29898), (29995, 29997), (30102, 30103), (30694, 30699), (30923, 30923), (30955, 30957), (31069, 31069), (31170, 31171), (31199, 31199), (31292, 31294), (31504, 31504), (31598, 31601), (31690, 31693), (31799, 31799), (31912, 31915), (32017, 32020), (32156, 32156), (32295, 32296), (32403, 32404), (32550, 32550), (32692, 32697), (32926, 32927), (32961, 32961), (32986, 32986), (33079, 33080), (33183, 33185), (33321, 33322), (33500, 33500), (33650, 33650), (33680, 33684), (33730, 33731), (33753, 33755), (33775, 33776), (33849, 33849), (33887, 33887), (33941, 33944), (34043, 34043), (34076, 34078), (34124, 34124), (34141, 34142), (34296, 34296), (34314, 34315), (34324, 34324), (34336, 34338), (34620, 34623), (34641, 34641), (34817, 34817), (35117, 35118), (35170, 35170), (35195, 35195), (35419, 35420), (35491, 35492), (35690, 35690), (35720, 35721), (35894, 35895), (36173, 36173), (36389, 36390), (36525, 36526), (36613, 36616), (36818, 36820), (37003, 37006), (37186, 37187), (37264, 37265), (37270, 37270), (37342, 37343), (37407, 37408), (37582, 37582), (37638, 37640), (37676, 37678), (37721, 37724), (38009, 38009), (38130, 38132), (38288, 38290), (38417, 38418), (38440, 38444), (38573, 38574), (38831, 38834), (39038, 39038), (39252, 39252), (39419, 39419), (39433, 39436), (39482, 39483), (39505, 39506), (39649, 39651), (39705, 39705), (39796, 39798), (40038, 40043), (40193, 40193), (40294, 40298), (40321, 40321), (40364, 40365), (40518, 40520), (40539, 40542), (40800, 40800), (40946, 40949), (40992, 40995), (41010, 41011), (41565, 41565), (41752, 41752), (41837, 41837), (41906, 41906), (42161, 42161), (42412, 42412), (42439, 42439), (42547, 42547), (43075, 43075), (43291, 43292), (43334, 43334), (43394, 43395), (43505, 43510), (43615, 43617), (43713, 43714), (43927, 43931), (44064, 44068), (44157, 44161), (44199, 44203), (44229, 44230), (44467, 44470), (44582, 44586), (44763, 44765), (44960, 44961), (44993, 44993), (45058, 45061), (45297, 45298), (45328, 45331), (45410, 45410), (45511, 45511), (45613, 45614), (45812, 45814), (45922, 45922), (46012, 46014), (46216, 46216), (46311, 46311), (46379, 46379), (46404, 46404), (46454, 46454), (46909, 46909), (46996, 46998), (47017, 47018), (47109, 47110), (47324, 47324), (47528, 47529), (47627, 47629), (47724, 47725), (47832, 47833), (47937, 47937), (48042, 48043), (48214, 48214), (48315, 48316), (48524, 48526), (48884, 48888), (49082, 49082), (49187, 49187), (49300, 49301), (49395, 49397), (49417, 49417), (49506, 49507), (49606, 49609), (49703, 49705), (49859, 49862), (49888, 49888), (50093, 50095), (50196, 50197), (50235, 50237), (50269, 50270), (50337, 50338), (50418, 50418), (50505, 50509), (50536, 50536), (50731, 50734), (50829, 50831), (51022, 51022), (51233, 51235), (51484, 51487), (51793, 51794), (51811, 51811), (51855, 51856), (51878, 51880), (51973, 51974), (51994, 51994), (52015, 52015), (52033, 52035), (52054, 52054), (52089, 52092), (52387, 52387), (52425, 52430), (52648, 52651), (52992, 52993), (53036, 53039), (53191, 53191), (53300, 53300), (53416, 53416), (53500, 53500), (53529, 53532), (53735, 53735), (53800, 53800), (53919, 53920), (54305, 54309), (54667, 54667), (54845, 54846), (54866, 54866), (55081, 55082), (55115, 55115), (55123, 55123), (55387, 55390), (55615, 55618), (55644, 55644), (55737, 55739), (55914, 55915), (56002, 56005), (56092, 56092), (56127, 56128), (56208, 56209), (56237, 56237), (56288, 56293), (56382, 56383), (56538, 56538), (56719, 56719), (56805, 56806), (56902, 56903), (56983, 56986), (57295, 57298), (57417, 57418), (57677, 57677), (57781, 57782), (57804, 57804), (57855, 57856), (57889, 57893), (57973, 57974), (57992, 57994), (58025, 58025), (58066, 58066), (58114, 58114), (58138, 58138), (58242, 58243), (58436, 58439), (58488, 58489), (58514, 58514), (58600, 58600), (58796, 58800), (58892, 58894), (58932, 58932), (59520, 59521), (59744, 59744), (59765, 59768), (60156, 60157), (60295, 60299), (61003, 61007), (61331, 61334), (61793, 61795), (62178, 62181), (62290, 62291), (62770, 62775), (62852, 62852), (62874, 62874), (62925, 62925), (62941, 62941), (63101, 63104), (63285, 63286), (63306, 63307), (63324, 63325), (63344, 63346)]\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "start found\n",
      "end found\n",
      "[(247, 258), (269, 276), (288, 297), (405, 427), (677, 696), (822, 831), (832, 847), (1288, 1299), (1323, 1343), (1382, 1394), (1449, 1471), (1629, 1645), (1675, 1683), (3739, 3754), (3850, 3858), (3967, 3986), (3994, 4013), (4182, 4190), (4497, 4522), (4760, 4784), (5153, 5162), (5422, 5429), (5428, 5435), (5548, 5558), (5619, 5630), (5673, 5697), (5784, 5796), (5960, 5970), (6343, 6356), (6456, 6465), (6482, 6494), (6719, 6727), (6901, 6915), (6947, 6960), (7178, 7188), (7278, 7291), (7405, 7419), (7432, 7439), (7438, 7455), (7508, 7520), (7725, 7746), (7795, 7809), (7814, 7828), (7945, 7956), (7978, 7986), (8084, 8095), (8188, 8196), (8815, 8826), (8843, 8851), (9148, 9159), (9325, 9334), (9337, 9352), (9781, 9792), (9796, 9803), (9823, 9831), (9913, 9921), (10071, 10082), (10174, 10185), (10187, 10197), (10196, 10209), (10327, 10335), (10465, 10478), (10573, 10582), (10774, 10791), (11089, 11099), (11178, 11190), (11369, 11380), (11481, 11558), (11700, 11721), (11763, 11773), (11787, 11799), (11804, 11816), (12021, 12042), (12101, 12112), (12282, 12299), (12540, 12556), (12648, 12662), (12691, 12724), (12740, 12757), (12782, 12795), (12968, 12986), (13490, 13510), (13525, 13535), (13654, 13669), (13736, 13763), (13851, 13861), (14081, 14094), (14153, 14165), (14186, 14212), (14256, 14266), (14355, 14366), (15378, 15391), (15514, 15521), (15621, 15628), (15789, 15799), (16097, 16109), (16225, 16236), (16246, 16255), (16256, 16279), (16541, 16559), (16718, 16730), (16818, 16827), (16922, 16930), (17206, 17222), (17342, 17351), (17368, 17375), (17672, 17689), (17823, 17836), (17919, 17932), (18050, 18064), (18144, 18154), (18273, 18293), (18387, 18406), (18496, 18505), (18606, 18615), (18711, 18723), (18810, 18824), (18914, 18928), (19187, 19196), (19303, 19332), (19418, 19433), (19510, 19521), (19637, 19646), (19741, 19772), (20268, 20279), (20362, 20369), (20457, 20470), (20575, 20590), (20768, 20780), (20864, 20873), (21006, 21023), (21006, 21023), (21230, 21241), (21498, 21505), (21601, 21611), (21698, 21720), (21735, 21745), (21836, 21851), (22012, 22039), (22157, 22170), (22184, 22195), (22274, 22289), (22309, 22337), (22381, 22394), (22572, 22595), (22679, 22689), (22781, 22803), (23006, 23020), (23307, 23319), (23326, 23340), (23371, 23385), (23719, 23727), (24054, 24064), (24104, 24115), (24232, 24246), (24302, 24310), (24376, 24400), (24483, 24508), (24483, 24508), (24742, 24749), (25342, 25356), (25381, 25415), (25381, 25415), (25654, 25676), (25964, 26071), (26180, 26222), (26295, 26313), (26295, 26313), (26380, 26387), (26410, 26418), (26640, 26666), (27119, 27128), (27222, 27241), (27262, 27276), (27555, 27568), (27621, 27633), (27649, 27657), (27662, 27672), (27700, 27710), (27795, 27805), (27808, 27825), (27826, 27842), (27991, 28004), (28174, 28185), (28386, 28399), (28481, 28489), (28709, 28718), (28738, 28746), (28750, 28758), (28804, 28824), (28886, 28898), (29015, 29027), (29147, 29155), (29219, 29234), (29294, 29305), (29320, 29329), (29422, 29436), (29522, 29531), (29625, 29638), (29718, 29733), (29858, 29866), (29894, 29901), (29993, 30000), (30095, 30108), (30692, 30704), (30919, 30927), (30953, 30961), (31029, 31075), (31163, 31178), (31196, 31204), (31290, 31298), (31501, 31508), (31593, 31608), (31685, 31696), (31796, 31805), (31907, 31918), (32012, 32023), (32153, 32162), (32291, 32309), (32400, 32419), (32542, 32553), (32690, 32703), (32922, 32946), (32948, 32967), (32984, 32991), (33074, 33085), (33179, 33190), (33319, 33325), (33497, 33504), (33640, 33656), (33677, 33689), (33710, 33736), (33749, 33766), (33772, 33780), (33845, 33855), (33882, 33898), (33938, 33949), (34038, 34048), (34074, 34084), (34104, 34129), (34139, 34148), (34294, 34302), (34311, 34328), (34311, 34328), (34333, 34356), (34614, 34630), (34638, 34649), (34798, 34830), (35114, 35122), (35166, 35179), (35191, 35230), (35417, 35424), (35488, 35498), (35685, 35698), (35716, 35755), (35889, 35898), (36170, 36180), (36386, 36398), (36522, 36530), (36611, 36622), (36815, 36823), (36999, 37010), (37184, 37196), (37260, 37274), (37260, 37274), (37333, 37349), (37403, 37412), (37579, 37586), (37635, 37650), (37672, 37682), (37718, 37731), (38006, 38030), (38127, 38137), (38283, 38296), (38412, 38426), (38438, 38449), (38571, 38589), (38829, 38843), (39034, 39042), (39250, 39260), (39402, 39423), (39430, 39444), (39480, 39489), (39498, 39511), (39646, 39655), (39701, 39710), (39794, 39802), (40034, 40049), (40176, 40198), (40288, 40305), (40318, 40324), (40360, 40372), (40516, 40524), (40534, 40545), (40796, 40806), (40944, 40953), (40988, 41000), (41007, 41016), (41559, 41758), (41559, 41758), (41832, 41894), (41893, 41932), (42157, 42367), (42408, 42417), (42436, 42444), (42542, 42554), (43072, 43079), (43288, 43298), (43331, 43339), (43390, 43398), (43499, 43524), (43611, 43625), (43709, 43717), (43922, 43937), (44061, 44076), (44155, 44165), (44196, 44211), (44225, 44236), (44463, 44478), (44577, 44592), (44760, 44769), (44957, 44966), (44990, 44997), (45048, 45067), (45294, 45311), (45326, 45337), (45407, 45426), (45507, 45518), (45611, 45621), (45809, 45822), (45918, 45929), (46009, 46021), (46209, 46224), (46308, 46332), (46375, 46385), (46400, 46411), (46448, 46461), (46890, 46918), (46992, 47001), (47014, 47023), (47106, 47118), (47319, 47332), (47526, 47535), (47625, 47635), (47721, 47728), (47827, 47836), (47932, 47955), (48039, 48046), (48195, 48222), (48312, 48322), (48521, 48533), (48882, 48893), (49080, 49089), (49183, 49204), (49296, 49307), (49391, 49401), (49414, 49423), (49502, 49515), (49603, 49612), (49698, 49708), (49854, 49866), (49884, 49892), (50089, 50099), (50193, 50205), (50233, 50242), (50266, 50277), (50334, 50343), (50414, 50423), (50503, 50515), (50533, 50542), (50729, 50737), (50826, 50836), (51019, 51026), (51228, 51243), (51482, 51491), (51776, 51800), (51806, 51815), (51851, 51865), (51875, 51886), (51969, 51980), (51990, 52000), (52012, 52024), (52029, 52043), (52052, 52058), (52086, 52097), (52370, 52392), (52423, 52435), (52645, 52657), (52989, 52996), (53033, 53047), (53188, 53196), (53296, 53309), (53413, 53419), (53494, 53507), (53524, 53536), (53731, 53741), (53788, 53805), (53916, 53924), (54302, 54317), (54659, 54672), (54841, 54852), (54863, 54872), (55079, 55089), (55113, 55132), (55113, 55132), (55379, 55394), (55613, 55625), (55641, 55648), (55734, 55746), (55911, 55921), (55997, 56010), (56089, 56097), (56124, 56131), (56204, 56219), (56219, 56253), (56277, 56296), (56379, 56386), (56535, 56543), (56699, 56731), (56801, 56815), (56898, 56906), (56980, 56992), (57290, 57304), (57413, 57425), (57673, 57696), (57776, 57789), (57797, 57808), (57852, 57866), (57887, 57897), (57970, 57977), (57989, 58001), (58021, 58029), (58047, 58071), (58111, 58120), (58135, 58142), (58240, 58246), (58433, 58445), (58485, 58495), (58511, 58518), (58597, 58604), (58793, 58807), (58889, 58900), (58928, 58953), (59517, 59525), (59727, 59751), (59762, 59775), (60149, 60163), (60290, 60307), (61000, 61016), (61303, 61340), (61785, 61802), (62174, 62187), (62286, 62295), (62764, 62782), (62848, 62862), (62868, 62878), (62922, 62929), (62937, 62945), (63098, 63109), (63282, 63292), (63301, 63316), (63321, 63329), (63339, 63353)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataFolderName = r'E:\\Data\\Data' # accessing external hard disk with the data\n",
    "a = re.compile('(?<=Data\\\\\\\\Data\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "subjectName_listElement = 3\n",
    "\n",
    "#dataFolderName = r'C:\\DTU\\Data\\201901_JanuaryExpt' # accessing data saved in the computer\n",
    "#a = re.compile('(?<=Data\\\\\\\\201901_JanuaryExpt\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "#subjectName_listElement = 4\n",
    "\n",
    "resultFileName = r'C:\\DTU\\Data\\201901_JanuaryExpt\\typing_speed2.xlsx'\n",
    "\n",
    "\n",
    "for root, dirs, subfolder in os.walk(dataFolderName):\n",
    "    \n",
    "    technique = 'dwell_time'\n",
    "    \n",
    "    if not dirs:\n",
    "        \n",
    "        #if 'notCompleted' in root or 'notInclude' in root: # Some subjects do not have gaze log and have been marked as \n",
    "        \n",
    "        if 'noData' in root or 'Trial' in root or 'trial' in root or 'Nothing' in root: # Some subjects do not have \n",
    "            #gaze log and have been marked as notInclude\n",
    "            continue\n",
    "        if 'Jonas' in root or 'Praktikant' in root or 'Villads' in root:\n",
    "            continue\n",
    "            \n",
    "        if 'ac\\\\1\\\\2019-02-11-11-18-30_1' not in root:\n",
    "            continue\n",
    "        if 'Picture' in root:\n",
    "            continue\n",
    "        #if '_MS' not in root:\n",
    "        #    continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #if '2019-1-16-16-36-17_1stPart_2' not in root:\n",
    "        #    continue\n",
    "            \n",
    "        keysSelected = None\n",
    "        userKeys = None\n",
    "        phraseLog = None\n",
    "        \n",
    "        for file in subfolder:\n",
    "            \n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'KeySelection*'):\n",
    "                try:\n",
    "                    \n",
    "                    fKeysSelected = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerKeysSelected = csv.reader(fKeysSelected)\n",
    "                    keysSelected = list(readerKeysSelected)\n",
    "                    \n",
    "                    keysSelected.remove(keysSelected[0])\n",
    "                except:\n",
    "                    if fKeysSelected is not None:\n",
    "                        fKeysSelected.close()\n",
    "                    else:\n",
    "                        print('error in opening the KeySelection log file')\n",
    "                        \n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'user*'):\n",
    "                try:\n",
    "                    fUserKey = open(root + '\\\\' + file, encoding='utf-8',  newline='')\n",
    "                    readerUserKey = csv.reader(fUserKey, quotechar=None)\n",
    "                    userKeys = list(readerUserKey)\n",
    "                    userKeys.remove(userKeys[0])\n",
    "                except:\n",
    "                    if fUserKey is not None:\n",
    "                        fUserKey.close()\n",
    "                    else:\n",
    "                        print('error in opening the user key log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'phrase*'):\n",
    "                try:\n",
    "                    fPhraseLog = open(root + '\\\\' + file, encoding='utf-8')\n",
    "                    readerPhraseLog = csv.reader(fPhraseLog, quotechar=None)\n",
    "                    phraseLog = list(readerPhraseLog)\n",
    "                    \n",
    "                except:\n",
    "                    if fPhraseLog is not None:\n",
    "                        fPhraseLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the phrase log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'multiKey*'):\n",
    "                technique = 'multiKey_selection'\n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'tobiiGazeLog*'):\n",
    "                try:\n",
    "                    fGazeLog = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerGazeLog = csv.reader(fGazeLog, quotechar=None)\n",
    "                    gazeLog = list(readerGazeLog)\n",
    "                    gazeLog.remove(gazeLog[0])\n",
    "                    gazeLog.remove(gazeLog[-1])\n",
    "                except:\n",
    "                    if fGazeLog is not None:\n",
    "                        fGazeLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the scratchpad log file')\n",
    "            \n",
    "                    \n",
    "                     \n",
    "        if keysSelected is None or userKeys is None or phraseLog is None or gazeLog is None:\n",
    "            continue\n",
    "        else:\n",
    "                \n",
    "            print('subject path', root)\n",
    "            subjAndSessionName = '__'.join(root.split('\\\\')[subjectName_listElement:])\n",
    "            subjName = subjAndSessionName.split('__')[0]\n",
    "            print('subject and session name: ', subjAndSessionName)\n",
    "            \n",
    "            # fix phraselog due to comma related file changes\n",
    "            phraseLog_new = FixScratchPad(phraseLog)\n",
    "            \n",
    "            # fix userKeys due to comma related file changes\n",
    "            userKeys_new = FixUserKeys(userKeys)\n",
    "            \n",
    "            sessionFolderName = root.split('\\\\')[-1]\n",
    "            if sessionFolderName in dict_noGazeData:\n",
    "                print('no gaze data present')\n",
    "                continue\n",
    "            \n",
    "            # detect saccades using Ryklin's method\n",
    "            saccadeDetectionVelocity(gazeLog, subjAndSessionName)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            # need to fix keys selected, where rows get combined because of an inverted comma\n",
    "            keysSelected_new = FixKeysSelected(keysSelected)\n",
    "            \n",
    "            # find start time of typing\n",
    "            timeTyping = OptiKeyTypingTime(userKeys_new)\n",
    "            \n",
    "            # divide complete data into epochs of phrases\n",
    "            timeStartEndDict_reading, timeStartEndDict_writing = FindTrialTimes(keysSelected_new, timeTyping, root)\n",
    "            \n",
    "            \n",
    "           # filter the data\n",
    "            pupilData_filtered = FilterPupilSize(gazeLog, timeTyping, subjAndSessionName)\n",
    "            \n",
    "            # find a vector indicating True for the start and end of trials, False otherwise\n",
    "            eventTrials_reading, eventTrials_readingIndex = DivideTimeIntoTrials(pupilData_filtered, timeStartEndDict_reading)\n",
    "            eventTrials_writing, eventTrials_writingIndex = DivideTimeIntoTrials(pupilData_filtered, timeStartEndDict_writing)\n",
    "            \n",
    "            \n",
    "            # divide trials into reading and writing\n",
    "            eventReading, eventReading_index = FindReadingPartsOfTrial(eventTrials_readingIndex, keysSelected_new, pupilData_filtered)\n",
    "            \n",
    "            eventWriting, eventWriting_index = FindWritingPartsOfTrial(eventTrials_writingIndex, keysSelected_new, pupilData_filtered, eventReading_index)\n",
    "            \n",
    "            # plot pupil size with the events\n",
    "            plotTitle = 'ReadingAndWritingMarked'\n",
    "            PlotPupilSizeWithEvents(pupilData_filtered, eventReading_index, eventWriting_index, plotTitle, subjAndSessionName, eventNumbers=2)\n",
    "            \n",
    "            \n",
    "            # if it is the 2nd part of the session, picture is not described:\n",
    "            if '2ndPart' in root:\n",
    "                picture = 'not_described'\n",
    "            else:\n",
    "                picture = 'described'\n",
    "            \n",
    "            \"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# open interactive plot of saccade velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = r'C:\\DTU\\Results\\201901_Expt\\saccade\\velocity\\ac__1__2019-02-11-11-18-30_1'\n",
    "figx = pickle.load(open(f, 'rb'))\n",
    "\n",
    "figx.show() # Show the figure, edit it, etc.!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
