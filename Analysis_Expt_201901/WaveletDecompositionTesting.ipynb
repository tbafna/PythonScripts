{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import copy\n",
    "import itertools\n",
    "import distance\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pywt\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import snowball\n",
    "\n",
    "from itertools import *\n",
    "from operator import *\n",
    "\n",
    "# import other jupyter notebooks\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptional removal of particular extra sentences not typed by the user \n",
    "dict_phraseStim = {\n",
    "    #'2019-02-05-14-10-39_2ndPart_2' : [1, 2, 3, 4, 5, 6, 9, 10],\n",
    "    #'2019-01-14-14-58-30' : [0], # ys, session_trial ()\n",
    "    '2019-01-16-16-36-17_1stPart_2' : [-1], # af_session1\n",
    "    '2019-01-16-17-00-12_2ndPart_2': [1], # af_session1\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-17-16-03-27_2ndPart_2' : [0, 1, 2], # Af session2\n",
    "    '2019-02-06-11-25-41_1' : [7],               # aq_session1    \n",
    "    '2019-02-08-11-33-53_1stPart_1' : [1],  # aq session3_1_part1\n",
    "    '2019-02-08-12-11-34_2ndPart_1' : [0, 1, 2, 3],  # aq session3_1_part2\n",
    "    '2019-01-31-09-37-5_2ndPart_2' : range(1,5), # bh1, session 4 , all sentences except the first one deleted\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4],  # bh1_session4_2_part1\n",
    "    '2019-02-21-16-09-44_1stPart_1' : [1], # bh2_session1\n",
    "    '2019-02-21-16-22-22_2ndPart_1' : [2, 3, 4],# bh2_session1\n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],       # bh2_session3\n",
    "    '2019-02-28-17-24-2_2ndPart_2' : [0, 2],     # bh2_session3\n",
    "    '2019-02-14-13-28-20_1stPart_2' : [2], # cw_session3_2_part1\n",
    "    '2019-02-14-13-57-41_2ndPart_2' : [0, 2, 3], # cw_session3_2_part2\n",
    "    '2019-02-21-15-01-4_1stPart_1' : [0],        # le_session3\n",
    "    '2019-02-21-15-25-56_2ndPart_1' : [1],        # le_session3\n",
    "    '2019-02-18-10-28-35_2' : [0],               # ls2_session4 # picture not described\n",
    "    '2019-02-05-14-00-27_1stPart_2' : [3],        # mh_session1\n",
    "    '2019-02-05-14-10-39_2ndPart_2' : [0, 1, 3],   # mh_session1\n",
    "    '2019-02-08-10-51-3_1stPart_1' : [4],        # mn_session1\n",
    "    '2019-02-08-11-05-7_2ndPart_1' : [0, 2, 3, 4], # mn_session1\n",
    "    '2019-02-19-10-34-7_1stPart_1' : [3],          # mn_session3\n",
    "    '2019-02-19-10-56-43_2ndPart_1' : [1, 2, 3, 4], # mn_session3\n",
    "    '2019-01-16-15-18-0_1' : [4],            # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-01-29-13-25-4_1' : [3],        # ph_session2\n",
    "    '2019-03-07-16-44-5_2' : [1],                   # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],                  # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4], # ys_session1\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [3, 4], # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [2], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [3, 5, 7],          # ys_session4\n",
    "    '2019-01-30-11-22-25_1' : [4, 6, 7] # ys, session 4\n",
    "}\n",
    "\n",
    "# exceptional removal of sentences/words typed by the user, but then deleted everything to have a blank scratchpad\n",
    "\n",
    "dict_phraseUser = {\n",
    "    \"2019-02-06-15-44-15_1\" : [2, 3, 6], \n",
    "    \"2019-02-06-16-19-9_2\" : [1, 3, 6, 7],\n",
    "    \"2019-02-12-11-21-21_2\" : [0],\n",
    "    \"2019-02-14-14-28-49_1\" : [0, 2, 3], # ac_session3_1\n",
    "    \"2019-02-14-14-45-49_2\" : [0, 5, 6], # ac_session3_2\n",
    "    '2019-01-29-14-19-26_1' : [0, 3, 4], # bh1_session2_1\n",
    "    '2019-01-29-14-40-36_2' : [0, 1, 2], # bh1_session2_2\n",
    "    '2019-01-30-14-29-29_2' : [4],       # bh1_session3_2\n",
    "    '2019-01-31-09-12-2_1' : [3],         # bh1_session4_1\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4], # bh1_session4_2_part1\n",
    "    '2019-03-05-09-15-11_1' : [1],         # bh2_session5_1\n",
    "    '2019-03-05-09-15-11_2' : [1],        # bh2_session5_2\n",
    "    '2019-02-21-15-55-56_2' : [2],       # ch_session5_2\n",
    "    '2019-01-30-15-19-36_2' : [1],       # jm_session2_1\n",
    "    '2019-01-30-15-04-30_1' : [0],         # jm_session2_2\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [1],  # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [0], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [2, 4],       # ys_session4\n",
    "    '2019-01-30-11-57-3_2' : [0] ,          # ys_session4\n",
    "    '2019-01-31-13-13-2_1' : [4],           # ys_session5\n",
    "    '2019-01-30-10-20-32_1' : [0, 1, 2, 3, 4, 5], # no_session4\n",
    "    '2019-01-30-10-46-38_2' : [0],          # \n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],   # bh2_session3\n",
    "    '2019-03-12-09-30-5_1' : [0],            # kj_session3\n",
    "    '2019-02-13-15-20-38_1' : [0, 1, 2, 3, 6], # ls1_session3\n",
    "    '2019-02-18-10-25-52_1' : [1],              # ls2_session4\n",
    "    '2019-02-18-10-46-26_2' : [0],            # ls2_session4\n",
    "    '2019-01-29-13-25-4_1' : [0, 1, 7],        # ph_session2\n",
    "    '2019-01-29-13-43-50_2' : [0],              # ph_session2\n",
    "    '2019-03-07-16-17-30_1' : [0],              # rh_session1\n",
    "    '2019-03-07-16-44-5_2' : [0, 1],         # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [0, 1, 3]         # rh_session3\n",
    "}\n",
    "\n",
    "# key selection can have extra selections of NextPhrase at the end\n",
    "dict_keySelectionOfNextPhrase = {\n",
    "    \"2019-02-11-11-18-30_1\" : [12, 13], # ac_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [12], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [12], # af_session2\n",
    "    \"2019-02-06-16-19-9_2\" : [12], # af_session3\n",
    "    \"2019-02-12-11-07-43_1\" : [12], # af_session4\n",
    "    \"2019-02-27-15-08-32_1\" : [12], # af_session5\n",
    "    \"2019-01-28-14-30-44_1\" : [12], # bh1_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [12], # bh2_session1\n",
    "    \"2019-02-18-14-02-56_2\" : [12], # le_session1\n",
    "    \"2019-02-19-10-03-14_1\" : [12], # le_session2\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [12], # mn_session1\n",
    "    \"2019-02-08-11-12-51_2\" : [12, 13], # mn_session1\n",
    "    \"2019-02-15-11-38-22_1\" : [12, 13], # mn_session2\n",
    "    \"2019-02-15-11-54-25_2\" : [12], # mn_session2\n",
    "    \"2019-01-16-15-18-0_1\" : [12], # no_session1\n",
    "    \"2019-01-28-13-31-51_1\" : [12], # ph_session1\n",
    "    \"2019-01-28-13-49-14_2\" : [12], # ph_session1\n",
    "    \"2019-01-14-15-07-21_1\" : [12], # ys_session1\n",
    "    \"2019-01-17-15-05-1_1\" : [12], # ys_session3\n",
    "    \"2019-01-30-11-22-25_1\" : [12], # ys_session4\n",
    "    \"2019-01-31-13-32-2_2\" : [12], # ys_session5\n",
    "}\n",
    "\n",
    "\n",
    "# key selection when participants skips some sentences\n",
    "dict_keySelectionNotCompleted = {\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : [0, 1, 3, 5, 7], # af_session1 ---- last sentence is not finished\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [0, 1, 3, 4, 5, 7, 9, 11], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [0, 1, 3, 5, 7, 9, 11], # af_session2 \n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 6, 7, 9, 11], # af_session2\n",
    "    \"2019-02-08-11-33-53_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # aq_session3\n",
    "    \"2019-02-08-12-11-34_2ndPart_1\" : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11], # aq_session3\n",
    "    \"2019-01-28-14-30-44_1\" : [0, 1, 3, 5], # bh1_session1\n",
    "    \"2019-01-31-09-22-49_1stPart_2\": [0, 1, 3, 5, 7, 9, 10, 11], # bh1_session4\n",
    "    \"2019-01-31-09-37-5_2ndPart_2\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # bh1_session4\n",
    "    \"2019-02-21-16-09-44_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # bh2_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [0, 1, 3, 5, 6, 7, 8, 9, 10, 11], # bh2_session1\n",
    "    \"2019-02-28-17-03-53_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # bh2_session3\n",
    "    \"2019-02-28-17-24-2_2ndPart_2\" : [0, 1, 2, 3, 5], # bh2_session3     ----\n",
    "    \"2019-02-14-13-28-20_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # cw_session3\n",
    "    \"2019-02-14-13-57-41_2ndPart_2\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 11], # cw_session3\n",
    "    \"2019-02-21-15-01-4_1stPart_1\" : [0, 1, 2, 3, 5, 7, 9, 11], # le_session3\n",
    "    \"2019-02-21-15-25-56_2ndPart_1\" : [0, 1, 3], # le_session3       ----\n",
    "    \"2019-02-05-14-00-27_1stPart_2\" : [0, 1, 3, 5, 7, 8], # mh_session1\n",
    "    \"2019-02-05-14-10-39_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 7, 8, 9, 11], # mh_session1\n",
    "    \"2019-02-08-10-51-3_1stPart_1\" : [0, 1, 3, 5, 7, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-19-10-34-7_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 11], # mn_session3\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # mn_session3\n",
    "    \"2019-01-29-13-25-4_1\" : [0, 1, 3, 5, 7], # ph_session2  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-18-50_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 10], # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : [0, 1, 3, 5],  # ys_session3  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-02-06-11-25-41_1\" : [0, 1, 3, 5, 11], # aq_session1 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2 -- different for reading and writing, this one is for\n",
    "    # writing\n",
    "    '2019-01-30-11-22-25_1' : [0, 1, 3, 5, 7, 9, 11]   # ys_session4 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "   \n",
    "}\n",
    "\n",
    "# dictionary for phrase removal just like in the dict_phraseStim, but since not all participants require that, some that \n",
    "# do, are added to this new dictionary here\n",
    "dict_keySelection_phraseStim = {\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-16-15-18-0_1' : [4],        # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-03-07-16-44-5_2' : [1],        # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],              # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4]         # ys_session1\n",
    "}\n",
    "\n",
    "\n",
    "# in the beginning experiments, not everyone started with 800 initial dwell time\n",
    "\n",
    "dict_dwellTimeOrig_not800 = {\n",
    "    \"2019-01-16-15-51-13_2\" : 600, # no_session1\n",
    "    \"2019-01-16-15-18-0_1\" : 600, # no_session1\n",
    "    \"2019-01-16-15-43-8_1\" : 100, # af_session1\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : 100, # af_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : 100, # af_session1\n",
    "    \"2019-01-17-15-03-40_1\" : 100, # af_session2\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : 0, # af_session2\n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : 100, # af_session2\n",
    "    \"2019-01-14-15-07-21_1\" : 500, # ys_session1\n",
    "    \"2019-01-14-15-25-55_2\" : 300, # ys_session1\n",
    "    \"2019-01-16-15-18-50_1stpart_1\" : 200, # ys_session2\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 100, # ys_session2\n",
    "    \"2019-01-16-15-59-55_2\" : 100, # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : 100, # ys_session3\n",
    "    \"2019-01-17-15-31-12_2\" : 100 # ys_session3\n",
    "}\n",
    "\n",
    "\n",
    "# list of all things that should be present when computing effective time\n",
    "list_keysToBeCounted = ['Comma', 'BackOne', 'BackMany', 'SpaceBar']\n",
    "\n",
    "# some sessions do not have gaze data\n",
    "dict_noGazeData = {\n",
    "    '2019-01-16-17-00-12_2ndPart_2' : 'no gaze data', # af_session2\n",
    "    '2019-01-17-15-31-12_2' : 'no gaze data', #ys_session2\n",
    "    '2019-01-30-11-57-3_2' : 'no gaze data' # ys_session4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keySelection_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 3, 5], # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_WritingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2   \n",
    "}\n",
    "\n",
    "# normally, reading part of trial ends when people look at the keyboardwithphrases. For some trials, this is not done,\n",
    "# as the reading is done, and the trial is accidentally skipped, and written in the next trial. Here, the trial number \n",
    "# given will have the reading time ending as sleep, and not keyboard with phrases. \n",
    "dict_keyboardNotChange_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 0, # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_firstSleepNotCounted = {\n",
    "    \"2019-01-28-14-50-41_2\" : (0, 2), # bh1_session1 -- 3rd sleep activation to be counted\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : 2  # mn_session3 -- 3rd sleep activation is to be counted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeDwellOrig = 800\n",
    "TimeFixation = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixUserKeys(UserKeys_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    Column_beforeDecimal = [item[2] for item in UserKeys_Old]\n",
    "    Column_afterDecimal = [item[3] if len(item)>3 else '00' for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_ProgressPercent = [float(Column_beforeDecimal[i]+'.'+ Column_afterDecimal[i]) for i in \n",
    "                                range(0, len(Column_beforeDecimal))]\n",
    "    UserKeys_Times = [item[0] for item in UserKeys_Old]\n",
    "    UserKeys_Keys = [item[1] for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_New = [[UserKeys_Times[ind], UserKeys_Keys[ind], UserKeys_ProgressPercent[ind]] for ind in \n",
    "                    range(0, len(UserKeys_ProgressPercent))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    \n",
    "    return UserKeys_New\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixScratchPad(ScratchPad_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    ScratchPad_Times = [item[0] for item in ScratchPad_Old]\n",
    "    \n",
    "    ScratchPad_Phrases = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    ScratchPadInd = -1 \n",
    "    while ScratchPadInd < len(ScratchPad_Old)-1:\n",
    "        ScratchPadInd = ScratchPadInd + 1\n",
    "        commasInPhrase = len(ScratchPad_Old[ScratchPadInd])-2\n",
    "        if commasInPhrase < 1:\n",
    "            #print(ScratchPad_Old[ScratchPadInd][1])\n",
    "            ScratchPad_Phrases.append(ScratchPad_Old[ScratchPadInd][1])\n",
    "            continue\n",
    "        scratchPadPhrase = ScratchPad_Old[ScratchPadInd][1]\n",
    "        for phraseJoinNr in range(1, commasInPhrase+1):\n",
    "            scratchPadPhrase = scratchPadPhrase + ', ' + ScratchPad_Old[ScratchPadInd][1+phraseJoinNr]\n",
    "        \n",
    "        ScratchPad_Phrases.append(scratchPadPhrase)\n",
    "            \n",
    "        \n",
    "    ScratchPad_New = [[ScratchPad_Times[ind], ScratchPad_Phrases[ind]] for ind in \n",
    "                    range(0, len(ScratchPad_Times))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    #print(ScratchPad_New)\n",
    "    return ScratchPad_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixKeysSelected(KeysSelected_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    KeysSelected_New = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    KeysSelectedInd = -1 \n",
    "    while KeysSelectedInd < len(KeysSelected_Old)-1:\n",
    "        KeysSelectedInd = KeysSelectedInd + 1\n",
    "        \n",
    "        if KeysSelected_Old[KeysSelectedInd][1].count(',') > 0:\n",
    "            \n",
    "            keys_split = KeysSelected_Old[KeysSelectedInd][1].split(\"\\r\\n\")\n",
    "            del keys_split[0]\n",
    "            del keys_split[-1]\n",
    "            \n",
    "            keys_split = [key.split(',') for key in keys_split]\n",
    "            \n",
    "            KeysSelected_New.extend(keys_split)\n",
    "        else:\n",
    "            KeysSelected_New.append(KeysSelected_Old[KeysSelectedInd])\n",
    "        \n",
    "    \n",
    "    return KeysSelected_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDwellTime(userKeys, full_path):\n",
    "    # modify userKeys to include a column of time instead of progress pct, which is dependent on the then dwell time\n",
    "    \n",
    "    TimeDwellOrig = 800\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    if session_folder_name in dic_dwellTimeOrig_not800:\n",
    "        TimeDwellOrig = dic_dwellTimeOrig_not800[session_folder_name]\n",
    "    \n",
    "    #print(TimeDwellOrig)\n",
    "    \n",
    "    timeDwell = TimeDwellOrig\n",
    "    nKey = -1\n",
    "    for key in userKeys:\n",
    "        nKey = nKey + 1\n",
    "        #print(key[1])\n",
    "        if key[1] == 'IncreaseDwellTime':\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell + 100\n",
    "        elif key[1] == 'DecreaseDwellTime':\n",
    "            #print(key[2])\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell - 100\n",
    "        else:\n",
    "            userKeys[nKey].append(str(float(key[2])*timeDwell))\n",
    "    \n",
    "    return userKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stimPhrasesEdit(PhraseLog, full_path):\n",
    "   \n",
    "    # Now extract phrases from the phrase file\n",
    "    phraseStim_Phrases = [item[1] for item in PhraseLog]\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "        \n",
    "    phraseStim_PhrasesReduced, phraseStim_timeReduced = zip(*[(x[0], PhraseLog[phraseStim_Phrases.index(x[0])][0]) for x in groupby(phraseStim_Phrases)])\n",
    "    \n",
    "    PhraseLogReduced = [[phraseStim_timeReduced[i], phraseStim_PhrasesReduced[i]] for i in range(0, len(phraseStim_PhrasesReduced))]\n",
    "    \n",
    "    if PhraseLogReduced[-1][1] == 'THE EXPERIMENT IS NOW DONE':\n",
    "        del PhraseLogReduced[-1]\n",
    "        \n",
    "    if PhraseLogReduced[0][1] == 'phraseText':\n",
    "        del PhraseLogReduced[0]\n",
    "\n",
    "    # Here, we want only the sentences typed\n",
    "    notSentencesToType = list()\n",
    "    for index in range(0,len(PhraseLogReduced)):\n",
    "        sentence = PhraseLogReduced[index][1]\n",
    "        if 'Svar på følgende spørgsmål' in sentence or 'Answer the question:' in sentence or 'What is the complete name of your university?' in sentence or '(give a score between 1 and 7)' in sentence or sentence == '':\n",
    "            notSentencesToType.append(index)    \n",
    "    \n",
    "    for index in sorted(notSentencesToType, reverse=True):\n",
    "        del PhraseLogReduced[index]\n",
    "    \n",
    "    replacingList = []\n",
    "    PhraseLogReduced = findAndRemoveTrials(session_name, dict_phraseStim, PhraseLogReduced, replacingList)\n",
    "    \n",
    "    \n",
    "    return PhraseLogReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will return the datetime in items which is the closest to the date pivot\n",
    "def nearestTimePoint(dates, date):\n",
    "    \n",
    "    for d in dates:\n",
    "        if d < date:\n",
    "            nearestTP = d\n",
    "        else:\n",
    "            continue\n",
    "    try: \n",
    "        nearestTP\n",
    "        nearestTPind = dates.index(nearestTP)\n",
    "    except:\n",
    "        nearestTP = 0\n",
    "        nearestTPind = -1\n",
    "        \n",
    "    return nearestTP, nearestTPind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert list of date and time into datetime format list\n",
    "def timeConversion(timeStrList):\n",
    "    timeList = list()\n",
    "    for time in timeStrList:\n",
    "        time1, t1, t2 = time.partition('+')\n",
    "        timeList.append(datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\"))\n",
    "    return timeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptiKeyTypingTime(UserKeys):\n",
    "    \n",
    "    TimeTyping = dict()\n",
    "    \n",
    "    time1, t1, t2 = UserKeys[0][0].partition('+')\n",
    "    startTime = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "    \n",
    "    time2, t1, t2 = UserKeys[-1][0].partition('+')\n",
    "    endTime = datetime.datetime.strptime(re.sub('[:.T]','-',time2[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "    \n",
    "    TimeTyping['startTime'] = startTime\n",
    "    TimeTyping['endTime'] = endTime\n",
    "    \n",
    "    return TimeTyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeTypingStart(userKeys):\n",
    "    # From the user keys, find when the user actually starts typing, after having looked at the phrase and all the other \n",
    "    # function keys\n",
    "    \n",
    "    timeTypingStartInd = 0\n",
    "    \n",
    "    timeTypingStartIndList = list()\n",
    "            \n",
    "    timeUserTimeInd = 0\n",
    "    \n",
    "    ind = 0\n",
    "    # Get start time of first trial\n",
    "    \n",
    "    while ind < len(userKeys):\n",
    "        #print(len(userKeys[ind][1]))\n",
    "        if len(userKeys[ind][1]) > 1:\n",
    "            ind = ind + 1\n",
    "        else:\n",
    "            timeTypingStartInd = ind\n",
    "            timeTypingStartIndList.append(ind)\n",
    "            break\n",
    "    \n",
    "    #print(timeTypingStartInd)\n",
    "    # Get every next phrase start timings\n",
    "    while ind < len(userKeys):\n",
    "        \n",
    "        if userKeys[ind][1] == 'NextPhrase' and float(userKeys[ind][2]) == 1:\n",
    "            \n",
    "            #timeTypingStartIndList.append(ind+1)\n",
    "            for ind2 in range(ind+1, len(userKeys)):\n",
    "                if len(userKeys[ind2][1]) > 1:\n",
    "                    ind = ind + 1\n",
    "                    continue\n",
    "                elif userKeys[ind2][1] == 'NextPhrase' and float(userKeys[ind][2]) == 1:\n",
    "                    ind = ind + 1\n",
    "                    continue\n",
    "                else:\n",
    "                    ind = ind2\n",
    "                    timeTypingStartIndList.append(ind)\n",
    "                    break\n",
    "                    \n",
    "        else:\n",
    "            ind = ind + 1\n",
    "            \n",
    "    #print(timeTypingStartIndList)\n",
    "    \n",
    "    return timeTypingStartIndList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAndRemoveTrials(session_name, dictionary_saved, trials, replacingList):\n",
    "    # function to check the session_name in the dictionary_saved and remove those trials from the dictionary_trial\n",
    "    \n",
    "    if session_name in dictionary_saved:\n",
    "        index_list = dictionary_saved[session_name]\n",
    "    else:\n",
    "        index_list = replacingList\n",
    "    \n",
    "    \n",
    "    if index_list:\n",
    "        if type(trials) == list:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials[index]\n",
    "                \n",
    "        else:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials['start'][index]\n",
    "                del trials['end'][index]\n",
    "        \n",
    "    return trials    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gazeConvert2ColumnsTo1(GazeLog, columnIndwValidity_list, indValidity):\n",
    "    # function to convert pupilsizes from 2 columns for every pupil due to comma use instead of decimal, \n",
    "    # to proper pupil sizes\n",
    "    \n",
    "    #columnInd_list = [joinColumn1_1, joinColumn1_2, joinColumn2_1, joinColumn2_2]\n",
    "    \n",
    "    # number of columns in the final dictionary\n",
    "    nColumns = int(len(columnIndwValidity_list)/2)\n",
    "    \n",
    "    # dictionary of columns that are to be joined later\n",
    "    columns_beforeDecimal = dict()\n",
    "    columns_afterDecimal = dict()\n",
    "    \n",
    "    # dictionary of joined columns\n",
    "    columnsFinal = dict()\n",
    "    \n",
    "    # dictionary to find and equalize missing values in every column\n",
    "    missingVal_column = dict()\n",
    "    missingVal = list()\n",
    "    \n",
    "    # find correct index of validity column to be used, to find the actual columns relative to that\n",
    "    columnsValidity_inUse = list()\n",
    "    \n",
    "    for ind, row in enumerate(GazeLog):\n",
    "        #print(ind)\n",
    "        #print(sorted(list(np.where(np.array(row) == 'Valid')[0])+list(np.where(np.array(row)=='Invalid')[0]))[indValidity])\n",
    "\n",
    "        columnsValidity = (sorted(list(np.where(np.array(row) == 'Valid')[0])+list(np.where(np.array(row)=='Invalid')[0]))[indValidity])\n",
    "        columnsValidity_inUse.append(int(columnsValidity))\n",
    "    \n",
    "    columnsValidity_inUse = np.array(columnsValidity_inUse)\n",
    "    \n",
    "    columnInd_list = [[columnsValidity_inUse+i] for i in columnIndwValidity_list]\n",
    "    \n",
    "    \n",
    "    for ind in range(0, nColumns):\n",
    "        \n",
    "        dict_name = 'column' + str(ind+1)\n",
    "        columnsFinal[dict_name] = list()\n",
    "        columns_afterDecimal[dict_name] = list()\n",
    "                \n",
    "        #for indItem, item4 in enumerate(GazeLog):\n",
    "        #    if 'Invalid' not in item4:\n",
    "        #        if columnInd_list[2*ind+1][0][indItem] < len(item4):\n",
    "        #            columns_afterDecimal[dict_name].append(item4[columnInd_list[2*ind+1][0][indItem]])\n",
    "        #        else:\n",
    "        #            columns_afterDecimal[dict_name].append('0')\n",
    "        #    else:\n",
    "        #        columns_afterDecimal[dict_name].append('nan')\n",
    "        \n",
    "                \n",
    "        columns_beforeDecimal[dict_name] = [item4[columnInd_list[2*ind][0][indItem]] if 'Invalid' not in item4 else 'nan' for indItem, item4 in enumerate(GazeLog)]\n",
    "        columns_afterDecimal[dict_name] = [item4[columnInd_list[2*ind+1][0][indItem]] if 'Invalid' not in item4 and columnInd_list[2*ind+1][0][indItem] < len(item4) else 'nan' for indItem, item4 in enumerate(GazeLog)]\n",
    "\n",
    "        \n",
    "        for i in range(0, len(columns_beforeDecimal[dict_name])):\n",
    "            if 'Valid' not in columns_beforeDecimal[dict_name][i] and 'Valid' not in columns_afterDecimal[dict_name][i]:\n",
    "                if 'nan' not in columns_beforeDecimal[dict_name][i] and 'nan' not in columns_afterDecimal[dict_name][i]:\n",
    "                    if float(columns_afterDecimal[dict_name][i]) > 0: \n",
    "                        columnsFinal[dict_name].append(float(columns_beforeDecimal[dict_name][i]+'.'+columns_afterDecimal[dict_name][i]))\n",
    "                    else:\n",
    "                        columnsFinal[dict_name].append(np.nan)\n",
    "                else:\n",
    "                    columnsFinal[dict_name].append(np.nan)\n",
    "            else:\n",
    "                # Rarely, the pupil size is a whole number\n",
    "                columnsFinal[dict_name].append(np.nan) # we will ignore the row, since there is no way of automatically knowing which - \n",
    "                # right or left eye has whole number pupil size\n",
    "    \n",
    "        missingVal_column[dict_name] = np.argwhere(np.isnan(columnsFinal[dict_name]))\n",
    "        missingVal_column[dict_name] = list(itertools.chain.from_iterable(missingVal_column[dict_name])) # flatten the list\n",
    "        \n",
    "        missingVal.extend(missingVal_column[dict_name])\n",
    "        \n",
    "        \n",
    "    \n",
    "    missingVal = sorted(set(missingVal))\n",
    "    \n",
    "    # if one of the columns are nan, the other one is converted too\n",
    "    for column in range(0, nColumns):\n",
    "        dict_name = 'column' + str(column+1)\n",
    "        for ind in missingVal:\n",
    "            if ind < len(columnsFinal[dict_name]):\n",
    "                columnsFinal[dict_name][ind] = np.nan\n",
    "                \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(len(columnsFinal['column1']), len(columnsFinal['column2']))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return columnsFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterBlinks(pupilDataL, pupilDataR, timeInDatetime):\n",
    "    # filter any blinks and nan values lasting around 250ms (on average)\n",
    "    # first the single nan occurances are replaced with mean of the values on either sides, \n",
    "    # as they are assumed to be from hardware problems\n",
    "    # for the rest of the blinks, 250ms before and after the nan values are interpolated with a linear function\n",
    "    # returns a dataframe with pupil size, and timestamp\n",
    "    # http://faculty.washington.edu/chudler/facts.html\n",
    "   \n",
    "    \n",
    "    # create a dataframe from the pupilsize and time\n",
    "    pupilData_df = pd.DataFrame(list(zip(timeInDatetime, pupilDataL, pupilDataR)), columns=['timeStamp', 'pupilLeft', 'pupilRight'])\n",
    "    \n",
    "    # blink is every nan value in the range of 100-400ms \n",
    "    # 250 ms (22 samples) before and after the blink will also be removed\n",
    "    extraBlinkSamples = 22   \n",
    "    \n",
    "    \n",
    "    #pupilData_woSingleMissingData = pupilData.copy()\n",
    "    #timeList_woSingleMissingData = timeInDatetime.copy()\n",
    "    #timeInS_woSingleMissingData = timeInS_Trial[-1]\n",
    "    \n",
    "    # in case of single missing data, that are due to hardware error, replace with the mean of the pupil size before and\n",
    "    # after the nan value\n",
    "    # missing values will be the same for left and right pupil\n",
    "    missingVal_Single = np.argwhere(np.isnan(pupilDataL))\n",
    "    missingVal_Single = list(itertools.chain.from_iterable(missingVal_Single)) # flatten the list \n",
    "    \n",
    "    # if no blinks present, return the data\n",
    "    if len(missingVal_Single) == 0:\n",
    "        interpolatedNan = np.array([False]*len(pupilData_df))\n",
    "        return pupilData_df, interpolatedNan\n",
    "    \n",
    "    # find the index and values to replace for single nan values\n",
    "    pupilData_tuples_replaceSingleNan_left = [(val, np.mean([pupilDataL[val-1], pupilDataL[val+1]])) for i, val in enumerate(missingVal_Single) if (val != 0 and val != (len(pupilDataL)-1)) if not np.isnan(pupilDataL[val-1]) and not np.isnan(pupilDataL[val+1])]\n",
    "    pupilData_tuples_replaceSingleNan_right = [(val, np.mean([pupilDataR[val-1], pupilDataR[val+1]])) for i, val in enumerate(missingVal_Single) if (val != 0 and val != (len(pupilDataR)-1)) if not np.isnan(pupilDataR[val-1]) and not np.isnan(pupilDataR[val+1])]\n",
    "    \n",
    "    \n",
    "    interpolatedNan = np.array([True if ind in dict(pupilData_tuples_replaceSingleNan_left) else False for ind, val in enumerate(pupilDataL)])\n",
    "    \n",
    "    \n",
    "    # replace the single nan values with the mean of the pupil size on either sides\n",
    "    indList = -1\n",
    "    for ind, val in pupilData_tuples_replaceSingleNan_left:\n",
    "        indList = indList + 1\n",
    "        pupilData_df.iloc[ind, pupilData_df.columns.get_loc('pupilLeft')] = val\n",
    "        pupilData_df.iloc[ind, pupilData_df.columns.get_loc('pupilRight')] = pupilData_tuples_replaceSingleNan_right[indList][1]\n",
    "        \n",
    "    \n",
    "    # again, find the nan values in the pupil size\n",
    "    # the list missingVal_SingleDifference contains the index of the first blink, followed by the difference in the index\n",
    "    # to the next nan value\n",
    "    \n",
    "    \n",
    "    \n",
    "    # find the nan values again from pupilData['pupilLeft']\n",
    "    missingVal_Rest_trueFalse = pupilData_df['pupilLeft'].isnull()\n",
    "    missingVal_Rest = [i for i, x in enumerate(missingVal_Rest_trueFalse) if x]\n",
    "    \n",
    "    # if no blinks left, return the current pupilData\n",
    "    if len(missingVal_Rest) == 0:\n",
    "        return pupilData_df, interpolatedNan\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # in the blinks left, find when the blinks start by finding a difference in the consecutive values of the indices\n",
    "    missingVal_RestDifference = [t - s for s, t in zip(missingVal_Rest, missingVal_Rest[1:])]\n",
    "    missingVal_RestDifference.insert(0, missingVal_Rest[0])\n",
    "    \n",
    "    blinkStart_tupleList = [(ind, sum(missingVal_RestDifference[0:ind+1])) for ind, val in enumerate(missingVal_RestDifference) if val != 1]\n",
    "    \n",
    "    blinkStart_tupleList_wLength = list()\n",
    "    \n",
    "    # create a list of tuples of blink start index and the length of the blink\n",
    "    ind = -1\n",
    "    blinkLengthSum = 0\n",
    "    for blink_ind, blinkStartInd in blinkStart_tupleList:\n",
    "        ind = ind + 1\n",
    "        if ind != len(blinkStart_tupleList) - 1:\n",
    "            \n",
    "            blinkLength = blinkStart_tupleList[ind+1][0]-blink_ind\n",
    "            blinkLengthSum = blinkLengthSum + blinkLength\n",
    "            \n",
    "            blinkStart_tupleList_wLength.append(tuple((blinkStartInd, blinkLength)))\n",
    "        else:\n",
    "            # for the last blink -- all blink lengths summed and subtracted from the length of the list\n",
    "            # missingVal_RestDifference \n",
    "            blinkLength = len(missingVal_RestDifference)-blinkLengthSum\n",
    "            blinkStart_tupleList_wLength.append(tuple((blinkStartInd, blinkLength)))\n",
    "     \n",
    "    # add to vector with start and end of tuple\n",
    "    #beforeAfterNan = [False]*len(pupilData_df['pupilLeft'])\n",
    "    #for blinkStart, blinkLength in blinkStart_tupleList_wLength:\n",
    "    #    beforeAfterNan[blinkStart] = True\n",
    "    #    beforeAfterNan[blinkStart+blinkLength] = True\n",
    "    #    #print('start and end points: ', pupilData_df['timeStamp'][blinkStart], pupilData_df['timeStamp'][blinkStart + blinkLength])\n",
    "    \n",
    "    \n",
    "    # create lists with start and end values for the blinks, based on blinkStart_tupleList_wLength, regardless of the blink length\n",
    "    blink_missingData_startList = [blinkStartInd - extraBlinkSamples if (blinkStartInd - extraBlinkSamples) > 0 else 0 for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    blink_missingData_endList = [blinkStartInd + blinkLength + extraBlinkSamples if (blinkStartInd + blinkLength + extraBlinkSamples) < (len(pupilData_df['pupilLeft'])-1) else (len(pupilData_df['pupilLeft'])-1) for blinkStartInd, blinkLength in blinkStart_tupleList_wLength]\n",
    "    # create a list of tuples from the start and end points of the blink\n",
    "    blink_missingData_startEndTuple = [(blinkStart, blink_missingData_endList[ind]) for ind, blinkStart in enumerate(blink_missingData_startList)] \n",
    "    \n",
    "    \n",
    "    # check if blinks need to be combined - blinksCombine is a list of list of 2 elements, the index of the blinks that should be combined\n",
    "    blinksCombine = [[ind, ind+1] for ind, blink in enumerate(blink_missingData_startEndTuple[0:-1]) if blink[1] > blink_missingData_startEndTuple[ind+1][0]]\n",
    "        \n",
    "    if blinksCombine:\n",
    "        # combine blinks that need to be combined - if multiple consecutive blinks need to be removed: eg - [1, 2], [2, 3] \n",
    "        # are included in the blinksCombine, the combined version should be [1, 3] \n",
    "        blinksCombineFinal = list()\n",
    "        ind = -1\n",
    "        while ind < len(blinksCombine)-2:\n",
    "            \n",
    "            ind = ind + 1\n",
    "            blinkCombining = blinksCombine[ind]\n",
    "            blinksCombineFinal.append(blinkCombining)\n",
    "            while ind < len(blinksCombine)-2 and blinkCombining[1] == blinksCombine[ind+1][0]:\n",
    "                # change the ending of the last added blink of blinksCombineFinal\n",
    "                blinksCombineFinal[-1][1] = blinksCombine[ind+1][1]\n",
    "                ind = ind + 1\n",
    "            \n",
    "            \n",
    "        if len(blinksCombine) == 1:\n",
    "            blinksCombineFinal = blinksCombine.copy()\n",
    "            \n",
    "        \n",
    "        if blinksCombine[-1][1] != blinksCombineFinal[-1][1]:\n",
    "            if blinksCombine[-1][0] == blinksCombineFinal[-1][1]:\n",
    "                blinksCombineFinal[-1][1] = blinksCombine[-1][1]\n",
    "            else:\n",
    "                blinksCombineFinal.append(blinksCombine[-1])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #    for w, z in groupby(sorted(list(blinksCombine)), lambda x, y=itertools.count(): next(y)-x):\n",
    "    #        group = list(z)\n",
    "    #        blinksCombineFinal.append(tuple((group[0], group[-1])))\n",
    "        \n",
    "        for x in sorted(blinksCombineFinal, reverse=True):\n",
    "            new_start = blink_missingData_startEndTuple[x[0]][0] \n",
    "            new_end = blink_missingData_startEndTuple[x[1]][1] \n",
    "            \n",
    "            x_start = x[0]\n",
    "            x_end = x[1]\n",
    "            \n",
    "            # delete also the blinkStart_tupleList_wLength, since it is going to be used to compute other metrics\n",
    "            for blinkRemove in range(x[1], x[0]-1, -1):\n",
    "                del blink_missingData_startEndTuple[blinkRemove]\n",
    "            \n",
    "            blink_missingData_startEndTuple.insert(x[0], tuple((new_start, new_end)))\n",
    "    \n",
    "    \n",
    "    #blinkAndNonBlinkDurationList = [length/90 for start, length in blinkStart_tupleList_wLength]\n",
    "    #timeInS_Trial_filter = timeInS_Trial[-1] - sum(blinkAndNonBlinkDurationList) \n",
    "    \n",
    "    \n",
    "    # remove blinks from data\n",
    "    for blinkStart, blinkEnd in blink_missingData_startEndTuple:\n",
    "        pupilData_df.loc[blinkStart:blinkEnd,'pupilLeft'] = np.nan\n",
    "        pupilData_df.loc[blinkStart:blinkEnd,'pupilRight'] = np.nan\n",
    "        replaceTrueList = range(blinkStart, blinkEnd+1, 1)\n",
    "        interpolatedNan[replaceTrueList] = True\n",
    "    \n",
    "    \n",
    "    \n",
    "    pupilData_df['pupilLeft'] = pupilData_df['pupilLeft'].astype(float).interpolate('linear', limit_direction = 'both')\n",
    "    pupilData_df['pupilRight'] = pupilData_df['pupilRight'].astype(float).interpolate('linear', limit_direction = 'both')\n",
    "    \n",
    "    if pupilData_df.isnull().any().any():\n",
    "        print('nan values in filtered data')\n",
    "        #for i,val in enumerate(pupilData_filter[0:5000]):\n",
    "        #    print(i, val, pupilData_woSingleMissingData[i])\n",
    "        \n",
    "    \n",
    "    return pupilData_df, interpolatedNan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel(dvec, radius=5, nsigma=3, rem_nomed=False):\n",
    "\n",
    "    # replace outliers with median values (hampel filter)\n",
    "    \n",
    "    mvec = pd.Series(dvec).rolling(radius*2+1, center=True, min_periods=radius).median()\n",
    "    svec = 1.4862 * np.abs(dvec-mvec).rolling(radius*2+1, center=True, min_periods=radius).median()\n",
    "    plonk = np.abs(dvec-mvec) > nsigma*svec\n",
    "    dvec = np.array(dvec)\n",
    "    dvec[plonk.tolist()] = mvec[plonk.tolist()]\n",
    "\n",
    "    # remove \"bad data\" where we cannot calculate a median value due to already missing values\n",
    "    if (rem_nomed):\n",
    "        dvec[np.isnan(mvec)] = np.nan\n",
    "    return dvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterPupilSize(GazeLog, TimeTyping, subjectAndSessionName):\n",
    "    # function that uses the list of start and end trial times to find the pupil sizes for those trials and plots them\n",
    "    \n",
    "    # first create a list of times in gaze log\n",
    "    timeStrGazeLog = [item3[0] for item3 in GazeLog]\n",
    "    # convert the list of strings to datetime formats\n",
    "    timeGazeLog = timeConversion(timeStrGazeLog)\n",
    "    \n",
    "    # internal time, to depict seconds\n",
    "    timeInternalGazeLog = [float(item3[1]) for item3 in GazeLog]\n",
    "    \n",
    "    # extract pupil sizes in decimals from the strange 2 columns for every pupil\n",
    "    pupil_indWrtValidityL = [1, 2]\n",
    "    pupil_validityL = 4\n",
    "    pupilLogL_raw = gazeConvert2ColumnsTo1(GazeLog, pupil_indWrtValidityL, pupil_validityL)\n",
    "    \n",
    "    pupil_indWrtValidityR = [1, 2]\n",
    "    pupil_validityR = 5\n",
    "    pupilLogR_raw = gazeConvert2ColumnsTo1(GazeLog, pupil_indWrtValidityR, pupil_validityR)\n",
    "    \n",
    "    \n",
    "    # reduce the data to start and end of typing time\n",
    "    timeTyping_start, timeTyping_startInd = nearestTimePoint(timeGazeLog, TimeTyping['startTime'])\n",
    "    timeTyping_end, timeTyping_endInd = nearestTimePoint(timeGazeLog, TimeTyping['endTime'])\n",
    "    \n",
    "    pupilLogL_wDefinedTime = pupilLogL_raw['column1'][timeTyping_startInd:timeTyping_endInd+1]\n",
    "    pupilLogR_wDefinedTime = pupilLogR_raw['column1'][timeTyping_startInd:timeTyping_endInd+1]\n",
    "    \n",
    "    timeGazeLog_wDefinedTime = timeGazeLog[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    \n",
    "    timeInS_GazeLog_wDefinedTime = timeInternalGazeLog[timeTyping_startInd:timeTyping_endInd+1]\n",
    "    timeInS_Difference = [(t - s)/1000000 for s, t in zip(timeInS_GazeLog_wDefinedTime, timeInS_GazeLog_wDefinedTime[1:])]\n",
    "    timeInS_Difference.insert(0, 0)\n",
    "    \n",
    "    \n",
    "    #timeInS = [sum(timeInS_Difference[:i]) for i, v in enumerate(timeInS_Difference)]\n",
    "    \n",
    "    pupilData_df, interpolated_items = filterBlinks(pupilLogL_wDefinedTime, pupilLogR_wDefinedTime, timeGazeLog_wDefinedTime)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #timeGazeLog_plot = np.arange(0, timeInS[-1], 1/90)\n",
    "    \n",
    "    #plotPupilSize_checkFilter(pupilData_df, pupilLogL_wDefinedTime, blinkStartAndEnd, 'blink removal', subjectAndSessionName)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    pupilData_df_hampel = dict()\n",
    "    \n",
    "    pupilData_df_hampel = pupilData_df.copy()\n",
    "    pupilData_df_hampel['pupilLeft'] = hampel(pupilData_df['pupilLeft'], 25, 3, False)\n",
    "    pupilData_df_hampel['pupilRight'] = hampel(pupilData_df['pupilRight'], 25, 3, False)\n",
    "        \n",
    "    \n",
    "        \n",
    "    return pupilData_df_hampel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindTrialTimes(KeysSelected, timeTyping, full_path):\n",
    "    # function to find start and end of tasks in experiments\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    timeTrialDict = dict()\n",
    "    timeTrialDict = {'start': [],\n",
    "                    'end':[]}\n",
    "    \n",
    "    nTrial = -1\n",
    "    \n",
    "    for keys in KeysSelected:\n",
    "        \n",
    "            \n",
    "        \n",
    "        if keys[1] == 'NextPhrase':\n",
    "            nTrial = nTrial + 1\n",
    "            time1, t1, t2 = keys[0].partition('+')\n",
    "            endTimeTrial = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "            \n",
    "            if nTrial != 0:\n",
    "                # print('end: ', endTimeTrial)\n",
    "                #print('')\n",
    "                timeTrialDict['end'].append(endTimeTrial)\n",
    "            \n",
    "            \n",
    "            # add 5s for the start time of the next phrase\n",
    "            seconds_start = keys[0][17:19]\n",
    "            \n",
    "            if int(seconds_start) > 54:\n",
    "                minute_start = keys[0][14:16]\n",
    "                seconds_start_new = str(int(seconds_start) - 55)\n",
    "            \n",
    "                if int(minute_start) > 58:\n",
    "                    minute_start_new = str(int(minute_start) - 59)\n",
    "                    hour_start_new = str(int(keys[0][11:13]) + 1)\n",
    "                        \n",
    "                else:\n",
    "                    minute_start_new = str(int(minute_start) + 1)\n",
    "                    hour_start_new = str(int(keys[0][11:13]))\n",
    "                        \n",
    "            else:\n",
    "                seconds_start_new = str(int(seconds_start) + 5)\n",
    "                minute_start_new = str(keys[0][14:16])\n",
    "                hour_start_new = str(int(keys[0][11:13]))\n",
    "                    \n",
    "            endTimew5s = keys[0][0:11] + hour_start_new + ':' + minute_start_new + ':' + seconds_start_new + keys[0][19:]\n",
    "            \n",
    "            time1, t1, t2 = endTimew5s.partition('+')\n",
    "            startTimeTrial = datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\")\n",
    "            \n",
    "            #print('start: ', startTimeTrial)\n",
    "            timeTrialDict['start'].append(startTimeTrial)\n",
    "        \n",
    "    del timeTrialDict['start'][-1]\n",
    "    \n",
    "    \n",
    "    # remove the extra selections of NewPhrase at the end of some sessions\n",
    "    replacingList = []\n",
    "    timeTrialDict = findAndRemoveTrials(session_folder_name, dict_keySelectionOfNextPhrase, timeTrialDict, replacingList)\n",
    "    \n",
    "    timeTrialDict_copy = copy.deepcopy(timeTrialDict)\n",
    "    \n",
    "    # separate the reading and writing trials for some participants who read in the actual trial, but write in the next\n",
    "    # trial\n",
    "    if session_folder_name in dict_keySelection_ReadingTrials:\n",
    "        # check the reading and writing separate dictionaries\n",
    "        print('reading and writing sessions are separate')\n",
    "        \n",
    "        #print(len(timeTrialDict['start']))\n",
    "        # writing trials - \n",
    "        timeTrialDict_writing = findAndRemoveTrials(session_folder_name, dict_keySelection_WritingTrials, timeTrialDict, replacingList)\n",
    "        #print(len(timeTrialDict_copy['start']))\n",
    "        \n",
    "        # reading trials\n",
    "        timeTrialDict_reading = findAndRemoveTrials(session_folder_name, dict_keySelection_ReadingTrials, timeTrialDict_copy, replacingList)\n",
    "    else:\n",
    "        # some participants skip some sentences, and then it affects the scoreQuestions too. Remove the skipped sentences or \n",
    "        # remove the score questions \n",
    "        # for these participants, reading and writing trials are the same\n",
    "        \n",
    "        print('same reading and writing trials')\n",
    "        scoreQuestions = [0, 1, 3, 5, 7, 9, 11]\n",
    "        timeTrialDict = findAndRemoveTrials(session_folder_name, dict_keySelectionNotCompleted, timeTrialDict, scoreQuestions)\n",
    "        \n",
    "        # most of the skipped sentences are removed, but for those that are not removed\n",
    "        timeTrialDict_writing = findAndRemoveTrials(session_folder_name, dict_keySelection_phraseStim, timeTrialDict, replacingList)\n",
    "        \n",
    "        timeTrialDict_reading = timeTrialDict_writing \n",
    "        \n",
    "            \n",
    "    return timeTrialDict_reading, timeTrialDict_writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DivideTimeIntoTrials(PupilData, TimeEpochTrial):\n",
    "    # use the dictionary of TimeEpochTrial to find the divide between trials\n",
    "    \n",
    "    EventTrials = dict()\n",
    "    EventTrials['start'] =  [False]*len(PupilData['pupilLeft'])\n",
    "    EventTrials['end'] = [False]*len(PupilData['pupilLeft'])\n",
    "    \n",
    "    \n",
    "    for trialNr in range(0, len(TimeEpochTrial['start'])):\n",
    "        \n",
    "        # find start and end time in gazeLog\n",
    "        timeStart_trial, timeStartInd_trial = nearestTimePoint(PupilData['timeStamp'].tolist(), TimeEpochTrial['start'][trialNr])\n",
    "        timeEnd_trial, timeEndInd_trial = nearestTimePoint(PupilData['timeStamp'].tolist(), TimeEpochTrial['end'][trialNr])\n",
    "        \n",
    "        EventTrials['start'][timeStartInd_trial] = True\n",
    "        EventTrials['end'][timeEndInd_trial] = True\n",
    "        \n",
    "    EventTrials_index = dict()\n",
    "    EventTrials_index['start'] = [i for i, x in enumerate(EventTrials['start']) if x] \n",
    "    EventTrials_index['end'] = [i for i, x in enumerate(EventTrials['end']) if x] \n",
    "            \n",
    "    return EventTrials, EventTrials_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindReadingPartsOfTrial(EventTrials_reading, KeysSelected_new, PupilSize_df, full_path):\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    KeysSelected_timeStr = [key[0] for key in KeysSelected_new]\n",
    "    KeysSelected_time = timeConversion(KeysSelected_timeStr)\n",
    "    \n",
    "    KeysSelected_keys = [key[1] for key in KeysSelected_new]\n",
    "    \n",
    "    EventReading = dict()    \n",
    "    EventReading['start'] = list()\n",
    "    EventReading['end'] = list()\n",
    "    \n",
    "    EventReading_index = dict()    \n",
    "    EventReading_index['start'] = list()\n",
    "    EventReading_index['end'] = list()\n",
    "    \n",
    "    for ind, startTimeInd in enumerate(EventTrials_reading['start']):\n",
    "        \n",
    "        startTimeTrial = PupilSize_df['timeStamp'][startTimeInd]\n",
    "        endTimeTrial = PupilSize_df['timeStamp'][EventTrials_reading['end'][ind]]\n",
    "        \n",
    "        # reading start is the same as trial start\n",
    "        EventReading['start'].append(startTimeTrial)\n",
    "        EventReading_index['start'].append(startTimeInd)\n",
    "        \n",
    "        # start and end time of trial, with respect to the keysSelected\n",
    "        startTrial_keysTime, startTrial_keysInd = nearestTimePoint(KeysSelected_time, startTimeTrial)\n",
    "        endTrial_keysTime, endTrial_keysInd = nearestTimePoint(KeysSelected_time, endTimeTrial)\n",
    "        \n",
    "        # create keysSelected_trial - keys selected in the trial, to make it easier to find the Sleep button selection \n",
    "        # after the trial start\n",
    "        keysSelected_trial = KeysSelected_keys[startTrial_keysInd:endTrial_keysInd]\n",
    "        \n",
    "        # compute the end of reading time -- which is taken when the keyboard with phrases key is pressed\n",
    "        if session_folder_name not in dict_keyboardNotChange_ReadingTrials:\n",
    "            endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('KeyboardWithPhrases')])\n",
    "        else:\n",
    "            if ind == dict_keyboardNotChange_ReadingTrials[session_folder_name]:\n",
    "                endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('Sleep')])\n",
    "            else:\n",
    "                endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('KeyboardWithPhrases')])\n",
    "                \n",
    "                \n",
    "                \n",
    "        # compute the end of reading time -- which is when the sleep button is selected, most of the times\n",
    "        #if session_folder_name not in dict_keySelection_firstSleepNotCounted:\n",
    "        #    endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('Sleep')])\n",
    "            \n",
    "        #else:\n",
    "        #    sleepActivated_allInd = [ind for ind, val in enumerate(keysSelected_trial) if 'Sleep' in val][dict_keySelection_firstSleepNotCounted[session_folder_name]]\n",
    "        #    print(keysSelected_trial[sleepActivated_allInd], KeysSelected_time[startTrial_keysInd+sleepActivated_allInd])\n",
    "        #    endReading_gazeTime, endReading_gazeInd = nearestTimePoint(PupilSize_df['timeStamp'].tolist(), KeysSelected_time[startTrial_keysInd + sleepActivated_allInd])\n",
    "            \n",
    "        #print(KeysSelected_keys[startTrial_keysInd + keysSelected_trial.index('Sleep')], KeysSelected_time[startTrial_keysInd + keysSelected_trial.index('Sleep')], endReading_gazeTime, PupilSize_df['timeStamp'][endReading_gazeInd+1])\n",
    "        \n",
    "        # add the end time of reading, but the next element after the sleep selection matches gaze, to allow complete \n",
    "        # sleep selection\n",
    "        \n",
    "        EventReading['end'].append(PupilSize_df['timeStamp'][endReading_gazeInd+1])\n",
    "        EventReading_index['end'].append(endReading_gazeInd+1)\n",
    "        \n",
    "        #print(ind)\n",
    "        #print('reading: ', EventReading['start'][ind], EventReading['end'][ind])\n",
    "        \n",
    "    \n",
    "    \n",
    "    return EventReading, EventReading_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindWritingPartsOfTrial(EventTrials_writing, PupilSize_df, EventReading_index):\n",
    "    \n",
    "    \n",
    "    EventWriting = dict()    \n",
    "    EventWriting['start'] = list()\n",
    "    EventWriting['end'] = list()\n",
    "    \n",
    "    EventWriting_index = dict()    \n",
    "    EventWriting_index['start'] = list()\n",
    "    EventWriting_index['end'] = list()\n",
    "    \n",
    "    for ind, startTimeInd in enumerate(EventTrials_writing['start']):\n",
    "        startTimeTrial = PupilSize_df['timeStamp'][startTimeInd]\n",
    "        endTimeTrial = PupilSize_df['timeStamp'][EventTrials_writing['end'][ind]]\n",
    "        \n",
    "        endTimeReading = PupilSize_df['timeStamp'][EventReading_index['end'][ind]]\n",
    "        \n",
    "        #print(endTimeReading)\n",
    "        \n",
    "        # for some participants, reading and writing trials are different. So their writing times will not be the end of \n",
    "        # the reading time.\n",
    "        # Regardless, the writing time should start later than when the reading time ends.\n",
    "        # We choose the starting time for writing as the one that is later than the start time from writing trials\n",
    "        # and end time from reading trials\n",
    "        \n",
    "        if startTimeTrial > endTimeReading:\n",
    "            EventWriting['start'].append(startTimeTrial)\n",
    "            EventWriting_index['start'].append(startTimeInd)\n",
    "        else:\n",
    "            EventWriting['start'].append(endTimeReading)\n",
    "            EventWriting_index['start'].append(EventReading_index['end'][ind])\n",
    "        \n",
    "        EventWriting['end'].append(endTimeTrial)\n",
    "        EventWriting_index['end'].append(EventTrials_writing['end'][ind])\n",
    "        \n",
    "        #print(ind)\n",
    "        #print('writing: ', EventWriting['start'][ind], EventWriting['end'][ind])\n",
    "        \n",
    "    #print('Writing')\n",
    "    #print(EventWriting)\n",
    "    \n",
    "    return EventWriting, EventWriting_index     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modmax(d):\n",
    "    # modulus maxima detection\n",
    "    \n",
    "    # compute signal modulus\n",
    "    m = [0.0]*len(d)\n",
    "    for i in range(0, len(d)):\n",
    "        m[i] = math.fabs(d[i])\n",
    "    \n",
    "    # if value is larger than both neighbours , and strictly\n",
    "    # larger than either , then it is a local maximum\n",
    "    t = [0.0]*len(d)\n",
    "    for i in range(0, len(d)):\n",
    "        ll = m[i-1] if i >= 1 else m[i]\n",
    "        oo = m[i]\n",
    "        rr = m[i+1] if i < len(d)-2 else m[i]\n",
    "        if (ll <= oo and oo >= rr) and (ll < oo or oo > rr):\n",
    "            # compute magnitude\n",
    "            t[i] = math.sqrt(d[i]**2)\n",
    "        else:\n",
    "            t[i] = 0.0\n",
    "    #print(len(t))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdwtlevel(pupilData,dwtlvl):\n",
    "    \n",
    "    d = pupilData.pupilSize.tolist()\n",
    "    \n",
    "    cA = pywt.downcoef('a',d,'sym16','per',level=dwtlvl)\n",
    "    cD = pywt.downcoef('d',d,'sym16','per',level=dwtlvl)\n",
    "\n",
    "    cA[:] = [x / math.sqrt(2**dwtlvl) for x in cA]\n",
    "    cD[:] = [x / math.sqrt(2**dwtlvl) for x in cD]\n",
    "\n",
    "    cDm = modmax(cD)\n",
    "    cDt = cDm\n",
    "    \n",
    "    lambda_univ = np.std(cDm) * math.sqrt(2.0*np.log2(len(cDm)))\n",
    "    cDt = pywt.threshold(cDm,lambda_univ,mode=\"hard\")\n",
    "\n",
    "    \n",
    "    return cDt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pIPAlevel(pupilData, cDt):\n",
    "\n",
    "    # this function computes Sandra P. Marshall's Index of Cognitive Activity\n",
    "    # using the DWT of the pupil diamter: pwdt (above) must be run first!\n",
    "\n",
    "    ts = pupilData.timeStamp.iloc[0]\n",
    "    te = pupilData.timeStamp.iloc[-1]\n",
    "    tt = (te - ts).total_seconds()\n",
    "#   print \"ts: \", ts\n",
    "#   print \"te: \", te\n",
    "#   print \"total time: \", tt\n",
    "    ctr = 0\n",
    "    for i in range(len(cDt)):\n",
    "        if math.fabs(cDt[i]) > 0:\n",
    "            ctr += 1\n",
    "    ICA = float(ctr)/tt\n",
    "\n",
    "    return ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_plot(yyy, **kwargs):\n",
    "    \"\"\"Plot signal vector on x [0,1] independently of amount of values it contains.\"\"\"\n",
    "    plt.plot(np.linspace(0, 1, len(yyy)), yyy, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdwtLH1(pupilData, lof, hif):\n",
    "    \n",
    "    d = pupilData.pupilSize.tolist()\n",
    "    d.insert(0, 0)\n",
    "\n",
    "    # lof, hif are low frequency band, high frequency band, resp.\n",
    "    # maxlevel = \\floor{\\log_2{\\left(\\frac{n_d}{n_f-1}\\right)}}\n",
    "    w = pywt.Wavelet('sym16')\n",
    "    maxlevel = pywt.dwt_max_level(len(d), filter_len=w.dec_len)\n",
    "    #print(\"max DWT level: \", maxlevel)\n",
    "\n",
    "    hif = 1\n",
    "    lof = int(maxlevel/2)\n",
    "    #plt.figure()\n",
    "    #plt.plot(w.dec_lo)\n",
    "    coeffs = pywt.wavedec(d, w, level=4)\n",
    "    \n",
    "    plt.figure()\n",
    "    reconstruction_plot(pywt.waverec(coeffs, w))\n",
    "    reconstruction_plot(pywt.waverec(coeffs[:-1] + [None] * 1, w)) # leaving out detail coefficients up to lvl 3\n",
    "    reconstruction_plot(pywt.waverec(coeffs[:-3] + [None] * 3, w)) # leaving out all detail coefficients = reconstruction using lvl1 approximation only\n",
    "    plt.legend(['Full reconstruction', 'Reconstruction using detail coefficients lvl 1+2', 'Reconstruction using lvl 1 approximation only'])\n",
    "    plt.figure()\n",
    "    plt.stem(coeffs[1]); plt.legend(['Lvl 1 detail coefficients'])\n",
    "    plt.figure()\n",
    "    plt.stem(coeffs[2]); plt.legend(['Lvl 2 detail coefficients'])\n",
    "    plt.figure()\n",
    "    plt.stem(coeffs[3]); plt.legend(['Lvl 3 detail coefficients'])\n",
    "    plt.figure()\n",
    "    plt.stem(coeffs[4]); plt.legend(['Lvl 4 detail coefficients'])\n",
    "    \n",
    "    # get the wavelet detail coefficients for high and low frequenices\n",
    "    cD_H = pywt.downcoef('d',d,'sym16','per',level=hif)\n",
    "    cD_L = pywt.downcoef('d',d,'sym16','per',level=lof)\n",
    "\n",
    "    # normalize\n",
    "    cD_H[:] = [x / math.sqrt(2**hif) for x in cD_H]\n",
    "    cD_L[:] = [x / math.sqrt(2**lof) for x in cD_L]\n",
    "\n",
    "    cD_LH = cD_L\n",
    "\n",
    "    # obtain the LH ratio (HF:LH in this case)\n",
    "    #\n",
    "    # note that I am artificially extending the signal by using the 2*i\n",
    "    # index into the original data's timeline\n",
    "#   print \"len(cD_L): \", len(cD_L)\n",
    "#   print \"len(cD_H): \", len(cD_H)\n",
    "    # length of lower frequency detail coefficients will be shorter than\n",
    "    # the high frequency octaves, hence we should iterate over the low\n",
    "    # frequency octave and then use those inndices to index to the high\n",
    "    # frequency octave\n",
    "    for i in range(len(cD_L)):\n",
    "#     print \"cD_L: \", cD_L[i]\n",
    "#     print \"cD_H: \", cD_H[((2**lof)/(2**hif))*i]\n",
    "      # does low:high ratio really make sense?\n",
    "        cD_LH[i] = cD_L[i] / cD_H[int(((2**lof)/(2**hif))*i)]\n",
    "      # why not high:low signal ratio\n",
    "#     cD_LH[i] = cD_H[((2**lof)/(2**hif))*i] / cD_L[i]\n",
    "\n",
    "    \n",
    "    # the rest of the algorithm proceeds like the original IPA:\n",
    "    # find the modmax spikes in the LH signal and threshold\n",
    "    cD_LHm = modmax(cD_LH)\n",
    "    cD_LHt = cD_LHm\n",
    "    lambda_univ = np.std(cD_LHm) * math.sqrt(2.0*np.log2(len(cD_LHm)))\n",
    "    cD_LHt = pywt.threshold(cD_LHm,lambda_univ,mode=\"hard\")\n",
    "\n",
    "    \n",
    "    return cD_LHt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet_cwt(ax, time, signal, scales, waveletname = 'cmor', \n",
    "                 cmap = plt.cm.seismic, title = '', ylabel = '', xlabel = ''):\n",
    "    \n",
    "    dt = time[1] - time[0]\n",
    "    [coefficients, frequencies] = pywt.cwt(signal, scales, waveletname, dt)\n",
    "    power = (abs(coefficients)) ** 2\n",
    "    period = 1. / frequencies\n",
    "    levels = [0.0625, 0.125, 0.25, 0.5, 1, 2, 4, 8]\n",
    "    contourlevels = np.log2(levels)\n",
    "    \n",
    "    im = ax.contourf(time, np.log2(period), np.log2(power), contourlevels, extend='both',cmap=cmap)\n",
    "    \n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.set_ylabel(ylabel, fontsize=18)\n",
    "    ax.set_xlabel(xlabel, fontsize=18)\n",
    "    \n",
    "    yticks = 2**np.arange(np.ceil(np.log2(period.min())), np.ceil(np.log2(period.max())))\n",
    "    ax.set_yticks(np.log2(yticks))\n",
    "    ax.set_yticklabels(yticks)\n",
    "    ax.invert_yaxis()\n",
    "    ylim = ax.get_ylim()\n",
    "    ax.set_ylim(ylim[0], -1)\n",
    "    return yticks, ylim\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wavelet_dwt(time, signal, level, indTrial):\n",
    "    \n",
    "    dt = time[1] - time[0]\n",
    "    w = pywt.Wavelet('sym16')\n",
    "    maxlevel = pywt.dwt_max_level(len(signal), filter_len=w.dec_len)\n",
    "    \n",
    "    #print(len(signal))\n",
    "    \n",
    "    coefficients = pywt.wavedec(signal, 'sym16', mode='symmetric', level=1, axis=-1)\n",
    "    \n",
    "    n = np.shape(coefficients)[0]\n",
    "    print(n)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig.add_subplot(n+1,1,1)\n",
    "    ax1.plot(signal)\n",
    "    ax1.set_ylabel('signal')\n",
    "    \n",
    "    PlotTitle = 'level1'\n",
    "    plt.title('{} decomposition on trial {}'.format(PlotTitle, indTrial+1))\n",
    "    \n",
    "    # plot the approximation coefficients\n",
    "    ax = fig.add_subplot(n+1,1,2)\n",
    "    ax.plot(coefficients[0])\n",
    "    ax.set_ylabel('cA')\n",
    "    \n",
    "    # plot the determinant coefficients\n",
    "    for ind in range(1, n):\n",
    "        ax = fig.add_subplot(n+1, 1, ind+2)\n",
    "        ax.plot(coefficients[ind])\n",
    "        ax.set_ylabel('cD{}'.format(n-ind))\n",
    "        \n",
    "    \n",
    "    pickle.dump(fig, open(r'C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions' + '\\\\' + PlotTitle + '_trial' + str(indTrial), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdwtLH(pupilData, lof, hif, indTrial):\n",
    "    \n",
    "    d = pupilData.pupilSize.tolist()\n",
    "    d.insert(0, 0)\n",
    "\n",
    "    # lof, hif are low frequency band, high frequency band, resp.\n",
    "    # maxlevel = \\floor{\\log_2{\\left(\\frac{n_d}{n_f-1}\\right)}}\n",
    "    w = pywt.Wavelet('sym16')\n",
    "    maxlevel = pywt.dwt_max_level(len(d), filter_len=w.dec_len)\n",
    "    print(\"max DWT level: \", maxlevel)\n",
    "\n",
    "    hif = 1\n",
    "    lof = int(maxlevel/2)\n",
    "    \n",
    "    scales = np.arange(1, 128)\n",
    "    title = 'Wavelet Transform (Power Spectrum) of signal'\n",
    "    ylabel = 'Period (years)'\n",
    "    xlabel = 'Time'\n",
    "\n",
    "    \n",
    "    samplingFreq = 90\n",
    "    time = np.arange(0, len(d)) / samplingFreq\n",
    "    plot_wavelet_dwt(time, d, level = maxlevel, indTrial=indTrial)\n",
    "    \n",
    "    \n",
    "    # get the wavelet detail coefficients for high and low frequenices\n",
    "    cD_H = pywt.downcoef('d',d,'sym16','per',level=hif)\n",
    "    cD_L = pywt.downcoef('d',d,'sym16','per',level=lof)\n",
    "\n",
    "    # normalize\n",
    "    cD_H[:] = [x / math.sqrt(2**hif) for x in cD_H]\n",
    "    cD_L[:] = [x / math.sqrt(2**lof) for x in cD_L]\n",
    "\n",
    "    cD_LH = cD_L\n",
    "\n",
    "    # obtain the LH ratio (HF:LH in this case)\n",
    "    #\n",
    "    # note that I am artificially extending the signal by using the 2*i\n",
    "    # index into the original data's timeline\n",
    "#   print \"len(cD_L): \", len(cD_L)\n",
    "#   print \"len(cD_H): \", len(cD_H)\n",
    "    # length of lower frequency detail coefficients will be shorter than\n",
    "    # the high frequency octaves, hence we should iterate over the low\n",
    "    # frequency octave and then use those inndices to index to the high\n",
    "    # frequency octave\n",
    "    for i in range(len(cD_L)):\n",
    "#     print \"cD_L: \", cD_L[i]\n",
    "#     print \"cD_H: \", cD_H[((2**lof)/(2**hif))*i]\n",
    "      # does low:high ratio really make sense?\n",
    "        cD_LH[i] = cD_L[i] / cD_H[int(((2**lof)/(2**hif))*i)]\n",
    "      # why not high:low signal ratio\n",
    "#     cD_LH[i] = cD_H[((2**lof)/(2**hif))*i] / cD_L[i]\n",
    "\n",
    "    \n",
    "    # the rest of the algorithm proceeds like the original IPA:\n",
    "    # find the modmax spikes in the LH signal and threshold\n",
    "    cD_LHm = modmax(cD_LH)\n",
    "    cD_LHt = cD_LHm\n",
    "    lambda_univ = np.std(cD_LHm) * math.sqrt(2.0*np.log2(len(cD_LHm)))\n",
    "    cD_LHt = pywt.threshold(cD_LHm,lambda_univ,mode=\"hard\")\n",
    "\n",
    "    \n",
    "    return cD_LHt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pIPALH(pupilData, cD_LHt):\n",
    "\n",
    "    # this function computes Sandra P. Marshall's Index of Cognitive Activity\n",
    "    # using the DWT of the pupil diamter: pwdt (above) must be run first!\n",
    "\n",
    "    ts = pupilData.timeStamp.iloc[0]\n",
    "    te = pupilData.timeStamp.iloc[-1]\n",
    "    tt = (te - ts).total_seconds()\n",
    "#   print \"ts: \", ts\n",
    "#   print \"te: \", te\n",
    "#   print \"total time: \", tt\n",
    "    ctr = 0\n",
    "    for i in range(len(cD_LHt)):\n",
    "        if math.fabs(cD_LHt[i]) > 0:\n",
    "            ctr += 1\n",
    "    ICA_LH = float(ctr)/tt\n",
    "\n",
    "    return ICA_LH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ipaComputeForTrials(Events, PupilData):\n",
    "    \n",
    "    ipaNew_list = list()\n",
    "    for ind in range(0, len(Events['start'])): \n",
    "        eventStartTime, eventStartInd = nearestTimePoint(PupilData['timeStamp'].tolist(), Events['start'][ind])\n",
    "        eventEndTime, eventEndInd = nearestTimePoint(PupilData['timeStamp'].tolist(), Events['end'][ind])\n",
    "        \n",
    "        trialPupilSize = PupilData.iloc[eventStartInd:eventEndInd,:]\n",
    "        \n",
    "        pupilMean = (np.array(trialPupilSize['pupilLeft']) + np.array(trialPupilSize['pupilRight']))/2 \n",
    "        df = pd.DataFrame(list(zip(trialPupilSize['timeStamp'], pupilMean)), columns=['timeStamp', 'pupilSize'])\n",
    "        \n",
    "    \n",
    "        #herz = 90\n",
    "        #sfdegree = 3\n",
    "        #sfcutoff = 1.0/herz\n",
    "        dwtlvl = 2\n",
    "        lof = 4\n",
    "        hif = 1\n",
    "        \n",
    "        #cDt = pdwtlevel(df,dwtlvl)\n",
    "        #ICA = pIPAlevel(df, cDt)\n",
    "        \n",
    "        cD_LHt = pdwtLH(df, lof, hif, ind)\n",
    "        ICA_LH = pIPALH(df, cD_LHt)        \n",
    "        \n",
    "        ipaNew_list.append(ICA_LH)\n",
    "    \n",
    "    \n",
    "    return ipaNew_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataForEveryTrial:\n",
    "    subjectID = ''\n",
    "    blockNumber = ''\n",
    "    sessionNumber = ''\n",
    "    variable = ''\n",
    "    dataForTrial = ''\n",
    "    resultPathName = ''\n",
    "   \n",
    "    \n",
    "    def printInfo(self):\n",
    "        dataFrame = pd.DataFrame(list(zip([self.subjectID]*len(self.dataForTrial), [self.blockNumber]*len(self.dataForTrial), [self.sessionNumber]*len(self.dataForTrial), range(0,len(self.dataForTrial)+1), self.dataForTrial)), columns=['subjectID', 'block', 'session', 'trial', self.variable])\n",
    "        \n",
    "        return dataFrame\n",
    "    \n",
    "    def AddToFile(self):\n",
    "        \n",
    "        dataFrame = pd.DataFrame(list(zip([self.subjectID]*len(self.dataForTrial), [self.blockNumber]*len(self.dataForTrial), [self.sessionNumber]*len(self.dataForTrial), range(0,len(self.dataForTrial)+1), self.dataForTrial)), columns=['subjectID', 'block', 'session', 'trial', self.variable])\n",
    "        book = load_workbook(self.resultPathName)\n",
    "        writer = pd.ExcelWriter(self.resultPathName, engine='openpyxl')\n",
    "        writer.book = book\n",
    "        writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "        startrow = writer.sheets['Sheet1'].max_row\n",
    "        dataFrame.to_excel(writer, startrow = startrow, index = False, header = False)\n",
    "        \n",
    "        writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CombineReadingWriting(EventReading, EventWriting):\n",
    "    \n",
    "    EventTrial = EventReading.copy()\n",
    "    EventTrialEnd = [endTime for endTime in EventWriting['end']]\n",
    "    EventTrial['end'] = EventTrialEnd\n",
    "    \n",
    "    return EventTrial       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject path C:\\DTU\\Data\\201901_JanuaryExpt\\no\\1\\2019-01-16-15-18-0_1\n",
      "subject and session name:  no__1__2019-01-16-15-18-0_1\n",
      "same reading and writing trials\n",
      "max DWT level:  7\n",
      "2\n",
      "max DWT level:  8\n",
      "2\n",
      "max DWT level:  8\n",
      "2\n",
      "max DWT level:  8\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "metricComputed_read = 'IpaNewReading_8s'\n",
    "metricComputed_write = 'IpaNewWriting_8s'\n",
    "#metricComputed_trial = 'IpaNewTrial_8s'\n",
    "\n",
    "#dataFolderName = r'E:\\Data\\Data' # accessing external hard disk with the data\n",
    "#a = re.compile('(?<=Data\\\\\\\\Data\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "#subjectName_listElement = 3\n",
    "\n",
    "dataFolderName = r'C:\\DTU\\Data\\201901_JanuaryExpt' # accessing data saved in the computer\n",
    "a = re.compile('(?<=Data\\\\\\\\201901_JanuaryExpt\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "subjectName_listElement = 4\n",
    "\n",
    "resultFileName_read = r'C:\\DTU\\Data\\201901_JanuaryExpt\\DataExtracted\\IndividualTrials\\Subject_Block_Session_Trial_' + metricComputed_read +  '.xlsx'\n",
    "resultFileName_write = r'C:\\DTU\\Data\\201901_JanuaryExpt\\DataExtracted\\IndividualTrials\\Subject_Block_Session_Trial_' + metricComputed_write +  '.xlsx'\n",
    "#resultFileName_trial = r'C:\\DTU\\Data\\201901_JanuaryExpt\\DataExtracted\\IndividualTrials\\Subject_Block_Session_Trial_' + metricComputed_trial +  '.xlsx'\n",
    "\n",
    "\n",
    "\n",
    "for root, dirs, subfolder in os.walk(dataFolderName):\n",
    "    \n",
    "    technique = 'dwell_time'\n",
    "    \n",
    "    if not dirs:\n",
    "        \n",
    "        #if 'notCompleted' in root or 'notInclude' in root: # Some subjects do not have gaze log and have been marked as \n",
    "        \n",
    "        if 'noData' in root or 'Trial' in root or 'trial' in root or 'Nothing' in root: # Some subjects do not have gaze log and have been marked as \n",
    "            #notInclude\n",
    "            continue\n",
    "        if 'Jonas' in root or 'Praktikant' in root or 'Villads' in root:\n",
    "            continue\n",
    "            \n",
    "        if 'no\\\\1\\\\2019-01-16-15-18-0_1' not in root:\n",
    "            continue\n",
    "        if 'Picture' in root:\n",
    "            continue\n",
    "        #if '_MS' not in root:\n",
    "        #    continue\n",
    "        \n",
    "        \n",
    "        \n",
    "        #if '2019-1-16-16-36-17_1stPart_2' not in root:\n",
    "        #    continue\n",
    "            \n",
    "        keysSelected = None\n",
    "        userKeys = None\n",
    "        phraseLog = None\n",
    "        \n",
    "        for file in subfolder:\n",
    "            \n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'KeySelection*'):\n",
    "                try:\n",
    "                    \n",
    "                    fKeysSelected = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerKeysSelected = csv.reader(fKeysSelected)\n",
    "                    keysSelected = list(readerKeysSelected)\n",
    "                    \n",
    "                    keysSelected.remove(keysSelected[0])\n",
    "                except:\n",
    "                    if fKeysSelected is not None:\n",
    "                        fKeysSelected.close()\n",
    "                    else:\n",
    "                        print('error in opening the KeySelection log file')\n",
    "                        \n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'user*'):\n",
    "                try:\n",
    "                    fUserKey = open(root + '\\\\' + file, encoding='utf-8',  newline='')\n",
    "                    readerUserKey = csv.reader(fUserKey, quotechar=None)\n",
    "                    userKeys = list(readerUserKey)\n",
    "                    userKeys.remove(userKeys[0])\n",
    "                except:\n",
    "                    if fUserKey is not None:\n",
    "                        fUserKey.close()\n",
    "                    else:\n",
    "                        print('error in opening the user key log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'phrase*'):\n",
    "                try:\n",
    "                    fPhraseLog = open(root + '\\\\' + file, encoding='utf-8')\n",
    "                    readerPhraseLog = csv.reader(fPhraseLog, quotechar=None)\n",
    "                    phraseLog = list(readerPhraseLog)\n",
    "                    \n",
    "                except:\n",
    "                    if fPhraseLog is not None:\n",
    "                        fPhraseLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the phrase log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'multiKey*'):\n",
    "                technique = 'multiKey_selection'\n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'tobiiGazeLog*'):\n",
    "                try:\n",
    "                    fGazeLog = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerGazeLog = csv.reader(fGazeLog, quotechar=None)\n",
    "                    gazeLog = list(readerGazeLog)\n",
    "                    gazeLog.remove(gazeLog[0])\n",
    "                    gazeLog.remove(gazeLog[-1])\n",
    "                    \n",
    "                except:\n",
    "                    if fGazeLog is not None:\n",
    "                        fGazeLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the scratchpad log file')\n",
    "            \n",
    "                    \n",
    "                     \n",
    "        if keysSelected is None or userKeys is None or phraseLog is None or gazeLog is None:\n",
    "            continue\n",
    "        else:\n",
    "                \n",
    "            print('subject path', root)\n",
    "            subjAndSessionName = '__'.join(root.split('\\\\')[subjectName_listElement:])\n",
    "            subjName = subjAndSessionName.split('__')[0]\n",
    "            print('subject and session name: ', subjAndSessionName)\n",
    "            sessionFolderName = root.split('\\\\')[-1]\n",
    "            \n",
    "            \n",
    "            # fix phraselog due to comma related file changes\n",
    "            phraseLog_new = FixScratchPad(phraseLog)\n",
    "            \n",
    "            # fix userKeys due to comma related file changes\n",
    "            userKeys_new = FixUserKeys(userKeys)\n",
    "            \n",
    "            \n",
    "            # need to fix keys selected, where rows get combined because of an inverted comma\n",
    "            keysSelected_new = FixKeysSelected(keysSelected)\n",
    "            \n",
    "            # find start time of typing\n",
    "            timeTyping = OptiKeyTypingTime(userKeys_new)\n",
    "            \n",
    "            # divide complete data into epochs of phrases\n",
    "            timeStartEndDict_reading, timeStartEndDict_writing = FindTrialTimes(keysSelected_new, timeTyping, root)\n",
    "            \n",
    "            if sessionFolderName in dict_noGazeData:\n",
    "                print('no gaze data present')\n",
    "                ipa_reading = [np.nan]*len(timeStartEndDict_reading['start'])\n",
    "                ipa_writing = [np.nan]*len(timeStartEndDict_writing['start'])\n",
    "                ipa_trial = [np.nan]*len(timeStartEndDict_writing['start'])\n",
    "                \n",
    "            else:\n",
    "                \n",
    "            \n",
    "               # filter the data\n",
    "                pupilData_filtered = FilterPupilSize(gazeLog, timeTyping, subjAndSessionName)\n",
    "            \n",
    "                # find a vector indicating True for the start and end of trials, False otherwise\n",
    "                eventTrials_reading, eventTrials_readingIndex = DivideTimeIntoTrials(pupilData_filtered, timeStartEndDict_reading)\n",
    "                eventTrials_writing, eventTrials_writingIndex = DivideTimeIntoTrials(pupilData_filtered, timeStartEndDict_writing)\n",
    "            \n",
    "                # divide trials into reading and writing\n",
    "                eventReading, eventReading_index = FindReadingPartsOfTrial(eventTrials_readingIndex, keysSelected_new, pupilData_filtered, root)\n",
    "                eventWriting, eventWriting_index = FindWritingPartsOfTrial(eventTrials_writingIndex, pupilData_filtered, eventReading_index)\n",
    "                eventTrial = CombineReadingWriting(eventReading, eventWriting)\n",
    "                \n",
    "                ipa_reading = ipaComputeForTrials(eventReading, pupilData_filtered)\n",
    "                #ipa_writing = ipaComputeForTrials(eventWriting, pupilData_filtered)\n",
    "                #ipa_trial = ipaComputeForTrials(eventTrial, pupilData_filtered)\n",
    "                \n",
    "                \n",
    "            if '1stPart' in root:\n",
    "                print('1stPart')\n",
    "                ipa_reading1 = ipa_reading\n",
    "                ipa_writing1 = ipa_writing\n",
    "                ipa_trial1 = ipa_trial\n",
    "                continue\n",
    "            \n",
    "            if '2ndPart' in root:\n",
    "                print('2ndPart')\n",
    "                ipa_reading2 = ipa_reading\n",
    "                ipa_writing2 = ipa_writing\n",
    "                ipa_trial2 = ipa_trial\n",
    "                \n",
    "                ipa_reading = ipa_reading1 + ipa_reading2\n",
    "                ipa_writing = ipa_writing1 + ipa_writing2\n",
    "                ipa_trial = ipa_trial1 + ipa_trial2\n",
    "                \n",
    "                ipa_reading1 = list()\n",
    "                ipa_writing1 = list()\n",
    "                ipa_trial1 = list()\n",
    "            \n",
    "            \n",
    "            # save the ipa_reading\n",
    "            dataToSave_read = DataForEveryTrial()\n",
    "            dataToSave_read.subjectID = subjAndSessionName.split('__')[0]\n",
    "            dataToSave_read.blockNumber = subjAndSessionName.split('__')[1]\n",
    "            dataToSave_read.sessionNumber = subjAndSessionName[-1]\n",
    "            dataToSave_read.variable = metricComputed_read\n",
    "            dataToSave_read.dataForTrial = ipa_reading\n",
    "            dataToSave_read.resultPathName = resultFileName_read\n",
    "            \n",
    "            #print(dataToSave_read.printInfo())\n",
    "            #dataToSave_read.AddToFile()\n",
    "            \n",
    "            \n",
    "            # save the ipa_writing\n",
    "            dataToSave_write = DataForEveryTrial()\n",
    "            dataToSave_write.subjectID = subjAndSessionName.split('__')[0]\n",
    "            dataToSave_write.blockNumber = subjAndSessionName.split('__')[1]\n",
    "            dataToSave_write.sessionNumber = subjAndSessionName[-1]\n",
    "            dataToSave_write.variable = metricComputed_write\n",
    "            dataToSave_write.dataForTrial = ipa_writing\n",
    "            dataToSave_write.resultPathName = resultFileName_write\n",
    "            \n",
    "            #print(dataToSave_write.printInfo())\n",
    "            #dataToSave_write.AddToFile()\n",
    "            \n",
    "            \"\"\"\n",
    "            # save the ipa_trial\n",
    "            dataToSave_trial = DataForEveryTrial()\n",
    "            dataToSave_trial.subjectID = subjAndSessionName.split('__')[0]\n",
    "            dataToSave_trial.blockNumber = subjAndSessionName.split('__')[1]\n",
    "            dataToSave_trial.sessionNumber = subjAndSessionName[-1]\n",
    "            dataToSave_trial.variable = metricComputed_trial\n",
    "            dataToSave_trial.dataForTrial = ipa_trial\n",
    "            dataToSave_trial.resultPathName = resultFileName_trial\n",
    "            \n",
    "            #print(dataToSave_trial.printInfo())\n",
    "            #dataToSave_trial.AddToFile()\n",
    "            \n",
    "            \"\"\"\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the interactive plots saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\DTU\\Results\\201901_Expt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\level1_trial0\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\level1_trial1\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\level1_trial2\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\level1_trial3\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_half_trial0\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_half_trial1\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_half_trial2\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_half_trial3\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_trial0\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_trial1\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_trial2\n",
      "C:\\DTU\\Results\\201901_Expt\\ipa\\WaveletDecompositions\\maxlevel_trial3\n"
     ]
    }
   ],
   "source": [
    "# create a list of all saved figures in the folder\n",
    "\n",
    "path_dwt = path + '\\\\ipa\\\\WaveletDecompositions' + '\\\\'\n",
    "\n",
    "files_dwt = []\n",
    "# r=root, d=directories, f = files\n",
    "for r, d, f in os.walk(path_dwt):\n",
    "    for file in f:\n",
    "        files_dwt.append(os.path.join(r, file))\n",
    "\n",
    "for f in files_dwt:\n",
    "    print(f)\n",
    "    figx = pickle.load(open(f, 'rb'))\n",
    "\n",
    "    figx.show() # Show the figure, edit it, etc.!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
