{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import fnmatch\n",
    "from pathlib import Path\n",
    "from itertools import groupby\n",
    "import copy\n",
    "import itertools\n",
    "import distance\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import snowball\n",
    "\n",
    "from itertools import *\n",
    "from operator import *\n",
    "\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceptional removal of particular extra sentences not typed by the user \n",
    "dict_phraseStim = {\n",
    "    #'2019-02-05-14-10-39_2ndPart_2' : [1, 2, 3, 4, 5, 6, 9, 10],\n",
    "    #'2019-01-14-14-58-30' : [0], # ys, session_trial ()\n",
    "    '2019-01-16-16-36-17_1stPart_2' : [-1], # af_session1\n",
    "    '2019-01-16-17-00-12_2ndPart_2': [1], # af_session1\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-17-16-03-27_2ndPart_2' : [0, 1, 2], # Af session2\n",
    "    '2019-02-06-11-25-41_1' : [7],               # aq_session1    \n",
    "    '2019-02-08-11-33-53_1stPart_1' : [1],  # aq session3_1_part1\n",
    "    '2019-02-08-12-11-34_2ndPart_1' : [0, 1, 2, 3],  # aq session3_1_part2\n",
    "    '2019-01-31-09-37-5_2ndPart_2' : range(1,5), # bh1, session 4 , all sentences except the first one deleted\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4],  # bh1_session4_2_part1\n",
    "    '2019-02-21-16-09-44_1stPart_1' : [1], # bh2_session1\n",
    "    '2019-02-21-16-22-22_2ndPart_1' : [2, 3, 4],# bh2_session1\n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],       # bh2_session3\n",
    "    '2019-02-28-17-24-2_2ndPart_2' : [0, 2],     # bh2_session3\n",
    "    '2019-02-14-13-28-20_1stPart_2' : [2], # cw_session3_2_part1\n",
    "    '2019-02-14-13-57-41_2ndPart_2' : [0, 2, 3], # cw_session3_2_part2\n",
    "    '2019-02-21-15-01-4_1stPart_1' : [0],        # le_session3\n",
    "    '2019-02-21-15-25-56_2ndPart_1' : [1],        # le_session3\n",
    "    #'2019-02-18-10-28-35_2' : [0],               # ls2_session4 # picture not described\n",
    "    '2019-02-05-14-00-27_1stPart_2' : [3],        # mh_session1\n",
    "    '2019-02-05-14-10-39_2ndPart_2' : [0, 1, 3],   # mh_session1\n",
    "    '2019-02-08-10-51-3_1stPart_1' : [4],        # mn_session1\n",
    "    '2019-02-08-11-05-7_2ndPart_1' : [0, 2, 3, 4], # mn_session1\n",
    "    '2019-02-19-10-34-7_1stPart_1' : [3],          # mn_session3\n",
    "    '2019-02-19-10-56-43_2ndPart_1' : [1, 2, 3, 4], # mn_session3\n",
    "    '2019-01-16-15-18-0_1' : [4],            # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-01-29-13-25-4_1' : [3],        # ph_session2\n",
    "    '2019-03-07-16-44-5_2' : [1],                   # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],                  # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4], # ys_session1\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [3, 4], # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [2], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [3, 5, 7],          # ys_session4\n",
    "    '2019-01-30-11-22-25_1' : [4, 6, 7] # ys, session 4\n",
    "}\n",
    "\n",
    "# exceptional removal of sentences/words typed by the user, but then deleted everything to have a blank scratchpad\n",
    "\n",
    "dict_phraseUser = {\n",
    "    \"2019-02-06-15-44-15_1\" : [2, 3, 6], \n",
    "    \"2019-02-06-16-19-9_2\" : [1, 3, 6, 7],\n",
    "    \"2019-02-12-11-21-21_2\" : [0],\n",
    "    \"2019-02-14-14-28-49_1\" : [0, 2, 3], # ac_session3_1\n",
    "    \"2019-02-14-14-45-49_2\" : [0, 5, 6], # ac_session3_2\n",
    "    '2019-01-29-14-19-26_1' : [0, 3, 4], # bh1_session2_1\n",
    "    '2019-01-29-14-40-36_2' : [0, 1, 2], # bh1_session2_2\n",
    "    '2019-01-30-14-29-29_2' : [4],       # bh1_session3_2\n",
    "    '2019-01-31-09-12-2_1' : [3],         # bh1_session4_1\n",
    "    '2019-01-31-09-22-49_1stPart_2' : [4], # bh1_session4_2_part1\n",
    "    '2019-03-05-09-15-11_1' : [1],         # bh2_session5_1\n",
    "    '2019-03-05-09-15-11_2' : [1],        # bh2_session5_2\n",
    "    '2019-02-21-15-55-56_2' : [2],       # ch_session5_2\n",
    "    '2019-01-30-15-19-36_2' : [1],       # jm_session2_1\n",
    "    '2019-01-30-15-04-30_1' : [0],         # jm_session2_2\n",
    "    '2019-01-16-15-18-50_1stPart_1' : [1],  # ys_session2\n",
    "    '2019-01-16-15-42-51_2ndPart_1' : [0], # ys_session2\n",
    "    '2019-01-30-11-22-25_1' : [2, 4],       # ys_session4\n",
    "    '2019-01-30-11-57-3_2' : [0] ,          # ys_session4\n",
    "    '2019-01-31-13-13-2_1' : [4],           # ys_session5\n",
    "    '2019-01-30-10-20-32_1' : [0, 1, 2, 3, 4, 5], # no_session4\n",
    "    '2019-01-30-10-46-38_2' : [0],          # \n",
    "    '2019-02-28-17-03-53_1stPart_2' : [2],   # bh2_session3\n",
    "    '2019-03-12-09-30-5_1' : [0],            # kj_session3\n",
    "    '2019-02-13-15-20-38_1' : [0, 1, 2, 3, 6], # ls1_session3\n",
    "    '2019-02-18-10-25-52_1' : [1],              # ls2_session4\n",
    "    '2019-02-18-10-46-26_2' : [0],            # ls2_session4\n",
    "    '2019-01-29-13-25-4_1' : [0, 1, 7],        # ph_session2\n",
    "    '2019-01-29-13-43-50_2' : [0],              # ph_session2\n",
    "    '2019-03-07-16-17-30_1' : [0],              # rh_session1\n",
    "    '2019-03-07-16-44-5_2' : [0, 1],         # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [0, 1, 3]         # rh_session3\n",
    "}\n",
    "\n",
    "# key selection can have extra selections of NextPhrase at the end\n",
    "dict_keySelectionOfNextPhrase = {\n",
    "    \"2019-02-11-11-18-30_1\" : [12, 13], # ac_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [12], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [12], # af_session2\n",
    "    \"2019-02-06-16-19-9_2\" : [12], # af_session3\n",
    "    \"2019-02-12-11-07-43_1\" : [12], # af_session4\n",
    "    \"2019-02-27-15-08-32_1\" : [12], # af_session5\n",
    "    \"2019-01-28-14-30-44_1\" : [12], # bh1_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [12], # bh2_session1\n",
    "    \"2019-02-18-14-02-56_2\" : [12], # le_session1\n",
    "    \"2019-02-19-10-03-14_1\" : [12], # le_session2\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [12], # mn_session1\n",
    "    \"2019-02-08-11-12-51_2\" : [12, 13], # mn_session1\n",
    "    \"2019-02-15-11-38-22_1\" : [12, 13], # mn_session2\n",
    "    \"2019-02-15-11-54-25_2\" : [12], # mn_session2\n",
    "    \"2019-01-16-15-18-0_1\" : [12], # no_session1\n",
    "    \"2019-01-28-13-31-51_1\" : [12], # ph_session1\n",
    "    \"2019-01-28-13-49-14_2\" : [12], # ph_session1\n",
    "    \"2019-01-14-15-07-21_1\" : [12], # ys_session1\n",
    "    \"2019-01-17-15-05-1_1\" : [12], # ys_session3\n",
    "    \"2019-01-30-11-22-25_1\" : [12], # ys_session4\n",
    "    \"2019-01-31-13-32-2_2\" : [12], # ys_session5\n",
    "}\n",
    "\n",
    "\n",
    "# key selection when participants skips some sentences\n",
    "dict_keySelectionNotCompleted = {\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : [0, 1, 3, 5, 7], # af_session1 ---- last sentence is not finished\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : [0, 1, 3, 4, 5, 7, 9, 11], # af_session1\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : [0, 1, 3, 5, 7, 9, 11], # af_session2 \n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 6, 7, 9, 11], # af_session2\n",
    "    \"2019-02-08-11-33-53_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # aq_session3\n",
    "    \"2019-02-08-12-11-34_2ndPart_1\" : [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11], # aq_session3\n",
    "    \"2019-01-28-14-30-44_1\" : [0, 1, 3, 5], # bh1_session1\n",
    "    \"2019-01-31-09-22-49_1stPart_2\": [0, 1, 3, 5, 7, 9, 10, 11], # bh1_session4\n",
    "    \"2019-01-31-09-37-5_2ndPart_2\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # bh1_session4\n",
    "    \"2019-02-21-16-09-44_1stPart_1\" : [0, 1, 3, 4, 5, 7, 9, 11], # bh2_session1\n",
    "    \"2019-02-21-16-22-22_2ndPart_1\" : [0, 1, 3, 5, 6, 7, 8, 9, 10, 11], # bh2_session1\n",
    "    \"2019-02-28-17-03-53_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # bh2_session3\n",
    "    \"2019-02-28-17-24-2_2ndPart_2\" : [0, 1, 2, 3, 5], # bh2_session3     ----\n",
    "    \"2019-02-14-13-28-20_1stPart_2\" : [0, 1, 3, 5, 6, 7, 9, 11], # cw_session3\n",
    "    \"2019-02-14-13-57-41_2ndPart_2\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 11], # cw_session3\n",
    "    \"2019-02-21-15-01-4_1stPart_1\" : [0, 1, 2, 3, 5, 7, 9, 11], # le_session3\n",
    "    \"2019-02-21-15-25-56_2ndPart_1\" : [0, 1, 3], # le_session3       ----\n",
    "    \"2019-02-05-14-00-27_1stPart_2\" : [0, 1, 3, 5, 7, 8], # mh_session1\n",
    "    \"2019-02-05-14-10-39_2ndPart_2\" : [0, 1, 2, 3, 4, 5, 7, 8, 9, 11], # mh_session1\n",
    "    \"2019-02-08-10-51-3_1stPart_1\" : [0, 1, 3, 5, 7, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-08-11-05-7_2ndPart_1\" : [0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11], # mn_session1\n",
    "    \"2019-02-19-10-34-7_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 11], # mn_session3\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : [0, 1, 3, 4, 5, 6, 7, 8, 9, 10, 11], # mn_session3\n",
    "    \"2019-01-29-13-25-4_1\" : [0, 1, 3, 5, 7], # ph_session2  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-18-50_1stPart_1\" : [0, 1, 3, 5, 7, 8, 9, 10], # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : [0, 1, 3, 5],  # ys_session3  -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-02-06-11-25-41_1\" : [0, 1, 3, 5, 11], # aq_session1 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2 -- different for reading and writing, this one is for\n",
    "    # writing\n",
    "    '2019-01-30-11-22-25_1' : [0, 1, 3, 5, 7, 9, 11]   # ys_session4 -- sessions where there are less score questions \n",
    "    # and more sentences typed\n",
    "   \n",
    "}\n",
    "\n",
    "# dictionary for phrase removal just like in the dict_phraseStim, but since not all participants require that, some that \n",
    "# do, are added to this new dictionary here\n",
    "dict_keySelection_phraseStim = {\n",
    "    '2019-01-17-15-27-20_1stPart_2' : [4], # Af session2\n",
    "    '2019-01-16-15-18-0_1' : [4],        # no_session1\n",
    "    '2019-02-19-17-10-45_1' : [3],                  # ph_session5\n",
    "    '2019-03-07-16-44-5_2' : [1],        # rh_session1\n",
    "    '2019-03-14-13-56-56_2' : [2],              # rh_session3\n",
    "    '2019-01-14-15-07-21_1' : [4]         # ys_session1\n",
    "}\n",
    "\n",
    "\n",
    "# in the beginning experiments, not everyone started with 800 initial dwell time\n",
    "\n",
    "dict_dwellTimeOrig_not800 = {\n",
    "    \"2019-01-16-15-51-13_2\" : 600, # no_session1\n",
    "    \"2019-01-16-15-18-0_1\" : 600, # no_session1\n",
    "    \"2019-01-16-15-43-8_1\" : 100, # af_session1\n",
    "    \"2019-01-16-16-36-17_1stPart_2\" : 100, # af_session1\n",
    "    \"2019-01-16-17-00-12_2ndPart_2\" : 100, # af_session1\n",
    "    \"2019-01-17-15-03-40_1\" : 100, # af_session2\n",
    "    \"2019-01-17-15-27-20_1stPart_2\" : 0, # af_session2\n",
    "    \"2019-01-17-16-03-27_2ndPart_2\" : 100, # af_session2\n",
    "    \"2019-01-14-15-07-21_1\" : 500, # ys_session1\n",
    "    \"2019-01-14-15-25-55_2\" : 300, # ys_session1\n",
    "    \"2019-01-16-15-18-50_1stpart_1\" : 200, # ys_session2\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 100, # ys_session2\n",
    "    \"2019-01-16-15-59-55_2\" : 100, # ys_session2\n",
    "    \"2019-01-17-15-05-1_1\" : 100, # ys_session3\n",
    "    \"2019-01-17-15-31-12_2\" : 100 # ys_session3\n",
    "}\n",
    "\n",
    "\n",
    "# list of all things that should be present when computing effective time\n",
    "list_keysToBeCounted = ['Comma', 'BackOne', 'BackMany', 'SpaceBar']\n",
    "\n",
    "# some sessions do not have gaze data\n",
    "dict_noGazeData = {\n",
    "    '2019-01-16-17-00-12_2ndPart_2' : 'no gaze data', # af_session2\n",
    "    '2019-01-17-15-31-12_2' : 'no gaze data', #ys_session2\n",
    "    '2019-01-30-11-57-3_2' : 'no gaze data' # ys_session4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keySelection_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 3, 5], # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_WritingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : [0, 1, 2, 5], # ys_session2   \n",
    "}\n",
    "\n",
    "# normally, reading part of trial ends when people look at the keyboardwithphrases. For some trials, this is not done,\n",
    "# as the reading is done, and the trial is accidentally skipped, and written in the next trial. Here, the trial number \n",
    "# given will have the reading time ending as sleep, and not keyboard with phrases. \n",
    "dict_keyboardNotChange_ReadingTrials = {\n",
    "    \"2019-01-16-15-42-51_2ndPart_1\" : 0, # ys_session2 \n",
    "}\n",
    "\n",
    "dict_keySelection_firstSleepNotCounted = {\n",
    "    \"2019-01-28-14-50-41_2\" : (0, 2), # bh1_session1 -- 3rd sleep activation to be counted\n",
    "    \"2019-02-19-10-56-43_2ndPart_1\" : 2  # mn_session3 -- 3rd sleep activation is to be counted\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeDwellOrig = 800\n",
    "TimeFixation = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixUserKeys(UserKeys_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    Column_beforeDecimal = [item[2] for item in UserKeys_Old]\n",
    "    Column_afterDecimal = [item[3] if len(item)>3 else '00' for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_ProgressPercent = [float(Column_beforeDecimal[i]+'.'+ Column_afterDecimal[i]) for i in \n",
    "                                range(0, len(Column_beforeDecimal))]\n",
    "    UserKeys_Times = [item[0] for item in UserKeys_Old]\n",
    "    UserKeys_Keys = [item[1] for item in UserKeys_Old]\n",
    "    \n",
    "    UserKeys_New = [[UserKeys_Times[ind], UserKeys_Keys[ind], UserKeys_ProgressPercent[ind]] for ind in \n",
    "                    range(0, len(UserKeys_ProgressPercent))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    \n",
    "    return UserKeys_New\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixScratchPad(ScratchPad_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    ScratchPad_Times = [item[0] for item in ScratchPad_Old]\n",
    "    \n",
    "    ScratchPad_Phrases = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    ScratchPadInd = -1 \n",
    "    while ScratchPadInd < len(ScratchPad_Old)-1:\n",
    "        ScratchPadInd = ScratchPadInd + 1\n",
    "        commasInPhrase = len(ScratchPad_Old[ScratchPadInd])-2\n",
    "        if commasInPhrase < 1:\n",
    "            #print(ScratchPad_Old[ScratchPadInd][1])\n",
    "            ScratchPad_Phrases.append(ScratchPad_Old[ScratchPadInd][1])\n",
    "            continue\n",
    "        scratchPadPhrase = ScratchPad_Old[ScratchPadInd][1]\n",
    "        for phraseJoinNr in range(1, commasInPhrase+1):\n",
    "            scratchPadPhrase = scratchPadPhrase + ', ' + ScratchPad_Old[ScratchPadInd][1+phraseJoinNr]\n",
    "        \n",
    "        ScratchPad_Phrases.append(scratchPadPhrase)\n",
    "            \n",
    "        \n",
    "    ScratchPad_New = [[ScratchPad_Times[ind], ScratchPad_Phrases[ind]] for ind in \n",
    "                    range(0, len(ScratchPad_Times))]\n",
    "    \n",
    "    #UserKeys_New = np.concatenate((UserKeys_Times, UserKeys_Keys, UserKeys_ProgressPercent), axis = 0)\n",
    "    \n",
    "    #print(ScratchPad_New)\n",
    "    return ScratchPad_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixKeysSelected(KeysSelected_Old):\n",
    "    # Fix the situation where comma has divided decimals into separate columns\n",
    "    \n",
    "    KeysSelected_New = list()\n",
    "    \n",
    "    # loop to combine phrases divided by commas\n",
    "    KeysSelectedInd = -1 \n",
    "    while KeysSelectedInd < len(KeysSelected_Old)-1:\n",
    "        KeysSelectedInd = KeysSelectedInd + 1\n",
    "        \n",
    "        if KeysSelected_Old[KeysSelectedInd][1].count(',') > 0:\n",
    "            \n",
    "            keys_split = KeysSelected_Old[KeysSelectedInd][1].split(\"\\r\\n\")\n",
    "            del keys_split[0]\n",
    "            del keys_split[-1]\n",
    "            \n",
    "            keys_split = [key.split(',') for key in keys_split]\n",
    "            \n",
    "            KeysSelected_New.extend(keys_split)\n",
    "        else:\n",
    "            KeysSelected_New.append(KeysSelected_Old[KeysSelectedInd])\n",
    "        \n",
    "    \n",
    "    return KeysSelected_New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeDwellTime(userKeys, full_path):\n",
    "    # modify userKeys to include a column of time instead of progress pct, which is dependent on the then dwell time\n",
    "    \n",
    "    TimeDwellOrig = 800\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    if session_folder_name in dict_dwellTimeOrig_not800:\n",
    "        TimeDwellOrig = dict_dwellTimeOrig_not800[session_folder_name]\n",
    "    \n",
    "    #print(TimeDwellOrig)\n",
    "    \n",
    "    timeDwell = TimeDwellOrig\n",
    "    nKey = -1\n",
    "    for key in userKeys:\n",
    "        nKey = nKey + 1\n",
    "        #print(key[1])\n",
    "        if key[1] == 'IncreaseDwellTime':\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell + 100\n",
    "        elif key[1] == 'DecreaseDwellTime':\n",
    "            #print(key[2])\n",
    "            if float(key[2]) == 1:\n",
    "                timeDwell = timeDwell - 100\n",
    "        else:\n",
    "            userKeys[nKey].append(str(float(key[2])*timeDwell))\n",
    "    \n",
    "    return userKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stimPhrasesEdit(PhraseLog, full_path):\n",
    "   \n",
    "    # Now extract phrases from the phrase file\n",
    "    phraseStim_Phrases = [item[1] for item in PhraseLog]\n",
    "    \n",
    "    # session name\n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "        \n",
    "    phraseStim_PhrasesReduced, phraseStim_timeReduced = zip(*[(x[0], PhraseLog[phraseStim_Phrases.index(x[0])][0]) for x in groupby(phraseStim_Phrases)])\n",
    "    \n",
    "    PhraseLogReduced = [[phraseStim_timeReduced[i], phraseStim_PhrasesReduced[i]] for i in range(0, len(phraseStim_PhrasesReduced))]\n",
    "    \n",
    "    if PhraseLogReduced[-1][1] == 'THE EXPERIMENT IS NOW DONE':\n",
    "        del PhraseLogReduced[-1]\n",
    "        \n",
    "    if PhraseLogReduced[0][1] == 'phraseText':\n",
    "        del PhraseLogReduced[0]\n",
    "\n",
    "    # Here, we want only the sentences typed\n",
    "    notSentencesToType = list()\n",
    "    for index in range(0,len(PhraseLogReduced)):\n",
    "        sentence = PhraseLogReduced[index][1]\n",
    "        if 'Svar på følgende spørgsmål' in sentence or 'Answer the question:' in sentence or 'What is the complete name of your university?' in sentence or '(give a score between 1 and 7)' in sentence or sentence == '':\n",
    "            notSentencesToType.append(index)\n",
    "         \n",
    "    \n",
    "    for index in sorted(notSentencesToType, reverse=True):\n",
    "        del PhraseLogReduced[index]\n",
    "    \n",
    "    if session_folder_name in dict_phraseStim:\n",
    "        #print('session in stim phrases found')\n",
    "        index_to_be_removed = dict_phraseStim[session_folder_name]\n",
    "    else:\n",
    "        index_to_be_removed = []\n",
    "        \n",
    "    #print(index_to_be_removed)\n",
    "    \n",
    "    if index_to_be_removed:\n",
    "        for index in sorted(index_to_be_removed, reverse=True):\n",
    "            del PhraseLogReduced[index]\n",
    "        \n",
    "    return PhraseLogReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will return the datetime in items which is the closest to the date pivot\n",
    "def nearestTimePoint(dates, date):\n",
    "    \n",
    "    for d in dates:\n",
    "        if d < date:\n",
    "            nearestTP = d\n",
    "        else:\n",
    "            continue\n",
    "    try: \n",
    "        nearestTP\n",
    "        nearestTPind = dates.index(nearestTP)\n",
    "    except:\n",
    "        nearestTP = 0\n",
    "        nearestTPind = -1\n",
    "        \n",
    "    return nearestTP, nearestTPind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert list of date and time into datetime format list\n",
    "def timeConversion(timeStrList):\n",
    "    timeList = list()\n",
    "    for time in timeStrList:\n",
    "        time1, t1, t2 = time.partition('+')\n",
    "        timeList.append(datetime.datetime.strptime(re.sub('[:.T]','-',time1[:-1]), \"%Y-%m-%d-%H-%M-%S-%f\"))\n",
    "    return timeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAndRemoveTrials(session_name, dictionary_saved, trials, replacingList):\n",
    "    # function to check the session_name in the dictionary_saved and remove those trials from the dictionary_trial\n",
    "    \n",
    "    if session_name in dictionary_saved:\n",
    "        index_list = dictionary_saved[session_name]\n",
    "    else:\n",
    "        index_list = replacingList\n",
    "    \n",
    "    \n",
    "    if index_list:\n",
    "        if type(trials) == list:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials[index]\n",
    "                \n",
    "        else:\n",
    "            for index in sorted(index_list, reverse=True):\n",
    "                del trials['start'][index]\n",
    "                del trials['end'][index]\n",
    "        \n",
    "    return trials    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scratchPadPhraseEdit(phraseUser, subjName, full_path, picture):\n",
    "    phraseUserEnd = list()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for row_ind in range(0, len(phraseUser)):\n",
    "        if row_ind!= 0 and phraseUser[row_ind][1] == '':\n",
    "            if phraseUser[row_ind-1][1] != 'scratchPadText':\n",
    "                if len(phraseUser[row_ind-1][1])>2: # this also removes any answers on the difficulty of the sentence\n",
    "                    phraseUserEnd.append(phraseUser[row_ind-1])\n",
    "                    #print(phraseUser[row_ind-1])\n",
    "                    \n",
    "    #print(' (1) phrases reduced to :       ', phraseUserEnd)\n",
    "    # remove first two trials of baseline question and text composition\n",
    "    if 'not_described' in picture or '2019-01-14-14-58-30' in full_path or '2019-02-06-12-37-45_2' in full_path or '2019-02-18-10-28-35_2' in full_path: \n",
    "        # yss has not copied one of the sentences, aq_session1_2 has not written the baseline question, ls2_session4_2 did not describe the picture\n",
    "        del phraseUserEnd[0]\n",
    "    else:\n",
    "        del phraseUserEnd[0:2]\n",
    "      \n",
    "    session_folder_name = full_path.split('\\\\')[-1]\n",
    "    \n",
    "    if session_folder_name in dict_phraseUser:\n",
    "        index_to_be_removed = dict_phraseUser[session_folder_name]\n",
    "        #print('session in user phrases found')\n",
    "    else:\n",
    "        index_to_be_removed = []\n",
    "        \n",
    "    \n",
    "    if index_to_be_removed:\n",
    "        for index in sorted(index_to_be_removed, reverse=True):\n",
    "            del phraseUserEnd[index]\n",
    "        \n",
    "    \n",
    "    #print(' (2) phrases reduced to:       ', phraseUserEnd)\n",
    "    \n",
    "    # remove the initial rating of complexity, if they have written it:\n",
    "    for index in range(0,len(phraseUserEnd)):\n",
    "        sentence = phraseUserEnd[index][1]\n",
    "        if sentence[0].isdigit():\n",
    "            \n",
    "            # if there is also a space after the digit:\n",
    "            \n",
    "            if sentence[1] == ' ':\n",
    "                phraseUserEnd[index][1] = phraseUserEnd[index][1][2:]\n",
    "            else:\n",
    "                phraseUserEnd[index][1] = phraseUserEnd[index][1][1:]\n",
    "            \n",
    "        elif sentence[1].isdigit():\n",
    "            if sentence[2] == ' ':\n",
    "                phraseUserEnd[index][1] = phraseUserEnd[index][1][3:]\n",
    "            else:\n",
    "                phraseUserEnd[index][1] = phraseUserEnd[index][1][2:]\n",
    "    \n",
    "    #print(' (3) phrases reduced to:       ', phraseUserEnd)\n",
    "    \n",
    "    return phraseUserEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataForEveryTrial:\n",
    "    subjectID = ''\n",
    "    blockNumber = ''\n",
    "    sessionNumber = ''\n",
    "    variable = ''\n",
    "    dataForTrial = ''\n",
    "    resultPathName = ''\n",
    "   \n",
    "    \n",
    "    def printInfo(self):\n",
    "        dataFrame = pd.DataFrame(list(zip([self.subjectID]*len(self.dataForTrial), [self.blockNumber]*len(self.dataForTrial), [self.sessionNumber]*len(self.dataForTrial), range(0,len(self.dataForTrial)+1), self.dataForTrial)), columns=['subjectID', 'block', 'session', 'trial', self.variable])\n",
    "        \n",
    "        return dataFrame\n",
    "    \n",
    "    def AddToFile(self):\n",
    "        \n",
    "        dataFrame = pd.DataFrame(list(zip([self.subjectID]*len(self.dataForTrial), [self.blockNumber]*len(self.dataForTrial), [self.sessionNumber]*len(self.dataForTrial), range(0,len(self.dataForTrial)+1), self.dataForTrial)), columns=['subjectID', 'block', 'session', 'trial', self.variable])\n",
    "        book = load_workbook(self.resultPathName)\n",
    "        writer = pd.ExcelWriter(self.resultPathName, engine='openpyxl')\n",
    "        writer.book = book\n",
    "        writer.sheets = dict((ws.title, ws) for ws in book.worksheets)\n",
    "        startrow = writer.sheets['Sheet1'].max_row\n",
    "        dataFrame.to_excel(writer, startrow = startrow, index = False, header = False)\n",
    "        \n",
    "        writer.save()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find minimum cost and the operations that give rise to it\n",
    "def minValnInd(costOptions, flagSame):\n",
    "    operator = list()\n",
    "    unique_entries = set(costOptions)\n",
    "    valInd = { value : [ i for i, v in enumerate(costOptions) if v == value ] for value in unique_entries }\n",
    "    keyVal = list(valInd.keys())\n",
    "    min_value = min(keyVal)\n",
    "    \n",
    "    if 0 in valInd[min_value]:\n",
    "        operator.append('D')\n",
    "    if 1 in valInd[min_value]:\n",
    "        operator.append('I')\n",
    "    if 2 in valInd[min_value]:\n",
    "        if flagSame == 0:\n",
    "            operator.append('S')\n",
    "        else:\n",
    "            operator.append('N')   \n",
    "    flagSame = None    \n",
    "    return min_value, ''.join(operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the MSD, with cost of 2 for substitution and 1 for insertion and deletion\n",
    "costSub = 1\n",
    "costIns = 1\n",
    "costDel = 1\n",
    "\n",
    "def levenshteinDist(phraseIn, phraseOut):\n",
    "    \n",
    "    lenStim = len(phraseIn)\n",
    "    lenUser = len(phraseOut)\n",
    "    costMatrix = np.zeros((lenStim+1, lenUser+1), dtype=int)\n",
    "    MSDoperation = np.empty([lenStim+1, lenUser+1], dtype=\"U4\")\n",
    "    costMatrix[0,0:] = range(0, lenUser+1)\n",
    "    costMatrix[0:,0] = range(0, lenStim+1)\n",
    "    MSDoperation[0,0:] = 'I'\n",
    "    MSDoperation[0:,0] = 'D'\n",
    "    \n",
    "    for i in range(1, len(phraseIn)+1):\n",
    "        iP = i - 1\n",
    "        for j in range(1, len(phraseOut)+1):\n",
    "            jP = j - 1\n",
    "            if phraseIn[iP].lower() == phraseOut[jP].lower():\n",
    "                # Define the possible cost array\n",
    "                costOptionArray = [costMatrix[i,j-1]+costDel, costMatrix[i-1,j]+costIns, costMatrix[i-1,j-1]] \n",
    "                flagSame = 1\n",
    "            else:\n",
    "                costOptionArray = [costMatrix[i,j-1]+costDel, costMatrix[i-1,j]+costIns, costMatrix[i-1,j-1]+costSub]\n",
    "                flagSame = 0\n",
    "            costMatrix[i,j], MSDoperation[i][j] = minValnInd(costOptionArray, flagSame)\n",
    "    #print(costMatrix)\n",
    "    return costMatrix[-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeErrorRate(PhraseInList, PhraseOutList, Language):\n",
    "    \n",
    "    # levenshtein distance at word level\n",
    "    dist_word = list()\n",
    "    \n",
    "    for ind, phraseIn_wTime in enumerate(PhraseInList):\n",
    "        phraseIn = phraseIn_wTime[1]\n",
    "        phraseOut = PhraseOutList[ind][1]\n",
    "        \n",
    "        # initialize the stemmer\n",
    "        if Language == 'Danish':\n",
    "            words_stemmer = snowball.DanishStemmer()\n",
    "        else:\n",
    "            words_stemmer = snowball.EnglishStemmer()\n",
    "            \n",
    "            \n",
    "        roots_phraseIn = list()\n",
    "        roots_phraseOut = list()\n",
    "                    \n",
    "        for word in phraseIn.split():\n",
    "            # find roots of words in stimulation phrase - \n",
    "            roots_phraseIn.append(words_stemmer.stem(word))\n",
    "\n",
    "        for word in phraseOut.split():\n",
    "            roots_phraseOut.append(words_stemmer.stem(word))\n",
    "                        \n",
    "        #print(roots_phraseStim)\n",
    "        #print('and')\n",
    "        #print(roots_phraseUser)\n",
    "                    \n",
    "        #print(distance.levenshtein(roots_phraseStim, roots_phraseUser))\n",
    "        dist_word.append(distance.levenshtein(roots_phraseIn, roots_phraseOut)/max(len(roots_phraseIn), \\\n",
    "                                                                                              len(roots_phraseOut)))\n",
    "\n",
    "    # levenshtein distance at character level\n",
    "    dist_char = list()\n",
    "    \n",
    "    for n in range(0,len(PhraseInList)):\n",
    "        dist_char.append((levenshteinDist(PhraseInList[n][1], PhraseOutList[n][1]))/max(len(PhraseInList[n][1]), \\\n",
    "                                                                                        len(PhraseOutList[n][1])))\n",
    "        \n",
    "    ErrorRate_trial = list(0.5*np.array(dist_char) + 0.5*np.array(dist_word))\n",
    "    \n",
    "    \n",
    "    return ErrorRate_trial   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject path E:\\Data\\Data\\ls2\\4\\2019-02-18-10-09-22_1\n",
      "subject and session name:  ls2__4__2019-02-18-10-09-22_1\n",
      "  subjectID block session  trial  ErrorRate\n",
      "0       ls2     4       1      0   0.000000\n",
      "1       ls2     4       1      1   0.000000\n",
      "2       ls2     4       1      2   0.000000\n",
      "3       ls2     4       1      3   0.000000\n",
      "4       ls2     4       1      4   0.145455\n",
      "subject path E:\\Data\\Data\\ls2\\4\\2019-02-18-10-28-35_2\n",
      "subject and session name:  ls2__4__2019-02-18-10-28-35_2\n",
      "  subjectID block session  trial  ErrorRate\n",
      "0       ls2     4       2      0   0.068765\n",
      "1       ls2     4       2      1   0.000000\n",
      "2       ls2     4       2      2   0.000000\n",
      "3       ls2     4       2      3   0.064777\n",
      "4       ls2     4       2      4   0.286718\n"
     ]
    }
   ],
   "source": [
    "metricComputed = 'ErrorRate'\n",
    "\n",
    "dataFolderName = r'E:\\Data\\Data' # accessing external hard disk with the data\n",
    "a = re.compile('(?<=Data\\\\\\\\Data\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "subjectName_listElement = 3\n",
    "\n",
    "#dataFolderName = r'C:\\DTU\\Data\\201901_JanuaryExpt' # accessing data saved in the computer\n",
    "#a = re.compile('(?<=Data\\\\\\\\201901_JanuaryExpt\\\\\\\\)(.*)(?=\\\\\\\\[1-9])')\n",
    "#subjectName_listElement = 4\n",
    "\n",
    "resultFileName = r'C:\\DTU\\Data\\201901_JanuaryExpt\\DataExtracted\\IndividualTrials\\Performance\\Subject_Block_Session_Trial_' + metricComputed +  '.xlsx'\n",
    "\n",
    "\n",
    "for root, dirs, subfolder in os.walk(dataFolderName):\n",
    "    \n",
    "    technique = 'dwell_time'\n",
    "    \n",
    "    if not dirs:\n",
    "        \n",
    "        #if 'notCompleted' in root or 'notInclude' in root: # Some subjects do not have gaze log and have been marked as \n",
    "        \n",
    "        if 'noData' in root or 'Trial' in root or 'trial' in root or 'Nothing' in root: # Some subjects do not have gaze log and have been marked as \n",
    "            #notInclude\n",
    "            continue\n",
    "        if 'Jonas' in root or 'Praktikant' in root or 'Villads' in root:\n",
    "            continue\n",
    "            \n",
    "        if 'ls2\\\\4\\\\' not in root:\n",
    "            continue\n",
    "        if 'Picture' in root:\n",
    "            continue\n",
    "        #if '_MS' not in root:\n",
    "        #    continue\n",
    "        \n",
    "        \n",
    "            \n",
    "        keysSelected = None\n",
    "        userKeys = None\n",
    "        phraseLog = None\n",
    "        \n",
    "        for file in subfolder:\n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'ScratchPadLog*'):\n",
    "                try:\n",
    "                    fScratchPad = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerScratchPad = csv.reader(fScratchPad, quotechar=None)\n",
    "                    scratchPad = list(readerScratchPad)\n",
    "                except:\n",
    "                    if fScratchPad is not None:\n",
    "                        fScratchPad.close()\n",
    "                    else:\n",
    "                        print('error in opening the scratchpad log file')\n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'KeySelection*'):\n",
    "                try:\n",
    "                    \n",
    "                    fKeysSelected = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerKeysSelected = csv.reader(fKeysSelected)\n",
    "                    keysSelected = list(readerKeysSelected)\n",
    "                    \n",
    "                    keysSelected.remove(keysSelected[0])\n",
    "                except:\n",
    "                    if fKeysSelected is not None:\n",
    "                        fKeysSelected.close()\n",
    "                    else:\n",
    "                        print('error in opening the KeySelection log file')\n",
    "                        \n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'user*'):\n",
    "                try:\n",
    "                    fUserKey = open(root + '\\\\' + file, encoding='utf-8',  newline='')\n",
    "                    readerUserKey = csv.reader(fUserKey, quotechar=None)\n",
    "                    userKeys = list(readerUserKey)\n",
    "                    userKeys.remove(userKeys[0])\n",
    "                except:\n",
    "                    if fUserKey is not None:\n",
    "                        fUserKey.close()\n",
    "                    else:\n",
    "                        print('error in opening the user key log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'phrase*'):\n",
    "                try:\n",
    "                    fPhraseLog = open(root + '\\\\' + file, encoding='utf-8')\n",
    "                    readerPhraseLog = csv.reader(fPhraseLog, quotechar=None)\n",
    "                    phraseLog = list(readerPhraseLog)\n",
    "                    \n",
    "                except:\n",
    "                    if fPhraseLog is not None:\n",
    "                        fPhraseLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the phrase log file')\n",
    "                        \n",
    "            if fnmatch.fnmatch(file, 'multiKey*'):\n",
    "                technique = 'multiKey_selection'\n",
    "            \n",
    "            if fnmatch.fnmatch(file, 'tobiiGazeLog*'):\n",
    "                try:\n",
    "                    fGazeLog = open(root + '\\\\' + file, encoding='utf-8', newline='')\n",
    "                    readerGazeLog = csv.reader(fGazeLog, quotechar=None)\n",
    "                    gazeLog = list(readerGazeLog)\n",
    "                    gazeLog.remove(gazeLog[0])\n",
    "                    gazeLog.remove(gazeLog[-1])\n",
    "                    \n",
    "                except:\n",
    "                    if fGazeLog is not None:\n",
    "                        fGazeLog.close()\n",
    "                    else:\n",
    "                        print('error in opening the scratchpad log file')\n",
    "            \n",
    "                    \n",
    "                     \n",
    "        if keysSelected is None or userKeys is None or phraseLog is None or gazeLog is None or scratchPad is None:\n",
    "            continue\n",
    "        else:\n",
    "                \n",
    "            print('subject path', root)\n",
    "            subjAndSessionName = '__'.join(root.split('\\\\')[subjectName_listElement:])\n",
    "            subjName = subjAndSessionName.split('__')[0]\n",
    "            print('subject and session name: ', subjAndSessionName)\n",
    "            sessionFolderName = root.split('\\\\')[-1]\n",
    "            \n",
    "            # fix scratchpad due to comma related file changes\n",
    "            scratchPad_new = FixScratchPad(scratchPad)\n",
    "            \n",
    "            # fix phraselog due to comma related file changes\n",
    "            phraseLog_new = FixScratchPad(phraseLog)\n",
    "            \n",
    "            # find the language of the data:\n",
    "            if 'Hvad er det fulde navn på dit universitet?' in phraseLog_new[1][1]: \n",
    "                language = 'Danish'\n",
    "            else:\n",
    "                language = 'English'            \n",
    "            \n",
    "            phraseStim_reduced = stimPhrasesEdit(phraseLog_new, root)\n",
    "            \n",
    "            # if it is the 2nd part of the session, picture is not described:\n",
    "            if '2ndPart' in root:\n",
    "                picture = 'not_described'\n",
    "            else:\n",
    "                picture = 'described'\n",
    "            \n",
    "            phraseUserEnd_reduced = scratchPadPhraseEdit(scratchPad_new, subjName, root, picture)\n",
    "            \n",
    "            errorRate = computeErrorRate(phraseStim_reduced, phraseUserEnd_reduced, language)\n",
    "            \n",
    "            \n",
    "            if '1stPart' in root:\n",
    "                errorRate1 = errorRate\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            \n",
    "            if '2ndPart' in root:\n",
    "                errorRate2 = errorRate\n",
    "                errorRate = errorRate1 + errorRate2\n",
    "                \n",
    "                \n",
    "                errorRate1 = list()\n",
    "                \n",
    "                \n",
    "            dataToSave = DataForEveryTrial()\n",
    "            dataToSave.subjectID = subjAndSessionName.split('__')[0]\n",
    "            dataToSave.blockNumber = subjAndSessionName.split('__')[1]\n",
    "            dataToSave.sessionNumber = subjAndSessionName[-1]\n",
    "            dataToSave.variable = metricComputed\n",
    "            dataToSave.dataForTrial = errorRate\n",
    "            dataToSave.resultPathName = resultFileName\n",
    "            \n",
    "            print(dataToSave.printInfo())\n",
    "            #dataToSave.AddToFile()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
