{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as lng\n",
    "import matplotlib\n",
    "from sklearn.cluster import KMeans\n",
    "import operator\n",
    "%matplotlib qt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import mord\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "from sklearn.tree import export_graphviz\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pydot\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(X):\n",
    "    \"\"\" Center the columns (variables) of a data matrix to zero mean.\n",
    "        \n",
    "        X, MU = center(X) centers the observations of a data matrix such that each variable\n",
    "        (column) has zero mean and also returns a vector MU of mean values for each variable.\n",
    "     \"\"\" \n",
    "    n = X.shape[0]\n",
    "    mu = np.mean(X,0)\n",
    "    #X = X - np.ones((n,1)) * mu\n",
    "    X = X - mu\n",
    "    \n",
    "    return X, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalize the columns (variables) of a data matrix to unit Euclidean length.\n",
    "    X, MU, D = normalize(X)\n",
    "    i) centers and scales the observations of a data matrix such\n",
    "    that each variable (column) has unit Euclidean length. For a normalized matrix X,\n",
    "    X'*X is equivalent to the correlation matrix of X.\n",
    "    ii) returns a vector MU of mean values for each variable.\n",
    "    iii) returns a vector D containing the Euclidean lengths for each original variable.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    n = np.size(X, 0)\n",
    "    X, mu = center(X)\n",
    "    d = np.linalg.norm(X, ord = 2, axis = 0)\n",
    "    d[np.where(d==0)] = 1\n",
    "    X = np.divide(X, np.ones((n,1)) * d)\n",
    "    return X, mu, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_excel(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Data\\Subject_Block_Session_Trial.xlsx')\n",
    "\n",
    "data.dropna(subset=['ScoreDifficulty'], inplace=True, axis=0) # drop only nan in ScoreDifficulty\n",
    "Y1_wNanInData = np.array(data.ScoreDifficulty.tolist())\n",
    "\n",
    "\n",
    "data_woNa = data.dropna(inplace=False, axis=0) # drop nan values in all data\n",
    "Y1 = data_woNa.ScoreDifficulty.tolist() # Y corresponding to data\n",
    "\n",
    "#Y = [int(item) for item in Y]\n",
    "\n",
    "#X = data.drop(['ScoreDifficulty'], axis = 1)\n",
    "\n",
    "# drop some variables that are higher correlated to some other variables\n",
    "data_new = data_woNa.drop(['PupilAbsolute_Median', 'BlinkCount', 'PupilRelative_Median', 'Subject', 'EffectiveTime'], axis=1)\n",
    "\n",
    "\n",
    "# convert some data to categorical first\n",
    "cols_categorical = ['SubjectID', 'BlockID', 'TypingTechniqueID', 'Session', 'Trial', 'ScoreDifficulty']\n",
    "for cols in cols_categorical:\n",
    "    data_new[cols] = data_new[cols].astype('category')\n",
    "\n",
    "X = data_new.drop(['ScoreDifficulty', 'SubjectID', 'Block', 'BlockID', 'TypingTechnique', 'TypingTechniqueID', 'Session', \\\n",
    "                   'SessionDifficulty', 'Trial', 'BlinkDurationTotal'], axis = 1)\n",
    "#X.drop(['ScoreDifficulty'], axis=1, inplace=True)\n",
    "\n",
    "X_norm, mu, d = normalize(X)\n",
    "\n",
    "X_exptDesign = data_new.drop(['Block', 'TypingTechnique', 'SessionDifficulty', 'TypingTechniqueID',\n",
    "       'TotalTime', 'ScoreDifficulty', 'Session', 'Trial',\n",
    "       'TypingSpeed', 'ErrorRate', 'PupilAbsolute_Mean',\n",
    "       #'PupilAbsolute_Median',\n",
    "       'PupilDifference_StartingEnding',\n",
    "       'PupilRelative_Mean', #'PupilRelative_Median',\n",
    "       'PupilRelative_Start', 'LHIPA', #'BlinkCount',\n",
    "       'BlinkFrequency', 'BlinkDurationTotal', 'BlinkDurationAverage'], axis=1)\n",
    "\n",
    "X_norm_all = pd.concat([X_norm, X_exptDesign], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X with missing data\n",
    "data_wNa = data.drop(['PupilAbsolute_Median', 'BlinkCount', 'PupilRelative_Median', 'Subject', 'EffectiveTime'], inplace=False, axis=1)\n",
    "\n",
    "# convert some data to categorical first\n",
    "cols_categorical = ['SubjectID', 'BlockID', 'TypingTechniqueID', 'Session', 'Trial', 'ScoreDifficulty']\n",
    "for cols in cols_categorical:\n",
    "    data_wNa[cols] = data_wNa[cols].astype('category')\n",
    "\n",
    "X_wNa = data_wNa.drop(['ScoreDifficulty', 'SubjectID', 'Block', 'BlockID', 'TypingTechnique', 'TypingTechniqueID', 'Session', \\\n",
    "                   'SessionDifficulty', 'Trial', 'BlinkDurationTotal'], axis = 1)\n",
    "#X.drop(['ScoreDifficultyNew'], axis=1, inplace=True)\n",
    "\n",
    "X_wNa_norm, mu, d = normalize(X_wNa)\n",
    "\n",
    "X_wNa_exptDesign = data_new.drop(['Block', 'TypingTechnique', 'SessionDifficulty',\n",
    "       'TotalTime', 'ScoreDifficulty',\n",
    "       'TypingSpeed', 'ErrorRate', 'PupilAbsolute_Mean',\n",
    "       #'PupilAbsolute_Median',\n",
    "       'PupilDifference_StartingEnding',\n",
    "       'PupilRelative_Mean', #'PupilRelative_Median',\n",
    "       'PupilRelative_Start', 'LHIPA', #'BlinkCount',\n",
    "       'BlinkFrequency', 'BlinkDurationTotal', 'BlinkDurationAverage'], axis=1)\n",
    "\n",
    "\n",
    "X_wNa_norm_all = pd.concat([X_wNa_norm, X_wNa_exptDesign], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((843, 10), (857, 10))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_wNa_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y1)\n",
    "le.classes_\n",
    "Y = le.transform(Y1)\n",
    "\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(Y1_wNanInData)\n",
    "le.classes_\n",
    "Y_wNanInData = le.transform(Y1_wNanInData)\n",
    "\n",
    "X_norm_wY = copy.deepcopy(X_norm)\n",
    "X_norm_wY['y'] = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = np.shape(X_norm)\n",
    "u, d, v = np.linalg.svd(X_norm)\n",
    "\n",
    "# Plot the two components and see if the perceived difficulty levels are differentiated\n",
    "L = v # the loading\n",
    "k = len(d)\n",
    "d = d[0:k]\n",
    "u = u[:, :k]\n",
    "v = v[:k, :] # Matrix that is returned from svd is ordered differently, therefor different slicing\n",
    "\n",
    "S = np.matmul(u, np.diag(d)) # The scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 105)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "d_varianceExplained = list()\n",
    "# Plot the variance explained by n components:\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(1, len(d)+1), (d/sum(d))*100)\n",
    "\n",
    "for i in range(1, len(d)+1):\n",
    "    #print(len(d[0:i]), (sum(d[0:i])/sum(d))*100)\n",
    "    d_varianceExplained.append((sum(d[0:i])/sum(d))*100)\n",
    "\n",
    "plt.grid(b=True)\n",
    "#d_varianceExplained = (d[0]/sum(d))*100\n",
    "plt.plot(range(1, len(d)+1), d_varianceExplained)\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('Percentage variance explained')\n",
    "plt.xticks(range(1,len(d)+1))\n",
    "plt.yticks(range(10,110,10))\n",
    "plt.ylim([0,105])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.281212432328005,\n",
       " 28.027008719145403,\n",
       " 39.41654406692184,\n",
       " 49.9243926225861,\n",
       " 59.96524183256816,\n",
       " 68.98713052453857,\n",
       " 77.08534938637862,\n",
       " 85.06470620097203,\n",
       " 92.99209085591073,\n",
       " 100.0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_varianceExplained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for perceived score of 1, plot pc2 vs pc1:\n",
    "#colorList = ['b', 'r', 'y', 'c', 'm', 'pink', 'g']\n",
    "\n",
    "indices = Y==0\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(S[indices, 0], S[indices, 1], 'ob', label=0)\n",
    "\n",
    "indices = Y==6\n",
    "plt.plot(S[indices, 0], S[indices, 1], 'or', label=6)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "#for i in range(1,8):\n",
    "#    indices = Y==i\n",
    "#    plt.plot(S[indices, 1], S[indices,0], 'o', markerfacecolor=colorList[i-1], markeredgecolor=colorList[i-1], label=i)\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\PCA\\PC1_PC2', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Scree plot')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scree plot\n",
    "sigma2 = d ** 2 / sum(d**2)\n",
    "plt.figure()\n",
    "plt.plot(range(1,len(d)+1), sigma2, '-ob')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.title('Scree plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Var1 : TotalTime\n",
      "Var2 : TypingSpeed\n",
      "Var3 : ErrorRate\n",
      "Var4 : PupilAbsolute_Mean\n",
      "Var5 : PupilDifference_StartingEnding\n",
      "Var6 : PupilRelative_Mean\n",
      "Var7 : PupilRelative_Start\n",
      "Var8 : LHIPA\n",
      "Var9 : BlinkFrequency\n",
      "Var10 : BlinkDurationAverage\n"
     ]
    }
   ],
   "source": [
    "# Biplot and factor loadings\n",
    "xs = S[:,0]\n",
    "ys = S[:,1]\n",
    "coeff = np.transpose(v[0:2,:])\n",
    "n = coeff.shape[0]\n",
    "\n",
    "scalex = 1.0/(xs.max() - xs.min())\n",
    "scaley = 1.0/(ys.max() - ys.min())\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.scatter(xs * scalex,ys * scaley, c = Y, alpha=0.5)\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    label_ = 'Var'+str(i+1)+' : '+X_norm.columns[i]\n",
    "    print(label_)\n",
    "    plt.arrow(0, 0, coeff[i,0], coeff[i,1], color = 'r', alpha = 1)\n",
    "    plt.text(coeff[i,0]*1.3, coeff[i,1]*1.5, \"Var\"+str(i+1), color = 'black', ha = 'center', va = 'center')\n",
    "    \n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.xlim(-0.75,0.75)\n",
    "plt.ylim(-0.75,0.75)\n",
    "\n",
    "plt.grid()\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\PCA\\loadings', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3187771 , -0.24961413],\n",
       "       [-0.30051223,  0.32577782],\n",
       "       [ 0.1881157 , -0.25081813],\n",
       "       [-0.3730992 , -0.41700562],\n",
       "       [-0.38317493, -0.36270558],\n",
       "       [-0.25415364, -0.02552506],\n",
       "       [-0.37594508, -0.38693076],\n",
       "       [-0.3049354 ,  0.3794124 ],\n",
       "       [ 0.32307256, -0.4025413 ],\n",
       "       [ 0.28873821,  0.0827202 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the new data from PCA components:\n",
    "X_PCA = pd.DataFrame.from_records(S, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9',\\\n",
    "                                              'PC10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics to compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "maeTrainMean, maeTrainStd, maeTestMean, maeTestStd = dict(), dict(), dict(), dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLS\n",
    "using statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.7711140577553747 with std:  0.00952190767566444\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.8701343894540405 with std:  0.09229090188613936\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_ols_complData, maeTest_ols_complData = list(), list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_norm_all.iloc[train_index], X_norm_all.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    model = smf.ols('y_train ~ TotalTime + TypingSpeed + ErrorRate + PupilAbsolute_Mean + PupilDifference_StartingEnding\\\n",
    "    + PupilRelative_Mean + PupilRelative_Start + LHIPA + BlinkFrequency + BlinkDurationAverage +\\\n",
    "    SubjectID + BlockID', data=X_train)\n",
    "    model = model.fit(cov_type='HC3')\n",
    "    #print(model.summary())\n",
    "\n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_predTrain = model.predict(X_train)\n",
    "    maeTrain_ols_complData.append(metrics.mean_absolute_error(y_predTrain, y_train))\n",
    "    \n",
    "    X_test = sm.add_constant(X_test)\n",
    "    y_predTest = model.predict(X_test)\n",
    "    maeTest_ols_complData.append(metrics.mean_absolute_error(y_predTest, y_test))    \n",
    "    \n",
    "    \n",
    "maeTrainMean['ols_complData'] = np.mean(maeTrain_ols_complData)\n",
    "maeTrainStd['ols_complData']= np.std(maeTrain_ols_complData)\n",
    "\n",
    "maeTestMean['ols_complData'] = np.mean(maeTest_ols_complData)\n",
    "maeTestStd['ols_complData']= np.std(maeTest_ols_complData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_ols_complData), 'with std: ', np.std(maeTrain_ols_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_ols_complData), 'with std: ', np.std(maeTest_ols_complData))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.699\n",
      "Model:                            OLS   Adj. R-squared:                  0.690\n",
      "Method:                 Least Squares   F-statistic:                     79.95\n",
      "Date:                Mon, 30 Sep 2019   Prob (F-statistic):          5.65e-200\n",
      "Time:                        16:49:16   Log-Likelihood:                -1175.1\n",
      "No. Observations:                 843   AIC:                             2402.\n",
      "Df Residuals:                     817   BIC:                             2525.\n",
      "Df Model:                          25                                         \n",
      "Covariance Type:                  HC3                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Intercept             3.6265      0.156     23.279      0.000       3.321       3.932\n",
      "SubjectID[T.2.0]      0.4158      0.223      1.862      0.063      -0.022       0.853\n",
      "SubjectID[T.3.0]     -1.6130      0.219     -7.372      0.000      -2.042      -1.184\n",
      "SubjectID[T.4.0]      0.1300      0.223      0.583      0.560      -0.307       0.567\n",
      "SubjectID[T.5.0]     -1.4253      0.215     -6.622      0.000      -1.847      -1.003\n",
      "SubjectID[T.6.0]     -1.2397      0.194     -6.403      0.000      -1.619      -0.860\n",
      "SubjectID[T.7.0]     -1.0561      0.180     -5.864      0.000      -1.409      -0.703\n",
      "SubjectID[T.8.0]     -0.2928      0.194     -1.506      0.132      -0.674       0.088\n",
      "SubjectID[T.9.0]     -1.3407      0.255     -5.249      0.000      -1.841      -0.840\n",
      "SubjectID[T.10.0]    -0.7744      0.216     -3.580      0.000      -1.198      -0.350\n",
      "SubjectID[T.11.0]     0.2442      0.214      1.141      0.254      -0.175       0.664\n",
      "SubjectID[T.12.0]     0.7883      0.239      3.304      0.001       0.321       1.256\n",
      "SubjectID[T.13.0]    -0.8126      0.219     -3.712      0.000      -1.242      -0.384\n",
      "SubjectID[T.14.0]     0.7153      0.210      3.404      0.001       0.303       1.127\n",
      "SubjectID[T.15.0]    -0.3318      0.208     -1.597      0.110      -0.739       0.075\n",
      "SubjectID[T.16.0]    -0.4537      0.262     -1.734      0.083      -0.967       0.059\n",
      "SubjectID[T.17.0]     0.2342      0.248      0.946      0.344      -0.251       0.719\n",
      "SubjectID[T.18.0]    -0.9510      0.223     -4.268      0.000      -1.388      -0.514\n",
      "BlockID[T.2.0]        0.0036      0.119      0.031      0.976      -0.230       0.237\n",
      "BlockID[T.3.0]       -0.4343      0.119     -3.642      0.000      -0.668      -0.201\n",
      "BlockID[T.4.0]       -0.2535      0.115     -2.212      0.027      -0.478      -0.029\n",
      "BlockID[T.5.0]       -0.3479      0.119     -2.915      0.004      -0.582      -0.114\n",
      "TotalTime            24.6902      1.299     19.009      0.000      22.144      27.236\n",
      "TypingSpeed          -8.3490      1.532     -5.451      0.000     -11.351      -5.347\n",
      "ErrorRate            18.5181      1.550     11.944      0.000      15.479      21.557\n",
      "BlinkFrequency        6.0131      1.679      3.582      0.000       2.723       9.303\n",
      "==============================================================================\n",
      "Omnibus:                        0.205   Durbin-Watson:                   1.527\n",
      "Prob(Omnibus):                  0.903   Jarque-Bera (JB):                0.200\n",
      "Skew:                           0.037   Prob(JB):                        0.905\n",
      "Kurtosis:                       2.990   Cond. No.                         55.4\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC3)\n"
     ]
    }
   ],
   "source": [
    "model = smf.ols('Y ~ TotalTime + TypingSpeed + ErrorRate + BlinkFrequency + SubjectID + BlockID', data=X_norm_all)\n",
    "model = model.fit(cov_type='HC3')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.7740658096716413 with std:  0.008633504130295398\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.8674425523661522 with std:  0.08938294390358462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# cross-validation for Linear regression on pDiff using leave-one-subject-out using OLS\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_ols_redData, maeTest_ols_redData = list(), list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train = X_norm_all.iloc[train_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    X_test = X_norm_all[['TotalTime', 'TypingSpeed', 'ErrorRate', 'BlinkFrequency', 'SubjectID', \\\n",
    "                         'BlockID']].iloc[test_index]\n",
    "    \n",
    "    model = smf.ols('y_train ~ TotalTime + TypingSpeed + ErrorRate + BlinkFrequency + SubjectID + BlockID', data=X_train)\n",
    "    model = model.fit(cov_type='HC3')\n",
    "    #print(model.summary())\n",
    "    \n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_predTrain = model.predict(X_train)\n",
    "    maeTrain_ols_redData.append(metrics.mean_absolute_error(y_predTrain, y_train))\n",
    "    \n",
    "    X_test = sm.add_constant(X_test)\n",
    "    y_predTest = model.predict(X_test)\n",
    "    maeTest_ols_redData.append(metrics.mean_absolute_error(y_predTest, y_test))    \n",
    "    \n",
    "    \n",
    "maeTrainMean['ols_redData'] = np.mean(maeTrain_ols_redData)\n",
    "maeTrainStd['ols_redData']= np.std(maeTrain_ols_redData)\n",
    "\n",
    "maeTestMean['ols_redData'] = np.mean(maeTest_ols_redData)\n",
    "maeTestStd['ols_redData']= np.std(maeTest_ols_redData)\n",
    "\n",
    "    \n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_ols_redData), 'with std: ', np.std(maeTrain_ols_redData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_ols_redData), 'with std: ', np.std(maeTest_ols_redData))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression on PCA_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.9568265160092728 with std:  0.014667824166014956\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.9956352021110394 with std:  0.1221531154053402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Linear regression on pDiff using OLS on PCA data\n",
    "\n",
    "# cross-validation for Linear regression on pDiff using leave-one-subject-out using OLS\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_ols_pcaData, maeTest_ols_pcaData = list(), list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_PCA, Y):\n",
    "    \n",
    "    \n",
    "    X_train, X_test = X_PCA.iloc[train_index], X_PCA.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    model = smf.ols('y_train ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10', data = X_train)\n",
    "    model = model.fit(cov_type='HC3')\n",
    "    \n",
    "    \n",
    "    X_train = sm.add_constant(X_train)\n",
    "    y_predTrain = model.predict(X_train)\n",
    "    maeTrain_ols_pcaData.append(metrics.mean_absolute_error(y_predTrain, y_train))\n",
    "    \n",
    "    X_test = sm.add_constant(X_test)\n",
    "    y_predTest = model.predict(X_test)\n",
    "    maeTest_ols_pcaData.append(metrics.mean_absolute_error(y_predTest, y_test))    \n",
    "    \n",
    "    \n",
    "maeTrainMean['ols_pcaData'] = np.mean(maeTrain_ols_pcaData)\n",
    "maeTrainStd['ols_pcaData']= np.std(maeTrain_ols_pcaData)\n",
    "\n",
    "maeTestMean['ols_pcaData'] = np.mean(maeTest_ols_pcaData)\n",
    "maeTestStd['ols_pcaData']= np.std(maeTest_ols_pcaData)\n",
    "\n",
    "    \n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_ols_pcaData), 'with std: ', np.std(maeTrain_ols_pcaData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_ols_pcaData), 'with std: ', np.std(maeTest_ols_pcaData))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm_allReduced = X_norm_all.drop(['PupilAbsolute_Mean', \\\n",
    "'PupilDifference_StartingEnding', 'PupilRelative_Mean', 'LHIPA', \\\n",
    "'BlinkDurationAverage', 'PupilRelative_Start'], axis=1)\n",
    "\n",
    "X_normReduced = X_norm.drop(['PupilAbsolute_Mean', \\\n",
    "'PupilDifference_StartingEnding', 'PupilRelative_Mean', 'LHIPA', \\\n",
    "'BlinkDurationAverage', 'PupilRelative_Start'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TotalTime</th>\n",
       "      <th>TypingSpeed</th>\n",
       "      <th>ErrorRate</th>\n",
       "      <th>BlinkFrequency</th>\n",
       "      <th>SubjectID</th>\n",
       "      <th>BlockID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.016127</td>\n",
       "      <td>-0.006768</td>\n",
       "      <td>-0.029719</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.024759</td>\n",
       "      <td>-0.015421</td>\n",
       "      <td>-0.029719</td>\n",
       "      <td>0.046282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.037566</td>\n",
       "      <td>-0.007165</td>\n",
       "      <td>-0.009149</td>\n",
       "      <td>0.023133</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016004</td>\n",
       "      <td>-0.004969</td>\n",
       "      <td>-0.021707</td>\n",
       "      <td>0.048643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036926</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>-0.029719</td>\n",
       "      <td>0.019890</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TotalTime  TypingSpeed  ErrorRate  BlinkFrequency SubjectID BlockID\n",
       "0  -0.016127    -0.006768  -0.029719        0.068978       1.0     1.0\n",
       "1  -0.024759    -0.015421  -0.029719        0.046282       1.0     1.0\n",
       "2  -0.037566    -0.007165  -0.009149        0.023133       1.0     1.0\n",
       "3  -0.016004    -0.004969  -0.021707        0.048643       1.0     1.0\n",
       "4  -0.036926     0.012007  -0.029719        0.019890       1.0     1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm_allReduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests \n",
    "Model optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'max_features' : (4,5,6,7,8,9,10)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n",
    "                                max_features=best_params[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 7, 'n_estimators': 500}\n",
      "Neg_mean_absolute error:  -1.0235573573138435 0.13534770388347003\n"
     ]
    }
   ],
   "source": [
    "scores_rfr, bestParams_rfr = rfr_model(X_norm, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_rfr), np.std(scores_rfr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot oob error\n",
    "maeOOBList = list()\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "\n",
    "for j, nTrees in enumerate([20, 50, 100, 200, 300, 500, 700, 1000]):\n",
    "    maeOOBList = list()\n",
    "    \n",
    "    for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "        X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        \n",
    "        rf = RandomForestRegressor(max_depth=6, n_estimators=nTrees,\n",
    "                                   max_features=7, random_state=False, verbose=False, \n",
    "                                   oob_score=True)\n",
    "        \n",
    "        \n",
    "        #rf = RandomForestRegressor(max_depth=bestParams_rfr[\"max_depth\"], n_estimators=nTrees,\n",
    "        #                           max_features=bestParams_rfr[\"max_features\"], random_state=False, verbose=False, \n",
    "        #                           oob_score=True)\n",
    "        # Train the model on training data\n",
    "        rf.fit(X_train, y_train)\n",
    "    \n",
    "        # Use out-of-bag prediction scores to estimate the number of trees\n",
    "        maeOOBList.append(1-(rf.oob_score_))\n",
    "        \n",
    "    plt.boxplot(maeOOBList, positions=[j+1])\n",
    "        \n",
    "plt.xlabel('Number of trees')\n",
    "plt.ylabel('Out of bag error')\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7, 8], [20, 50, 100, 200, 300, 500, 700, 1000])\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\Random Forests\\oobError_modelSelection', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(rf, cv = None, refit = False, n_iter = 50).fit(X_train, y_train)\n",
    "\n",
    "imp_df = pd.DataFrame({'feature': X_train.columns.values,\n",
    "                       'importance': perm.feature_importances_})\n",
    "\n",
    "# Reorder by importance\n",
    "ordered_df = imp_df.sort_values(by='importance')\n",
    "imp_range=range(1,len(imp_df.index)+1)\n",
    "\n",
    "## Barplot with confidence intervals\n",
    "height = ordered_df['importance']\n",
    "bars = ordered_df['feature']\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "# Create horizontal bars\n",
    "plt.barh(y_pos, height)\n",
    "\n",
    "# Create names on the y-axis\n",
    "plt.yticks(y_pos, bars)\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# Show graphic\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\Random Forests\\variable_importance', dpi=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.22906592396235767 with std:  0.008555950656215035\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.4600874752675206 with std:  0.10043159865384868\n",
      "Mean accuracy:  65.96519584913061 with std:  4.570615104290552\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_rf_complData, maeTest_rf_complData = list(), list()\n",
    "accuracy_rf_complData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=bestParams_rfr[\"max_depth\"], n_estimators=bestParams_rfr[\"n_estimators\"],\n",
    "                                max_features=bestParams_rfr[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_rf_complData.append(1-(rf.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_rf_complData.append(1-(rf.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = rf.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_rf_complData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['rf_complData'] = np.mean(maeTrain_rf_complData)\n",
    "maeTrainStd['rf_complData']= np.std(maeTrain_rf_complData)\n",
    "\n",
    "maeTestMean['rf_complData'] = np.mean(maeTest_rf_complData)\n",
    "maeTestStd['rf_complData']= np.std(maeTest_rf_complData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_rf_complData), 'with std: ', np.std(maeTrain_rf_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_rf_complData), 'with std: ', np.std(maeTest_rf_complData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_rf_complData), 'with std: ', np.std(accuracy_rf_complData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model_reduced(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'max_features' : (1,2,3,4)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n",
    "                                max_features=best_params[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 2, 'n_estimators': 50}\n",
      "Neg_mean_absolute error:  -1.0320243290350424 0.14248870308992692\n"
     ]
    }
   ],
   "source": [
    "scores_rfr_reduced, bestParams_rfr_reduced = rfr_model_reduced(X_normReduced, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_rfr_reduced), np.std(scores_rfr_reduced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 2, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 3, 'n_estimators': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 2, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 3, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 3, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 2, 'n_estimators': 20}\n",
      "optimized parameters:  {'max_depth': 6, 'max_features': 2, 'n_estimators': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 4, 'max_features': 3, 'n_estimators': 50}\n",
      "optimized parameters:  {'max_depth': 4, 'max_features': 4, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 3, 'n_estimators': 1000}\n",
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.3340580946717558 with std:  0.029041114162071217\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.4895547368654203 with std:  0.10845865457693213\n",
      "Mean accuracy:  65.35671361148758 with std:  4.484123078804191\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_rf_redData, maeTest_rf_redData = list(), list()\n",
    "accuracy_rf_redData = list()\n",
    "\n",
    "nFeatures = min(bestParams_rfr[\"max_features\"], len(X_normReduced.columns))\n",
    "                \n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_normReduced.iloc[train_index], X_normReduced.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=bestParams_rfr_reduced[\"max_depth\"], \n",
    "                               n_estimators=bestParams_rfr_reduced[\"n_estimators\"],\n",
    "                               max_features=bestParams_rfr_reduced[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_rf_redData.append(1-(rf.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_rf_redData.append(1-(rf.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = rf.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_rf_redData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['rf_redData'] = np.mean(maeTrain_rf_redData)\n",
    "maeTrainStd['rf_redData']= np.std(maeTrain_rf_redData)\n",
    "\n",
    "maeTestMean['rf_redData'] = np.mean(maeTest_rf_redData)\n",
    "maeTestStd['rf_redData']= np.std(maeTest_rf_redData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_rf_redData), 'with std: ', np.std(maeTrain_rf_redData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_rf_redData), 'with std: ', np.std(maeTest_rf_redData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_rf_redData), 'with std: ', np.std(accuracy_rf_redData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfr_model_pca(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'max_features' :(4,5,6,7,8,9,10)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n",
    "                                max_features=best_params[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(rfr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 500}\n",
      "Neg_mean_absolute error:  -1.0942529527969431 0.1498015283435986\n"
     ]
    }
   ],
   "source": [
    "scores_rfr_pca, bestParams_rfr_pca = rfr_model_pca(X_PCA, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_rfr_pca), np.std(scores_rfr_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 500}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 6, 'n_estimators': 50}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 300}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 5, 'n_estimators': 100}\n",
      "optimized parameters:  {'max_depth': 6, 'max_features': 5, 'n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 700}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 1000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 6, 'max_features': 4, 'n_estimators': 700}\n",
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.25127069332422836 with std:  0.010076387055002559\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.5028369663975583 with std:  0.10952769273179158\n",
      "Mean accuracy:  61.4411574658367 with std:  3.9478342467372913\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_rf_pcaData, maeTest_rf_pcaData = list(), list()\n",
    "accuracy_rf_pcaData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_PCA.iloc[train_index], X_PCA.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    scores_rfr_pca, bestParams_rfr_pca = rfr_model_pca(X_train, y_train)\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=bestParams_rfr_pca[\"max_depth\"], n_estimators=bestParams_rfr_pca[\"n_estimators\"],\n",
    "                                max_features=bestParams_rfr_pca[\"max_features\"], random_state=False, verbose=False)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_rf_pcaData.append(1-(rf.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_rf_pcaData.append(1-(rf.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = rf.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_rf_pcaData.append(accuracy)    \n",
    "\n",
    "    \n",
    "maeTrainMean['rf_pcaData'] = np.mean(maeTrain_rf_pcaData)\n",
    "maeTrainStd['rf_pcaData']= np.std(maeTrain_rf_pcaData)\n",
    "\n",
    "maeTestMean['rf_pcaData'] = np.mean(maeTest_rf_pcaData)\n",
    "maeTestStd['rf_pcaData']= np.std(maeTest_rf_pcaData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_rf_pcaData), 'with std: ', np.std(maeTrain_rf_pcaData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_rf_pcaData), 'with std: ', np.std(maeTest_rf_pcaData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_rf_pcaData), 'with std: ', np.std(accuracy_rf_pcaData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtr_model(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=DecisionTreeRegressor(criterion='mae', ),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'max_features' : (4,5,6,7,8,9,10)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    dtr = DecisionTreeRegressor(max_depth=best_params[\"max_depth\"], max_features=best_params[\"max_features\"],\n",
    "                                criterion='mae', \n",
    "                                random_state=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(dtr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 8}\n",
      "Neg_mean_absolute error:  -1.1057633053221287 0.17143324576319816\n"
     ]
    }
   ],
   "source": [
    "scores_dtr, bestParams_dtr = dtr_model(X_norm, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_dtr), np.std(scores_dtr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.3896850327040512 with std:  0.013926905043341181\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.6682990145186316 with std:  0.14241015878593755\n",
      "Mean accuracy:  63.85969168566821 with std:  5.045906566144584\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_decTree_complData, maeTest_decTree_complData = list(), list()\n",
    "accuracy_decTree_complData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    regr = DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr['max_depth'], \n",
    "                                 max_features=bestParams_dtr[\"max_features\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_decTree_complData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_decTree_complData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_decTree_complData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['decTree_complData'] = np.mean(maeTrain_decTree_complData)\n",
    "maeTrainStd['decTree_complData']= np.std(maeTrain_decTree_complData)\n",
    "\n",
    "maeTestMean['decTree_complData'] = np.mean(maeTest_decTree_complData)\n",
    "maeTestStd['decTree_complData']= np.std(maeTest_decTree_complData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_decTree_complData), 'with std: ', np.std(maeTrain_decTree_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_decTree_complData), 'with std: ', np.std(maeTest_decTree_complData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_decTree_complData), 'with std: ', np.std(accuracy_decTree_complData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtr_model_red(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=DecisionTreeRegressor(criterion='mae', ),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'max_features' : (1,2,3,4)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    dtr = DecisionTreeRegressor(max_depth=best_params[\"max_depth\"], max_features=best_params[\"max_features\"],\n",
    "                                criterion='mae', \n",
    "                                random_state=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(dtr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 5, 'max_features': 4}\n",
      "Neg_mean_absolute error:  -1.1131022408963585 0.2057053285561849\n"
     ]
    }
   ],
   "source": [
    "scores_dtr_red, bestParams_dtr_red = dtr_model_red(X_normReduced, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_dtr_red), np.std(scores_dtr_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.4044285134982915 with std:  0.018730387237539333\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.6011691891370963 with std:  0.14950524039250193\n",
      "Mean accuracy:  66.96824310458507 with std:  5.476058842136069\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_decTree_redData, maeTest_decTree_redData = list(), list()\n",
    "accuracy_decTree_redData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_normReduced.iloc[train_index], X_normReduced.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    regr = DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr_red['max_depth'], \n",
    "                                 max_features=bestParams_dtr_red[\"max_features\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_decTree_redData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_decTree_redData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_decTree_redData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['decTree_redData'] = np.mean(maeTrain_decTree_redData)\n",
    "maeTrainStd['decTree_redData']= np.std(maeTrain_decTree_redData)\n",
    "\n",
    "maeTestMean['decTree_redData'] = np.mean(maeTest_decTree_redData)\n",
    "maeTestStd['decTree_redData']= np.std(maeTest_decTree_redData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_decTree_redData), 'with std: ', np.std(maeTrain_decTree_redData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_decTree_redData), 'with std: ', np.std(maeTest_decTree_redData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_decTree_redData), 'with std: ', np.std(accuracy_decTree_redData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtr_model_pca(X, y):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=DecisionTreeRegressor(criterion='mae', ),\n",
    "        param_grid={\n",
    "            'max_depth': range(3,7),\n",
    "            'max_features' : (4,5,6,7,8,9,10)\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    dtr = DecisionTreeRegressor(max_depth=best_params[\"max_depth\"], max_features=best_params[\"max_features\"],\n",
    "                                criterion='mae', \n",
    "                                random_state=False)\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(dtr, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'max_depth': 4, 'max_features': 9}\n",
      "Neg_mean_absolute error:  -1.1653991596638655 0.1362809814835891\n"
     ]
    }
   ],
   "source": [
    "scores_dtr_pca, bestParams_dtr_pca = dtr_model_pca(X_PCA, Y)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_dtr_pca), np.std(scores_dtr_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.5015223613563137 with std:  0.02050002204457306\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.7690044368826843 with std:  0.22563416252340418\n",
      "Mean accuracy:  57.96089911636848 with std:  7.065980816365375\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_decTree_pcaData, maeTest_decTree_pcaData = list(), list()\n",
    "accuracy_decTree_pcaData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_PCA.iloc[train_index], X_PCA.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    regr = DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr_pca['max_depth'], \n",
    "                                 max_features=bestParams_dtr_pca[\"max_features\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_decTree_pcaData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_decTree_pcaData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_decTree_pcaData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['decTree_pcaData'] = np.mean(maeTrain_decTree_pcaData)\n",
    "maeTrainStd['decTree_pcaData']= np.std(maeTrain_decTree_pcaData)\n",
    "\n",
    "maeTestMean['decTree_pcaData'] = np.mean(maeTest_decTree_pcaData)\n",
    "maeTestStd['decTree_pcaData']= np.std(maeTest_decTree_pcaData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_decTree_pcaData), 'with std: ', np.std(maeTrain_decTree_pcaData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_decTree_pcaData), 'with std: ', np.std(maeTest_decTree_pcaData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_decTree_pcaData), 'with std: ', np.std(accuracy_decTree_pcaData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboosting with Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_model(X, y, bestParams_dtr):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=AdaBoostRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'loss' : ['linear', 'square', 'exponential']\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    ab = AdaBoostRegressor(DecisionTreeRegressor(max_depth=bestParams_dtr[\"max_depth\"], \n",
    "                           max_features=bestParams_dtr[\"max_features\"], criterion='mae', random_state=False), \n",
    "                           n_estimators=best_params[\"n_estimators\"])\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(ab, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'loss': 'exponential', 'n_estimators': 20}\n",
      "Neg_mean_absolute error:  -1.0277661064425767 0.14216965435542822\n"
     ]
    }
   ],
   "source": [
    "scores_ab, bestParams_ab = ab_model(X_norm, Y, bestParams_dtr)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_ab), np.std(scores_ab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.22976037772272898 with std:  0.010112797992370947\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.5166087981398905 with std:  0.09558212017147373\n",
      "Mean accuracy:  66.05705960678017 with std:  4.349315283596647\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_adaBoost_complData, maeTest_adaBoost_complData = list(), list()\n",
    "accuracy_adaBoost_complData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    #regr = DecisionTreeRegressor(max_depth=3)\n",
    "    regr = AdaBoostRegressor(DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr['max_depth'], \n",
    "                                 max_features=bestParams_dtr[\"max_features\"]), n_estimators=bestParams_ab['n_estimators'],\n",
    "                            loss = bestParams_ab[\"loss\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_adaBoost_complData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_adaBoost_complData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_adaBoost_complData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['adaBoost_complData'] = np.mean(maeTrain_adaBoost_complData)\n",
    "maeTrainStd['adaBoost_complData']= np.std(maeTrain_adaBoost_complData)\n",
    "\n",
    "maeTestMean['adaBoost_complData'] = np.mean(maeTest_adaBoost_complData)\n",
    "maeTestStd['adaBoost_complData']= np.std(maeTest_adaBoost_complData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_adaBoost_complData), 'with std: ', np.std(maeTrain_adaBoost_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_adaBoost_complData), 'with std: ', np.std(maeTest_adaBoost_complData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_adaBoost_complData), 'with std: ', np.std(accuracy_adaBoost_complData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_model_red(X, y, bestParams_dtr_red):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=AdaBoostRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'loss' : ['linear', 'square', 'exponential']\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    ab = AdaBoostRegressor(DecisionTreeRegressor(max_depth=bestParams_dtr_red[\"max_depth\"], \n",
    "                           max_features=bestParams_dtr_red[\"max_features\"], criterion='mae', random_state=False), \n",
    "                           n_estimators=best_params[\"n_estimators\"])\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(ab, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'loss': 'linear', 'n_estimators': 50}\n",
      "Neg_mean_absolute error:  -1.0694257703081234 0.15285888809356493\n"
     ]
    }
   ],
   "source": [
    "scores_ab_red, bestParams_ab_red = ab_model_red(X_normReduced, Y, bestParams_dtr_red)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_ab_red), np.std(scores_ab_red))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.23212028112732322 with std:  0.009272217340505474\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.5824717800415414 with std:  0.120513187611971\n",
      "Mean accuracy:  63.5444002929557 with std:  5.297643074441898\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_adaBoost_redData, maeTest_adaBoost_redData = list(), list()\n",
    "accuracy_adaBoost_redData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_normReduced.iloc[train_index], X_normReduced.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    #regr = DecisionTreeRegressor(max_depth=3)\n",
    "    regr = AdaBoostRegressor(DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr_red['max_depth'], \n",
    "                             max_features=bestParams_dtr_red[\"max_features\"]), \n",
    "                             n_estimators=bestParams_ab_red['n_estimators'], loss=bestParams_ab_red[\"loss\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_adaBoost_redData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_adaBoost_redData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_adaBoost_redData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['adaBoost_redData'] = np.mean(maeTrain_adaBoost_redData)\n",
    "maeTrainStd['adaBoost_redData']= np.std(maeTrain_adaBoost_redData)\n",
    "\n",
    "maeTestMean['adaBoost_redData'] = np.mean(maeTest_adaBoost_redData)\n",
    "maeTestStd['adaBoost_redData']= np.std(maeTest_adaBoost_redData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_adaBoost_redData), 'with std: ', np.std(maeTrain_adaBoost_redData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_adaBoost_redData), 'with std: ', np.std(maeTest_adaBoost_redData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_adaBoost_redData), 'with std: ', np.std(accuracy_adaBoost_redData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ab_model_pca(X, y, bestParams_dtr_pca):\n",
    "# Perform Grid-Search\n",
    "    gsc = GridSearchCV(\n",
    "        estimator=AdaBoostRegressor(),\n",
    "        param_grid={\n",
    "            'n_estimators': (20, 50, 100, 200, 300, 500, 700, 1000),\n",
    "            'loss' : ['linear', 'square', 'exponential']\n",
    "        },\n",
    "        cv=5, scoring='neg_mean_squared_error', verbose=0, n_jobs=-1)\n",
    "    \n",
    "    grid_result = gsc.fit(X, y)\n",
    "    best_params = grid_result.best_params_\n",
    "    \n",
    "    print('optimized parameters: ', best_params)\n",
    "    \n",
    "    ab = AdaBoostRegressor(DecisionTreeRegressor(max_depth=bestParams_dtr_pca[\"max_depth\"], \n",
    "                           max_features=bestParams_dtr_pca[\"max_features\"], criterion='mae', random_state=False), \n",
    "                           n_estimators=best_params[\"n_estimators\"])\n",
    "    # Perform K-Fold CV\n",
    "    scores = cross_val_score(ab, X, y, cv=10, scoring='neg_mean_absolute_error')\n",
    "\n",
    "    return scores, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimized parameters:  {'loss': 'square', 'n_estimators': 300}\n",
      "Neg_mean_absolute error:  -1.0592296918767508 0.17613301323439254\n"
     ]
    }
   ],
   "source": [
    "scores_ab_pca, bestParams_ab_pca = ab_model_pca(X_PCA, Y, bestParams_dtr_pca)\n",
    "print('Neg_mean_absolute error: ', np.mean(scores_ab_pca), np.std(scores_ab_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.2324581451393691 with std:  0.011453111879473387\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 0.5920453045733816 with std:  0.08053115075823052\n",
      "Mean accuracy:  64.08558759423171 with std:  3.8537682463432845\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_adaBoost_pcaData, maeTest_adaBoost_pcaData = list(), list()\n",
    "accuracy_adaBoost_pcaData = list()\n",
    "\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    \n",
    "    X_train, X_test = X_PCA.iloc[train_index], X_PCA.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    #regr = DecisionTreeRegressor(max_depth=3)\n",
    "    regr = AdaBoostRegressor(DecisionTreeRegressor(criterion='mae', max_depth=bestParams_dtr['max_depth'], \n",
    "                             max_features=bestParams_dtr[\"max_features\"]), n_estimators=bestParams_ab['n_estimators'],\n",
    "                             loss = bestParams_ab_pca[\"loss\"])\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    # Training data metrics:\n",
    "    maeTrain_adaBoost_pcaData.append(1-(regr.score(X_train, y_train)))\n",
    "    \n",
    "    # testing data metrics:\n",
    "    maeTest_adaBoost_pcaData.append(1-(regr.score(X_test, y_test)))\n",
    "    # Use the forest's predict method on the test data\n",
    "    test_pred = regr.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = np.sqrt((test_pred - y_test)**2)    \n",
    "    # Calculate mean absolute percentage error (MAPE)\n",
    "    mape = 100 * (errors / (y_test+1))\n",
    "    # Calculate and display accuracy\n",
    "    accuracy = 100 - np.mean(mape)\n",
    "    accuracy_adaBoost_pcaData.append(accuracy)    \n",
    "    \n",
    "\n",
    "maeTrainMean['adaBoost_pcaData'] = np.mean(maeTrain_adaBoost_pcaData)\n",
    "maeTrainStd['adaBoost_pcaData']= np.std(maeTrain_adaBoost_pcaData)\n",
    "\n",
    "maeTestMean['adaBoost_pcaData'] = np.mean(maeTest_adaBoost_pcaData)\n",
    "maeTestStd['adaBoost_pcaData']= np.std(maeTest_adaBoost_pcaData)\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_adaBoost_pcaData), 'with std: ', np.std(maeTrain_adaBoost_pcaData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_adaBoost_pcaData), 'with std: ', np.std(maeTest_adaBoost_pcaData))\n",
    "print('Mean accuracy: ', np.mean(accuracy_adaBoost_pcaData), 'with std: ', np.std(accuracy_adaBoost_pcaData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training error bar plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPlot_regr = ['ols', 'rf', 'decTree', 'adaBoost']\n",
    "dataPlot = ['compl', 'red', 'pca']\n",
    "\n",
    "trainingErrorMean_regr, trainingErrorStd_regr, testingErrorMean_regr, testingErrorStd_regr = list(), list(), list(), list()\n",
    "\n",
    "trainingErrorMean_regr = [maeTrainMean[modelName + '_' + dataType + 'Data'] for modelName in modelPlot_regr for dataType in dataPlot]\n",
    "trainingErrorStd_regr = [maeTrainStd[modelName + '_' + dataType + 'Data'] for modelName in modelPlot_regr for dataType in dataPlot]\n",
    "testingErrorMean_regr = [maeTestMean[modelName + '_' + dataType + 'Data'] for modelName in modelPlot_regr for dataType in dataPlot]\n",
    "testingErrorStd_regr = [maeTestStd[modelName + '_' + dataType + 'Data'] for modelName in modelPlot_regr for dataType in dataPlot]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = [1,2,3,5,6,7,9,10,11,13,14,15]\n",
    "plt.figure(figsize=(12,9))\n",
    "for i, model in enumerate(modelPlot_regr):\n",
    "    p1 = plt.bar(x_ticks[3*i], trainingErrorMean_regr[3*i], yerr = trainingErrorStd_regr[3*i], color ='b')\n",
    "    p2 = plt.bar(x_ticks[3*i+1], trainingErrorMean_regr[3*i+1], yerr = trainingErrorStd_regr[3*i+1], color ='r')\n",
    "    p3 = plt.bar(x_ticks[3*i+2], trainingErrorMean_regr[3*i+2], yerr = trainingErrorStd_regr[3*i+2], color ='g')\n",
    "\n",
    "plt.ylim([0,1.3])\n",
    "plt.xticks([2, 6, 10, 14], ['LR', 'RF', 'DT', 'Adaboost'])\n",
    "plt.ylabel('Training error (Mean absolute error)')\n",
    "plt.legend((p1, p2, p3), ('Complete data', 'Reduced data', 'PCA data'))\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\regression_trainingError', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ticks = [1,2,3,5,6,7,9,10,11,13,14,15]\n",
    "plt.figure(figsize=(12,9))\n",
    "for i, model in enumerate(modelPlot_regr):\n",
    "    p1 = plt.bar(x_ticks[3*i], testingErrorMean_regr[3*i], yerr = testingErrorStd_regr[3*i], color ='b')\n",
    "    p2 = plt.bar(x_ticks[3*i+1], testingErrorMean_regr[3*i+1], yerr = testingErrorStd_regr[3*i+1], color ='r')\n",
    "    p3 = plt.bar(x_ticks[3*i+2], testingErrorMean_regr[3*i+2], yerr = testingErrorStd_regr[3*i+2], color ='g')\n",
    "\n",
    "plt.ylim([0,1.3])\n",
    "plt.xticks([2, 6, 10, 14], ['LR', 'RF', 'DT', 'Adaboost'])\n",
    "plt.ylabel('Testing error (Mean absolute error)')\n",
    "plt.legend((p1, p2, p3), ('Complete data', 'Reduced data', 'PCA data'))\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\regression_testingError', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation before linear modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.604\n",
      "Model:                            OLS   Adj. R-squared:                  0.568\n",
      "Method:                 Least Squares   F-statistic:                     17.12\n",
      "Date:                Sun, 29 Sep 2019   Prob (F-statistic):           6.44e-43\n",
      "Time:                        00:10:36   Log-Likelihood:                -455.99\n",
      "No. Observations:                 307   AIC:                             964.0\n",
      "Df Residuals:                     281   BIC:                             1061.\n",
      "Df Model:                          25                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "Intercept                      -22.7683      4.014     -5.673      0.000     -30.669     -14.868\n",
      "X_norm_all.SubjectID[T.2.0]     -0.0333      0.750     -0.044      0.965      -1.509       1.442\n",
      "X_norm_all.SubjectID[T.3.0]     -2.0693      0.742     -2.789      0.006      -3.530      -0.609\n",
      "X_norm_all.SubjectID[T.4.0]      0.0898      0.760      0.118      0.906      -1.405       1.585\n",
      "X_norm_all.SubjectID[T.5.0]     -1.8305      0.730     -2.508      0.013      -3.267      -0.394\n",
      "X_norm_all.SubjectID[T.6.0]     -1.2907      0.748     -1.725      0.086      -2.763       0.182\n",
      "X_norm_all.SubjectID[T.7.0]     -1.3809      0.733     -1.883      0.061      -2.824       0.063\n",
      "X_norm_all.SubjectID[T.8.0]     -0.2628      0.763     -0.344      0.731      -1.765       1.240\n",
      "X_norm_all.SubjectID[T.9.0]     -1.3712      0.731     -1.876      0.062      -2.810       0.067\n",
      "X_norm_all.SubjectID[T.10.0]    -0.8871      0.800     -1.110      0.268      -2.461       0.687\n",
      "X_norm_all.SubjectID[T.11.0]    -0.0859      0.745     -0.115      0.908      -1.553       1.381\n",
      "X_norm_all.SubjectID[T.12.0]     0.5395      0.761      0.709      0.479      -0.958       2.037\n",
      "X_norm_all.SubjectID[T.13.0]    -1.2117      0.740     -1.638      0.102      -2.668       0.244\n",
      "X_norm_all.SubjectID[T.14.0]     0.5614      0.751      0.747      0.456      -0.917       2.040\n",
      "X_norm_all.SubjectID[T.15.0]    -0.2910      0.793     -0.367      0.714      -1.853       1.270\n",
      "X_norm_all.SubjectID[T.16.0]    -0.2176      0.725     -0.300      0.764      -1.645       1.210\n",
      "X_norm_all.SubjectID[T.17.0]    -0.9176      0.829     -1.107      0.269      -2.550       0.714\n",
      "X_norm_all.SubjectID[T.18.0]    -0.4076      0.754     -0.540      0.589      -1.893       1.077\n",
      "X_norm_all.BlockID[T.2.0]       -0.1503      0.201     -0.748      0.455      -0.546       0.245\n",
      "X_norm_all.BlockID[T.3.0]       -0.6743      0.212     -3.177      0.002      -1.092      -0.257\n",
      "X_norm_all.BlockID[T.4.0]       -0.5041      0.207     -2.436      0.015      -0.911      -0.097\n",
      "X_norm_all.BlockID[T.5.0]       -0.7201      0.221     -3.262      0.001      -1.155      -0.286\n",
      "np.exp(TotalTime)               21.1796      2.374      8.920      0.000      16.506      25.854\n",
      "TypingSpeed                     -7.0218      2.978     -2.358      0.019     -12.884      -1.160\n",
      "np.log(ErrorRate)                0.3187      0.056      5.736      0.000       0.209       0.428\n",
      "np.exp(BlinkFrequency)           7.4362      3.024      2.459      0.015       1.484      13.388\n",
      "==============================================================================\n",
      "Omnibus:                        1.611   Durbin-Watson:                   1.871\n",
      "Prob(Omnibus):                  0.447   Jarque-Bera (JB):                1.428\n",
      "Skew:                           0.015   Prob(JB):                        0.490\n",
      "Kurtosis:                       3.333   Cond. No.                         347.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:1: RuntimeWarning: invalid value encountered in log\n",
      "<string>:1: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "# cross-validation for Linear regression on pDiff using leave-one-subject-out using OLS\n",
    "\n",
    "model = smf.ols('Y ~ np.exp(TotalTime) + TypingSpeed + np.log(ErrorRate) + np.exp(BlinkFrequency) +\\\n",
    "    X_norm_all.SubjectID+X_norm_all.BlockID', data=X_norm)\n",
    "model = model.fit()\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1ScoreTestMean, f1ScoreTestStd = dict(), dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Multiple Logistic regression\n",
    "using minirank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy import linalg, optimize, sparse\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "BIG = 1e10\n",
    "SMALL = 1e-12\n",
    "\n",
    "\n",
    "def phi(t):\n",
    "    \"\"\"\n",
    "    logistic function, returns 1 / (1 + exp(-t))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.empty(t.size, dtype=np.float)\n",
    "    out[idx] = 1. / (1 + np.exp(-t[idx]))\n",
    "    exp_t = np.exp(t[~idx])\n",
    "    out[~idx] = exp_t / (1. + exp_t)\n",
    "    return out\n",
    "\n",
    "def log_logistic(t):\n",
    "    \"\"\"\n",
    "    (minus) logistic loss function, returns log(1 / (1 + exp(-t)))\n",
    "    \"\"\"\n",
    "    idx = t > 0\n",
    "    out = np.zeros_like(t)\n",
    "    out[idx] = np.log(1 + np.exp(-t[idx]))\n",
    "    out[~idx] = (-t[~idx] + np.log(1 + np.exp(t[~idx])))\n",
    "    return out\n",
    "\n",
    "\n",
    "def ordinal_logistic_fit(X, y, alpha=0, l1_ratio=0, n_class=None, max_iter=10000,\n",
    "                         verbose=False, solver='TNC', w0=None):\n",
    "    \"\"\"\n",
    "    Ordinal logistic regression or proportional odds model.\n",
    "    Uses scipy's optimize.fmin_slsqp solver.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : {array, sparse matrix}, shape (n_samples, n_feaures)\n",
    "        Input data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    max_iter : int\n",
    "        Maximum number of iterations\n",
    "    verbose: bool\n",
    "        Print convergence information\n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (n_features,)\n",
    "        coefficients of the linear model\n",
    "    theta : array, shape (k,), where k is the different values of y\n",
    "        vector of thresholds\n",
    "    \"\"\"\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    w0 = None\n",
    "\n",
    "    if not X.shape[0] == y.shape[0]:\n",
    "        raise ValueError('Wrong shape for X and y')\n",
    "\n",
    "    # .. order input ..\n",
    "    idx = np.argsort(y)\n",
    "    idx_inv = np.zeros_like(idx)\n",
    "    idx_inv[idx] = np.arange(idx.size)\n",
    "    X = X[idx]\n",
    "    y = y[idx].astype(np.int)\n",
    "    # make them continuous and start at zero\n",
    "    unique_y = np.unique(y)\n",
    "    for i, u in enumerate(unique_y):\n",
    "        y[y == u] = i\n",
    "    unique_y = np.unique(y)\n",
    "\n",
    "    # .. utility arrays used in f_grad ..\n",
    "    alpha = 0.\n",
    "    k1 = np.sum(y == unique_y[0])\n",
    "    E0 = (y[:, np.newaxis] == np.unique(y)).astype(np.int)\n",
    "    E1 = np.roll(E0, -1, axis=-1)\n",
    "    E1[:, -1] = 0.\n",
    "    E0, E1 = map(sparse.csr_matrix, (E0.T, E1.T))\n",
    "\n",
    "    def f_obj(x0, X, y):\n",
    "        \"\"\"\n",
    "        Objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        if np.any(c > 0):\n",
    "            return BIG\n",
    "\n",
    "        #loss = -(c[idx] + np.log(np.exp(-c[idx]) - 1)).sum()\n",
    "        loss = -np.log(1 - np.exp(c)).sum()\n",
    "\n",
    "        loss += b.sum() + log_logistic(b).sum() \\\n",
    "            + log_logistic(a).sum() \\\n",
    "            + .5 * alpha * w.dot(w) - np.log(z).sum()  # penalty\n",
    "        if np.isnan(loss):\n",
    "            pass\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "        return loss\n",
    "\n",
    "    def f_grad(x0, X, y):\n",
    "        \"\"\"\n",
    "        Gradient of the objective function\n",
    "        \"\"\"\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        # gradient for w\n",
    "        phi_a = phi(a)\n",
    "        phi_b = phi(b)\n",
    "        grad_w = -X[k1:].T.dot(phi_b) + X.T.dot(1 - phi_a) + alpha * w\n",
    "\n",
    "        # gradient for theta\n",
    "        idx = c > 0\n",
    "        tmp = np.empty_like(c)\n",
    "        tmp[idx] = 1. / (np.exp(-c[idx]) - 1)\n",
    "        tmp[~idx] = np.exp(c[~idx]) / (1 - np.exp(c[~idx])) # should not need\n",
    "        grad_theta = (E1 - E0)[:, k1:].dot(tmp) \\\n",
    "            + E0[:, k1:].dot(phi_b) - E0.dot(1 - phi_a)\n",
    "\n",
    "        grad_theta[:-1] += 1. / np.diff(theta_0)\n",
    "        grad_theta[1:] -= 1. / np.diff(theta_0)\n",
    "        out = np.concatenate((grad_w, grad_theta))\n",
    "        return out\n",
    "\n",
    "    def f_hess(x0, s, X, y):\n",
    "        x0 = np.asarray(x0)\n",
    "        w, theta_0 = np.split(x0, [X.shape[1]])\n",
    "        theta_1 = np.roll(theta_0, 1)\n",
    "        t0 = theta_0[y]\n",
    "        t1 = theta_1[y]\n",
    "        z = np.diff(theta_0)\n",
    "\n",
    "        Xw = X.dot(w)\n",
    "        a = t0 - Xw\n",
    "        b = t0[k1:] - X[k1:].dot(w)\n",
    "        c = (theta_1 - theta_0)[y][k1:]\n",
    "\n",
    "        D = np.diag(phi(a) * (1 - phi(a)))\n",
    "        D_= np.diag(phi(b) * (1 - phi(b)))\n",
    "        D1 = np.diag(np.exp(-c) / (np.exp(-c) - 1) ** 2)\n",
    "        Ex = (E1 - E0)[:, k1:].toarray()\n",
    "        Ex0 = E0.toarray()\n",
    "        H_A = X[k1:].T.dot(D_).dot(X[k1:]) + X.T.dot(D).dot(X)\n",
    "        H_C = - X[k1:].T.dot(D_).dot(E0[:, k1:].T.toarray()) \\\n",
    "            - X.T.dot(D).dot(E0.T.toarray())\n",
    "        H_B = Ex.dot(D1).dot(Ex.T) + Ex0[:, k1:].dot(D_).dot(Ex0[:, k1:].T) \\\n",
    "            - Ex0.dot(D).dot(Ex0.T)\n",
    "\n",
    "        p_w = H_A.shape[0]\n",
    "        tmp0 = H_A.dot(s[:p_w]) + H_C.dot(s[p_w:])\n",
    "        tmp1 = H_C.T.dot(s[:p_w]) + H_B.dot(s[p_w:])\n",
    "        return np.concatenate((tmp0, tmp1))\n",
    "\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        import pylab as pl\n",
    "        pl.matshow(H_B)\n",
    "        pl.colorbar()\n",
    "        pl.title('True')\n",
    "        import numdifftools as nd\n",
    "        Hess = nd.Hessian(lambda x: f_obj(x, X, y))\n",
    "        H = Hess(x0)\n",
    "        pl.matshow(H[H_A.shape[0]:, H_A.shape[0]:])\n",
    "        #pl.matshow()\n",
    "        pl.title('estimated')\n",
    "        pl.colorbar()\n",
    "        pl.show()\n",
    "\n",
    "\n",
    "    def grad_hess(x0, X, y):\n",
    "        grad = f_grad(x0, X, y)\n",
    "        hess = lambda x: f_hess(x0, x, X, y)\n",
    "        return grad, hess\n",
    "\n",
    "    x0 = np.random.randn(X.shape[1] + unique_y.size) / X.shape[1]\n",
    "    if w0 is not None:\n",
    "        x0[:X.shape[1]] = w0\n",
    "    else:\n",
    "        x0[:X.shape[1]] = 0.\n",
    "    x0[X.shape[1]:] = np.sort(unique_y.size * np.random.rand(unique_y.size))\n",
    "\n",
    "    #print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y))\n",
    "    #print(f_grad(x0, X, y))\n",
    "    #print(optimize.approx_fprime(x0, f_obj, 1e-6, X, y) - f_grad(x0, X, y))\n",
    "    #import ipdb; ipdb.set_trace()\n",
    "\n",
    "    def callback(x0):\n",
    "        x0 = np.asarray(x0)\n",
    "        # print('Check grad: %s' % optimize.check_grad(f_obj, f_grad, x0, X, y))\n",
    "        if verbose:\n",
    "        # check that gradient is correctly computed\n",
    "            print('OBJ: %s' % f_obj(x0, X, y))\n",
    "\n",
    "    if solver == 'TRON':\n",
    "        import pytron\n",
    "        out = pytron.minimize(f_obj, grad_hess, x0, args=(X, y))\n",
    "    else:\n",
    "        #options = {'maxiter' : max_iter, 'disp': 0, 'maxfun':10000}\n",
    "        #out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "        #    jac=f_grad, hessp=f_hess, options=options, callback=callback)\n",
    "        options = {'maxiter' : max_iter, 'disp': 0}\n",
    "        out = optimize.minimize(f_obj, x0, args=(X, y), method=solver,\n",
    "            jac=f_grad, options=options, callback=callback)\n",
    "\n",
    "    if not out.success:\n",
    "        warnings.warn(out.message)\n",
    "    w, theta = np.split(out.x, [X.shape[1]])\n",
    "    return w, theta\n",
    "\n",
    "\n",
    "def ordinal_logistic_predict(w, theta, X):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : coefficients obtained by ordinal_logistic\n",
    "    theta : thresholds\n",
    "    \"\"\"\n",
    "    unique_theta = np.sort(np.unique(theta))\n",
    "    out = X.dot(w)\n",
    "    unique_theta[-1] = np.inf # p(y <= max_level) = 1\n",
    "    tmp = out[:, None].repeat(unique_theta.size, axis=1)\n",
    "    return np.argmax(tmp < unique_theta, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 0.9992108776700034 with std:  0.010351434559721482\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 1.0080839746037087 with std:  0.09148769676553258\n",
      "Mean F1 score:  0.26341863735686094 with std:  0.04685497221238029\n"
     ]
    }
   ],
   "source": [
    "# cross-validation for Logistic regression using minirank on pDiff using leave-one-subject-out \n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "maeTrain_logR_complData, maeTest_logR_complData = list(), list()\n",
    "f1Score_logR_complData = list()\n",
    "\n",
    "j = 0\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    j = j + 1\n",
    "    X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    w, theta = ordinal_logistic_fit(X_train, y_train)\n",
    "    \n",
    "    pred_train = ordinal_logistic_predict(w, theta, X_train)\n",
    "    maeTrain_logR_complData.append(metrics.mean_absolute_error(y_train, pred_train))\n",
    "    \n",
    "    pred_test = ordinal_logistic_predict(w, theta, X_test)\n",
    "    maeTest_logR_complData.append(metrics.mean_absolute_error(y_test, pred_test))\n",
    "    f1Score_logR_complData.append(metrics.f1_score(y_test, pred_test, average='macro'))\n",
    "    \n",
    "    cr = confusion_matrix(y_test, pred_test)\n",
    "    plt.matshow(cr)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\OrdinalClassification_Method1\\cr_'+str(j), dpi = 600)\n",
    "    \n",
    "    \n",
    "maeTrainMean['logR_complData'] = np.mean(maeTrain_logR_complData)\n",
    "maeTrainStd['logR_complData']= np.std(maeTrain_logR_complData)\n",
    "\n",
    "maeTestMean['logR_complData'] = np.mean(maeTest_logR_complData)\n",
    "maeTestStd['logR_complData']= np.std(maeTest_logR_complData)\n",
    "\n",
    "f1ScoreTestMean['logR_complData'] = np.mean(f1Score_logR_complData)\n",
    "f1ScoreTestStd['logR_complData'] = np.std(f1Score_logR_complData)\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_logR_complData), 'with std: ', np.std(maeTrain_logR_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_logR_complData), 'with std: ', np.std(maeTest_logR_complData))\n",
    "print('Mean F1 score: ', np.mean(f1Score_logR_complData), 'with std: ', np.std(f1Score_logR_complData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:103: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING DATA\n",
      "Mean of mean absolute error 1.1706836889534178 with std:  0.013937258817964015\n",
      "\n",
      "TESTING DATA\n",
      "Mean of mean absolute error 1.2087625966510234 with std:  0.08060331860874445\n",
      "Mean accuracy:  0.10579191207404978 with std:  0.03222298673492697\n"
     ]
    }
   ],
   "source": [
    "# cross-validation for Logistic regression using leave-one-subject-out \n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "maeTrain_oLogR_complData, maeTest_oLogR_complData = list(), list()\n",
    "f1Score_ologR_complData = list()\n",
    "j = 0\n",
    "for train_index, test_index in skf.split(X_norm, Y):\n",
    "    j = j + 1 \n",
    "    X_train, X_test = X_norm.iloc[train_index], X_norm.iloc[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    \n",
    "    # training data for Target>1:\n",
    "    Y1 = copy.deepcopy(y_train)\n",
    "    Y1[Y1>=1] = 1\n",
    "    \n",
    "    clf1 = LogisticRegression(solver='lbfgs').fit(X_train, Y1)\n",
    "    Prob1 = clf1.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # training data for Target>2:\n",
    "    Y2 = copy.deepcopy(y_train)\n",
    "    Y2[Y2<=1] = 0\n",
    "    Y2[Y2>1] = 1\n",
    "    clf2 = LogisticRegression(solver='lbfgs').fit(X_train, Y2)\n",
    "    Prob2 = clf2.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # training data for Target>3:\n",
    "    Y3 = copy.deepcopy(y_train)\n",
    "    Y3[Y3<=2] = 0\n",
    "    Y3[Y3>2] = 1\n",
    "    clf3 = LogisticRegression(solver='lbfgs').fit(X_train, Y3)\n",
    "    Prob3 = clf3.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # training data for Target>4:\n",
    "    Y4 = copy.deepcopy(y_train)\n",
    "    Y4[Y4<=3] = 0\n",
    "    Y4[Y4>3] = 1\n",
    "    clf4 = LogisticRegression(solver='lbfgs').fit(X_train, Y4)\n",
    "    Prob4 = clf4.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # training data for Target>5:\n",
    "    Y5 = copy.deepcopy(y_train)\n",
    "    Y5[Y5<=4] = 0\n",
    "    Y5[Y5>4] = 1\n",
    "    clf5 = LogisticRegression(solver='lbfgs').fit(X_train, Y5)\n",
    "    Prob5 = clf5.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # training data for Target>6:\n",
    "    Y6 = copy.deepcopy(y_train)\n",
    "    Y6[Y6<=5] = 0\n",
    "    Y6[Y6>5] = 1\n",
    "    clf6 = LogisticRegression(solver='lbfgs').fit(X_train, Y6)\n",
    "    Prob6 = clf6.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    \n",
    "    ProbCol1 = 1-Prob1\n",
    "    ProbCol2 = Prob1*(1-Prob2)\n",
    "    ProbCol3 = Prob2*(1-Prob3)\n",
    "    ProbCol4 = Prob3*(1-Prob4)\n",
    "    ProbCol5 = Prob4*(1-Prob5)\n",
    "    ProbCol6 = Prob5*(1-Prob6)\n",
    "    ProbCol7 = Prob6\n",
    "    \n",
    "    \n",
    "    \n",
    "    ProbColumns = [[ProbCol1[i],ProbCol2[i],ProbCol3[i],ProbCol4[i],ProbCol5[i],ProbCol6[i], ProbCol7[i]] \\\n",
    "                   for i in range(len(Prob1))]\n",
    "    \n",
    "    pred_test = [np.argmax(probs) for probs in ProbColumns]\n",
    "    \n",
    "    f1Score_ologR_complData.append(metrics.f1_score(y_test, pred_test, average='macro'))\n",
    "    \n",
    "    maeTest_oLogR_complData.append(metrics.mean_absolute_error(pred_test, y_test))\n",
    "    \n",
    "    # mae for training\n",
    "    ProbTrain1 = clf1.predict_proba(X_train)[:,1]\n",
    "    ProbTrain2 = clf2.predict_proba(X_train)[:,1]\n",
    "    ProbTrain3 = clf3.predict_proba(X_train)[:,1]\n",
    "    ProbTrain4 = clf4.predict_proba(X_train)[:,1]\n",
    "    ProbTrain5 = clf5.predict_proba(X_train)[:,1]\n",
    "    ProbTrain6 = clf6.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    ProbColTest1 = 1-ProbTrain1\n",
    "    ProbColTest2 = ProbTrain1*(1-ProbTrain2)\n",
    "    ProbColTest3 = ProbTrain2*(1-ProbTrain3)\n",
    "    ProbColTest4 = ProbTrain3*(1-ProbTrain4)\n",
    "    ProbColTest5 = ProbTrain4*(1-ProbTrain5)\n",
    "    ProbColTest6 = ProbTrain5*(1-ProbTrain6)\n",
    "    ProbColTest7 = ProbTrain6\n",
    "    \n",
    "    \n",
    "    \n",
    "    ProbColumnsTest = [[ProbColTest1[i],ProbColTest2[i],ProbColTest3[i],ProbColTest4[i],ProbColTest5[i],ProbColTest6[i], \\\n",
    "                    ProbColTest7[i]] for i in range(len(ProbTrain1))]\n",
    "    \n",
    "    pred_train = [np.argmax(probs) for probs in ProbColumnsTest]\n",
    "    \n",
    "    maeTrain_oLogR_complData.append(metrics.mean_absolute_error(pred_train, y_train))\n",
    "    \n",
    "    cr = confusion_matrix(y_test, pred_test)\n",
    "    plt.matshow(cr)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\OrdinalClassification_Method2\\cr_'+str(j), dpi = 600)\n",
    "    \n",
    "maeTrainMean['oLogR_complData'] = np.mean(maeTrain_oLogR_complData)\n",
    "maeTrainStd['oLogR_complData']= np.std(maeTrain_oLogR_complData)\n",
    "\n",
    "maeTestMean['oLogR_complData'] = np.mean(maeTest_oLogR_complData)\n",
    "maeTestStd['oLogR_complData']= np.std(maeTest_oLogR_complData)\n",
    "\n",
    "f1ScoreTestMean['oLogR_complData'] = np.mean(f1Score_ologR_complData)\n",
    "f1ScoreTestStd['oLogR_complData'] = np.std(f1Score_ologR_complData)\n",
    "\n",
    "\n",
    "\n",
    "print('TRAINING DATA')    \n",
    "print('Mean of mean absolute error', np.mean(maeTrain_oLogR_complData), 'with std: ', np.std(maeTrain_oLogR_complData))\n",
    "print('')\n",
    "\n",
    "print('TESTING DATA')\n",
    "print('Mean of mean absolute error', np.mean(maeTest_oLogR_complData), 'with std: ', np.std(maeTest_oLogR_complData))\n",
    "print('Mean accuracy: ', np.mean(f1Score_ologR_complData), 'with std: ', np.std(f1Score_ologR_complData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPlot = ['logR', 'oLogR']\n",
    "dataType = 'compl'\n",
    "\n",
    "trainingErrorMean_class, trainingErrorStd_class, testingErrorMean_class, testingErrorStd_class, f1ScoreMean_class, \\\n",
    "f1ScoreStd_class = list(), list(), list(), list(), list(), list()\n",
    "\n",
    "trainingErrorMean_class = [maeTrainMean[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n",
    "trainingErrorStd_class = [maeTrainStd[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n",
    "testingErrorMean_class = [maeTestMean[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n",
    "testingErrorStd_class = [maeTestStd[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n",
    "\n",
    "f1ScoreMean_class = [f1ScoreTestMean[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n",
    "f1ScoreStd_class = [f1ScoreTestStd[modelName + '_' + dataType + 'Data'] for modelName in modelPlot]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.bar([1,2], trainingErrorMean_class, yerr = trainingErrorStd_class, color='b')\n",
    "plt.xticks([1,2], ['Method 1', 'Method 2'])\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    length = 0) # labels along the bottom edge are off\n",
    "plt.ylabel('Training error (Mean absolute error)')\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\classification_trainingError', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9))\n",
    "plt.bar([1,2], testingErrorMean_class, yerr = testingErrorStd_class, color = 'b')\n",
    "plt.ylabel('Testing error (Mean absolute error)')\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    length = 0) # labels along the bottom edge are off\n",
    "plt.xticks([1,2], ['Method 1', 'Method 2'])\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\classification_testingError', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,9)), plt.bar([1,2], f1ScoreMean_class, yerr = f1ScoreStd_class, color='b')\n",
    "plt.xticks([1,2], ['Method 1', 'Method 2'])\n",
    "plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    length = 0) # labels along the bottom edge are off\n",
    "plt.ylabel('F1 score')\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "plt.tight_layout()\n",
    "plt.savefig(r'M:\\Documents\\Courses\\Credits\\2019_04_August\\Computational Data Analysis\\Project\\Results\\perceivedDifficulty\\classification_f1Score', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.2651908420202376, 0.10579191207404978],\n",
       " [0.046627587474609454, 0.03222298673492697])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1ScoreMean_class, f1ScoreStd_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cross-validation for Logistic regression using leave-one-subject-out \n",
    "train_accuracyList = list()\n",
    "train_maeList = list()\n",
    "\n",
    "test_accuracyList = list()\n",
    "test_maeList = list()\n",
    "\n",
    "validate_accuracyList = list()\n",
    "validate_maeList = list()\n",
    "\n",
    "\n",
    "for i in range(1,11):\n",
    "    \n",
    "    subjectOut = random.sample(set(np.unique(X_norm_allReduced.SubjectID)), 2)\n",
    "    \n",
    "    X_train = X_normReduced[np.logical_and(X_norm_allReduced.SubjectID!=subjectOut[0],\\\n",
    "                                           X_norm_allReduced.SubjectID!=subjectOut[1])]\n",
    "    Y_train = Y[np.logical_and(X_norm_allReduced.SubjectID!=subjectOut[0],\\\n",
    "                                           X_norm_allReduced.SubjectID!=subjectOut[1])]\n",
    "    \n",
    "    X_validate = X_normReduced[X_norm_allReduced.SubjectID==subjectOut[0]]\n",
    "    Y_validate = Y[X_norm_allReduced.SubjectID==subjectOut[0]]\n",
    "    \n",
    "    X_test = X_normReduced[X_norm_allReduced.SubjectID==subjectOut[1]]\n",
    "    Y_test = Y[X_norm_allReduced.SubjectID==subjectOut[1]]\n",
    "    \n",
    "    \n",
    "    # training data for Target>1:\n",
    "    Y1 = copy.deepcopy(Y_train)\n",
    "    Y1[Y1>=1] = 1\n",
    "    \n",
    "    clf1 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y1)\n",
    "    Prob1 = clf1.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    # training data for Target>2:\n",
    "    Y2 = copy.deepcopy(Y_train)\n",
    "    Y2[Y2<=1] = 0\n",
    "    Y2[Y2>1] = 1\n",
    "    clf2 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y2)\n",
    "    Prob2 = clf2.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    # training data for Target>3:\n",
    "    Y3 = copy.deepcopy(Y_train)\n",
    "    Y3[Y3<=2] = 0\n",
    "    Y3[Y3>2] = 1\n",
    "    clf3 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y3)\n",
    "    Prob3 = clf3.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    # training data for Target>4:\n",
    "    Y4 = copy.deepcopy(Y_train)\n",
    "    Y4[Y4<=3] = 0\n",
    "    Y4[Y4>3] = 1\n",
    "    clf4 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y4)\n",
    "    Prob4 = clf4.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    # training data for Target>5:\n",
    "    Y5 = copy.deepcopy(Y_train)\n",
    "    Y5[Y5<=4] = 0\n",
    "    Y5[Y5>4] = 1\n",
    "    clf5 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y5)\n",
    "    Prob5 = clf5.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    # training data for Target>6:\n",
    "    Y6 = copy.deepcopy(Y_train)\n",
    "    Y6[Y6<=5] = 0\n",
    "    Y6[Y6>5] = 1\n",
    "    clf6 = SVC(random_state=0, probability = True, kernel='linear', gamma='auto').fit(X_train, Y6)\n",
    "    Prob6 = clf6.predict_proba(X_validate)[:,1]\n",
    "    \n",
    "    \n",
    "    ProbCol1 = 1-Prob1\n",
    "    ProbCol2 = Prob1*(1-Prob2)\n",
    "    ProbCol3 = Prob2*(1-Prob3)\n",
    "    ProbCol4 = Prob3*(1-Prob4)\n",
    "    ProbCol5 = Prob4*(1-Prob5)\n",
    "    ProbCol6 = Prob5*(1-Prob6)\n",
    "    ProbCol7 = Prob6\n",
    "    \n",
    "    \n",
    "    \n",
    "    ProbColumns = [[ProbCol1[i],ProbCol2[i],ProbCol3[i],ProbCol4[i],ProbCol5[i],ProbCol6[i], ProbCol7[i]] \\\n",
    "                   for i in range(len(Prob1))]\n",
    "    \n",
    "    ProbFinal = [np.argmax(probs) for probs in ProbColumns]\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(ProbFinal, Y_validate)\n",
    "    print(accuracy)\n",
    "    print(metrics.mean_absolute_error(ProbFinal, Y_validate))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
