{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from scipy import linalg as lng\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centerData(x):\n",
    "# function to center the data\n",
    "\n",
    "    x_c = x-np.mean(x)\n",
    "    \n",
    "    return x_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeData(x):\n",
    "    # function to normalize the data\n",
    "    \n",
    "    std_dev = np.std(x)\n",
    "    std_dev_c = np.where(std_dev==0, 1, std_dev) \n",
    "    x_n = (np.transpose(x)-np.mean(x, axis=1)).transpose()/std_dev_c\n",
    "    \n",
    "    return x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(X):\n",
    "    \"\"\" Center the columns (variables) of a data matrix to zero mean.\n",
    "        \n",
    "        X, MU = center(X) centers the observations of a data matrix such that each variable\n",
    "        (column) has zero mean and also returns a vector MU of mean values for each variable.\n",
    "     \"\"\" \n",
    "    n = X.shape[0]\n",
    "    mu = np.mean(X,0)\n",
    "    X = X - mu\n",
    "    return X, mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalize the columns (variables) of a data matrix to unit Euclidean length.\n",
    "    X, MU, D = normalize(X)\n",
    "    i) centers and scales the observations of a data matrix such\n",
    "    that each variable (column) has unit Euclidean length. For a normalized matrix X,\n",
    "    X'*X is equivalent to the correlation matrix of X.\n",
    "    ii) returns a vector MU of mean values for each variable.\n",
    "    iii) returns a vector D containing the Euclidean lengths for each original variable.\n",
    "    \n",
    "    See also CENTER\n",
    "    \"\"\"\n",
    "    \n",
    "    n = np.size(X, 0)\n",
    "    X, mu = center(X)\n",
    "    d = np.linalg.norm(X, ord = 2, axis = 0)\n",
    "    d[np.where(d==0)] = 1\n",
    "    X = np.divide(X, np.ones((n,1)) * d)\n",
    "    return X, mu, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizetest(X,mx,d):\n",
    "    \"\"\"Normalize the observations of a test data matrix given the mean mx and variance varx of the training.\n",
    "       X = normalizetest(X,mx,varx) centers and scales the observations of a data matrix such that each variable\n",
    "       (column) has unit length.\n",
    "       Returns X: the normalized test data\"\"\"\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    X = np.divide(np.subtract(X, np.ones((n,1))*mx), np.ones((n,1)) * d)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridgeMulti(X, _lambda, p, y):\n",
    "    inner_prod = np.linalg.inv(np.matmul(X.T, X) + _lambda * np.eye(p,p))\n",
    "    outer_prod = np.matmul(X.T, y)\n",
    "    betas = np.matmul(inner_prod, outer_prod)\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'M:\\Documents\\Courses\\Credits\\2019August\\Module3\\Exercises 3\\Python'\n",
    "mat = scipy.io.loadmat(path + '\\\\sand.mat')\n",
    "X = mat['X']\n",
    "y = mat['Y'].ravel()\n",
    "\n",
    "# standardize X, center y\n",
    "X_standard = standardizeData(X)\n",
    "y_center = centerData(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.68, 2.55, 2.55, 4.83, 4.62, 4.51, 7.3 , 7.02, 6.85, 2.34, 2.06,\n",
       "       2.12, 4.74, 4.82, 4.67, 6.77, 6.56, 6.43, 1.41, 1.26, 1.37, 2.04,\n",
       "       1.94, 2.16, 2.63, 2.63, 2.22, 2.69, 2.55, 2.69, 3.76, 3.71, 3.6 ,\n",
       "       4.58, 4.35, 3.93, 3.92, 3.6 , 4.85, 4.84, 4.71, 5.52, 5.71, 6.03,\n",
       "       6.6 , 6.41, 6.23, 3.88, 4.57, 4.11, 7.47, 7.57, 7.64, 9.15, 9.17,\n",
       "       8.82, 4.69, 4.67, 4.67])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1(a) - Cp statistic\n",
    "\n",
    "Y_OLS = np.matmul(X, (linalg.lstsq(X,y))[0]) # NOTE: OLS solution doesn't make sense for p>>n\n",
    "s2 = ((Y_OLS - y)**2).sum(axis=0) # add n for variance estimate\n",
    "# NOTE: Check the value of s2 - our estimate of the variance of the noise in data is numerically zero\n",
    "# so what we are saying with the Cp is: we believe data has no noise. Go ahead and make as complicated a model\n",
    "# as possible. Cp works when n > p.\n",
    "\n",
    "\n",
    "[n, p] = np.shape(X)\n",
    "\n",
    "off = np.ones(n)\n",
    "M = np.c_[off, X] # Include offset / intercept\n",
    "\n",
    "\n",
    "# Linear solver\n",
    "beta, _, rnk, s = lng.lstsq(X, y)\n",
    "\n",
    "yhat = np.matmul(X, beta)\n",
    "\n",
    "e = y - np.matmul(X, beta) # Low bias std\n",
    "s = np.std(e)\n",
    "\n",
    "k = len(y)\n",
    "\n",
    "Cp = np.zeros(k)\n",
    "\n",
    "Beta = np.zeros((p,n))\n",
    "\n",
    "for j in range(k):\n",
    "    regr = linear_model.Lars(n_nonzero_coefs=j)\n",
    "    regr.fit(X, y)\n",
    "    Beta[:,j] = regr.coef_\n",
    "    yHat = np.matmul(X, Beta[:,j])\n",
    "    \n",
    "    residuals = (y-yHat) \n",
    "    s = residuals.std()\n",
    "    \n",
    "    Cp[j] = (1/(s**2))*sum((y-yHat)**2) - n + 2*j\n",
    "    \n",
    "#for i in range(np.shape(Beta)[0]):\n",
    "#    plt.plot(Beta[i,:])\n",
    "    \n",
    "#plt.legend()\n",
    "#plt.plot(Cp)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-c16b2d1cea2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_nonzero_coefs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_center\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mBetas\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, Xy)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m         self._fit(X, y, max_iter=max_iter, alpha=alpha, fit_path=self.fit_path,\n\u001b[1;32m--> 709\u001b[1;33m                   Xy=Xy)\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_iter, alpha, fit_path, Xy)\u001b[0m\n\u001b[0;32m    666\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m                     \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_n_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m                     positive=self.positive)\n\u001b[0m\u001b[0;32m    669\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malphas_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iter_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py\u001b[0m in \u001b[0;36mlars_path\u001b[1;34m(X, y, Xy, Gram, max_iter, alpha_min, method, copy_X, eps, copy_Gram, verbose, return_path, return_n_iter, positive)\u001b[0m\n\u001b[0;32m    343\u001b[0m         least_squares, info = solve_cholesky(L[:n_active, :n_active],\n\u001b[0;32m    344\u001b[0m                                              \u001b[0msign_active\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_active\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                                              lower=True)\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleast_squares\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mleast_squares\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1(b)\n",
    "\n",
    "# split data into train and test\n",
    "\n",
    "CV = 5 # if CV = n leave-one-out, you may try different numbers\n",
    "# this corresponds to crossvalind in matlab\n",
    "# permutes observations - useful when n != 0\n",
    "I = np.asarray([0] * n)\n",
    "for i in range(n):\n",
    "    I[i] = (i + 1) % CV + 1\n",
    "     \n",
    "I = I[np.random.permutation(n)]\n",
    "\n",
    "mse_train = np.zeros((K, p))\n",
    "mse_test = np.zeros((K, p))\n",
    "\n",
    "for i in range(1, CV+1):\n",
    "    X_train = X[i!=I,:]\n",
    "    X_test = X[i==I,:]\n",
    "    y_train = y[i!=I]\n",
    "    y_test = y[i==I]\n",
    "    \n",
    "    # normalize X\n",
    "    X_train_norm, mx, varx = normalize(X_train) # normalize training data\n",
    "    X_test_norm = normalizetest(X_test, mx, varx) # use mean and variance of training data for normalizing test data\n",
    "    \n",
    "    \n",
    "    y_train_center, my = center(y_train) # center training response\n",
    "    \n",
    "    #Ytr = Ytr[0,:] # Indexing in python thingy, no time to solve it\n",
    "    y_test_center = y_test-my # use the mean value of the training response to center the test response\n",
    "    \n",
    "    # create a model on the training data\n",
    "    K = p\n",
    "    Betas = np.zeros((K, p))\n",
    "    for j in range(K):\n",
    "        \n",
    "        reg = linear_model.Lars(n_nonzero_coefs=j, fit_path = False, fit_intercept = False, verbose = True)\n",
    "        reg.fit(X_train_norm,y_train_center)\n",
    "        beta = reg.coef_.ravel()\n",
    "        Betas[i-1, :] = beta\n",
    "        \n",
    "        #reg = linear_model.Lars(n_nonzero_coefs=j) # LARS model\n",
    "        #reg.fit(X_train_norm, y_train_center)\n",
    "        \n",
    "        #beta = reg.coef_\n",
    "        \n",
    "        y_train_hat = np.matmul(X_train_norm, beta)\n",
    "        y_test_hat = np.matmul(X_test_norm, beta)\n",
    "        \n",
    "        mse_train[i-1,j] = np.mean(np.sqrt((y_train_center-y_train_hat)**2))\n",
    "        mse_test[i-1,j] = np.mean(np.sqrt((y_test_center-y_test_hat)**2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2(a)\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_center.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
